{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return is_bool(val) or is_collection(val) or is_decimal(val) or is_float(val) or is_integer(val) or is_string(val) or is_none(val)", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    if not url:\n        return url\n    try:\n        scheme, netloc, path, params, query, fragment = urllib.parse.urlparse(url)\n    except ValueError:\n        return url\n    if not netloc:\n        return url\n    try:\n        netloc.encode('ascii')\n    except UnicodeEncodeError:\n        netloc = netloc.encode('idna').decode('ascii')\n    return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    if min_level < 1:\n        min_level = 1\n    if max_level > 6:\n        max_level = 6\n\n    toc_items = []\n\n    def _add_toc_item(md, state, token):\n        if token['type'] == 'heading':\n            level = token['attrs']['level']\n            if min_level <= level <= max_level:\n                text = token['text']\n                attrs = token['attrs']\n                id = attrs.get('id')\n                if id:\n                    id = id.replace(' ', '-')\n                else:\n                    id = heading_id(text)\n                toc_items.append((level, id, text))\n\n    md.before_render_hooks.append(_add_toc_item)\n    md.env['toc_items'] = toc_items", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.insert_rule(md.block.quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.quote_rules, 'nptable', before='paragraph')", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(n_jobs) as executor:\n        futures_ = [executor.submit(partial(callback, text, **kwargs)) for text in texts]\n        for future in futures.as_completed(futures_):\n            yield future.result()", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if width == 0:\n        return suffix\n    if len(text) <= width:\n        return text\n    if width <= len(suffix):\n        return suffix\n    return text[: width - len(suffix)] + suffix", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.FunctionNamespace(None).unset(fname)\n    else:\n        etree.FunctionNamespace(None).set(fname, func)", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  output, error = p.communicate(data)\n  return output.decode()\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  return re.sub(r'%([0-9a-fA-F]{2})', lambda m: chr(int(m.group(1), 16)), data)", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, six.binary_type):\n            return b64encode(value).decode(\"ascii\")\n        else:\n            return six.text_type(value)", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return as_string(value)", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.binary_type):\n        raise ValueError(\"Value is binary, not string\")\n    else:\n        return six.text_type(value)", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(obj[index] if index < len(obj) else None for index in indexes)", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for w in f:\n                w = w.strip()\n                if w.startswith('#'):\n                    continue\n                if w:\n                    w_list = w.split()\n                    if len(w_list) == 2:\n                        words[w_list[0]] = w_list[1]\n    return words", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        while spans and spans[0].stop <= envelope.start:\n            spans.pop(0)\n\n        while spans and spans[-1].start >= envelope.stop:\n            spans.pop()\n\n        yield spans", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    params = urllib.parse.parse_qs(content, keep_blank_values=True)\n    for key in params:\n        if len(params[key]) > 1:\n            raise ValueError(\"Repeated key: {}\".format(key))\n    return {key: params[key][0] for key in params}", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for x in t.cast(\"t.AsyncIterable[V]\", iterable):\n            yield x\n\n    else:\n        for x in t.cast(\"t.Iterable[V]\", iterable):\n            yield x", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    deque(iterable, maxlen=0)", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            word_pos_seq = posseg.lcut(sentence)\n            word_seq, pos_seq = [], []\n            for w, p in word_pos_seq:\n                word_seq.append(w)\n                pos_seq.append(p)\n            return word_seq, pos_seq\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        return list(sentence)\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif isinstance(obj, type):\n        return f\"{obj.__module__} {obj.__name__} object\"\n    else:\n        return f\"{obj.__class__.__module__} {obj.__class__.__name__} object\"\n\n", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        with self._wlock:\n            if key not in self._mapping:\n                self[key] = default\n            return self[key]\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            word_freq[w] = word_freq.get(w, 0) + 1\n        return word_freq\n\n    ", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        if len(content_words_in_sentence) == 0:\n            return 0\n        else:\n            return sum(word_freq_in_doc[w] for w in content_words_in_sentence) / len(content_words_in_sentence)\n\n    ", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        # total number of documents\n        t = len(sentences)\n\n        # total number of documents where each term appears\n        df = {}\n        for sentence in sentences:\n            for term in sentence:\n                df[term] = df.get(term, 0) + 1\n\n        # compute idf for each term\n        idf = {}\n        for term, freq in df.items():\n            idf[term] = math.log(t / freq)\n\n        return idf\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        unique_words1 = frozenset(sentence1)\n        unique_words2 = frozenset(sentence2)\n\n        numerator = 0.0\n        for term in unique_words1:\n            if term in unique_words2:\n                numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n\n        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in unique_words1)\n        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in unique_words2)\n\n        if denominator1 > 0 and denominator2 > 0:\n            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n        else:\n            return 0.0\n\n    ", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise (ValueError(\"Object in collection must be of type Sentence\"))\n        words.extend(sentence.words)\n\n    return words", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(f\"Router class {router_class} is already registered\")\n\n    def decorator(func):\n        _supported_routers[router_class] = func\n        return func\n\n    return decorator", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for uri_template, method_map, resource, parameters in router._router:\n        methods = []\n        for method_name, method_handler in method_map.items():\n            source_info = _get_source_info(method_handler)\n            methods.append(\n                RouteMethodInfo(\n                    method_name,\n                    source_info,\n                    method_handler.__name__,\n                    _is_internal(method_handler),\n                )\n            )\n        source_info = _get_source_info(resource)\n        class_name = getattr(resource, '__name__', '[unknown]')\n        routes.append(RouteInfo(uri_template, class_name, source_info, methods))\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    module = getattr(obj, '__module__', None)\n    if module is None:\n        return False\n    return module.startswith('falcon')", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module_name, app_name = args.app_module.split(':')\n    except ValueError:\n        parser.error('The app_module must be in the form: module:app')\n\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        parser.error('Could not import module: {0}'.format(module_name))\n\n    try:\n        app = getattr(module, app_name)\n    except AttributeError:\n        parser.error('Could not find app: {0}'.format(app_name))\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except TypeError:\n            parser.error(\n                'The app must be an instance of falcon.App, '\n                'or a callable that returns an instance of falcon.App'\n            )\n\n    if not isinstance(app, falcon.App):\n        parser.error(\n            'The app must be an instance of falcon.App, '\n            'or a callable that returns an instance of falcon.App'\n        )\n\n    return app", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(\n        description='Inspects a Falcon app and displays its routes.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        '-r',\n        '--router',\n        action='store_true',\n        help='Display router information',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        action='store_true',\n        help='Display verbose information',\n    )\n    parser.add_argument(\n        '-i',\n        '--internal',\n        action='store_true',\n        help='Display internal information',\n    )\n    parser.add_argument(\n        'app_module',\n        help='The module and instance to inspect (e.g. \"mymod:app\")',\n    )\n    return parser", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string')\n\n    if len(quoted) < 2 or quoted[0] != '\"' or quoted[-1] != '\"':\n        return quoted\n\n    unquoted = ''\n    i = 1\n    while i < len(quoted) - 1:\n        if quoted[i] == '\\\\':\n            if quoted[i + 1] == '\\\\':\n                unquoted += '\\\\'\n                i += 2\n            elif quoted[i + 1] == '\"':\n                unquoted += '\"'\n                i += 2\n            else:\n                unquoted += quoted[i]\n                i += 1\n        else:\n            unquoted += quoted[i]\n            i += 1\n\n    return unquoted", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    argspec = inspect.getfullargspec(func)\n    return argspec.args + argspec.kwonlyargs", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return len(inspect.signature(app).parameters) == 3\n\n", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone=timezone.utc)\n    return dt", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    return cv + lv", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        return '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % {'resource': resource, 'expires': expires}\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if not p.startswith('/'):\n            p = '/' + p\n        return urllib.quote(p, safe='/')", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except ValueError:\n        return 400", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    elif scope is None:\n        return None\n    return scope.strip().split()", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None or isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, int):\n        return bytes([x])\n    if isinstance(x, float):\n        return str(x).encode(charset, errors)\n    raise TypeError('Expected bytes, str, int, or float')", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    return base64.urlsafe_b64decode(s + b'=' * (-len(s) % 4))", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    return bool(\n        conn.execute(\n            \"\"\"\n        SELECT name\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\",\n            (table,),\n        ).fetchall()\n    )", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.isfile(filename):\n            raise IOError('file {} does not exist'.format(filename))\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = [table[0] for table in cursor.fetchall()]\n        conn.close()\n        return tables\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_query = sqlparse.format(query, strip_comments=True).strip().lower()\n    if formatted_query:\n        first_word = formatted_query.split()[0]\n        if first_word in prefixes:\n            return True\n    return False", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if renderer.format == format:\n                filtered_renderers.append(renderer)\n        if not filtered_renderers:\n            raise Http404\n        return filtered_renderers", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return '%s' % value\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or isinstance(value, list):\n        return 'class=nested'\n    return ''\n\n", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except ValueError:\n            raise ValueError('Invalid pickle data')\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        storage = self.setdefault('_f_' + queue, [])\n        if not allow_duplicate:\n            if msg in storage:\n                return\n        storage.append(msg)\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        if storage:\n            return storage.pop(0)\n        return None", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        return self.pop('_f_' + queue, [])", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(cls):\n        setattr(cls, '__view_defaults__', settings)\n        return cls\n\n    return decorator", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    return dict(s.split('=') for s in args)", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path_info)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n\n        return None", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    converted = ''.join(c.title() for c in chunks)\n    return converted if initial else converted[0].lower() + converted[1:]", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if not isinstance(b, bytes):\n        raise TypeError(f\"input must be a byte string, got {type(b).__name__}\")\n    if len(b) == 0:\n        return None\n    pos = -1\n    while pos >= -len(b) and b[pos] == 0xFF:\n        pos -= 1\n    if pos == -len(b):\n        return None\n    return b[:pos] + bytes([b[pos] + 1])", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    parent_dir = dirname(path)\n    if not exists(parent_dir):\n        os.makedirs(parent_dir)", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n        current_time = datetime.now()\n        time_difference = current_time - file_modified_time\n        if time_difference > timedelta(hours=24):\n            return True\n    return False", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        with open(devnull, 'w') as devnull_file:\n            subprocess.Popen([command], stdout=devnull_file, stderr=devnull_file)\n    except OSError:\n        return False\n    return True", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # parsed can contain one or more semi-colon separated commands\n    parsed = sqlparse.parse(sql)\n    for p in parsed:\n        # p is a sqlparse.sql.Statement\n        tokens = p.flatten()\n        n_tok = len(tokens)\n        if n_tok > n_skip:\n            return tokens[n_tok - n_skip - 1].value, sql[:tokens[n_tok - n_skip - 1].offset]\n    return None, sql", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is not None:\n        return config\n\n    if filename is None:\n        filename = Path(__file__).parent / 'settings.cfg'\n\n    config = ConfigParser()\n    config.read(filename)\n    return config", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    myagents = None\n    mycookie = None\n    if config.has_option('DEFAULT', 'USER_AGENTS'):\n        myagents = config.get('DEFAULT', 'USER_AGENTS').split('\\n')\n    if config.has_option('DEFAULT', 'COOKIES'):\n        mycookie = config.get('DEFAULT', 'COOKIES')\n    return myagents, mycookie", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # init\n    bufferlist = []\n    # get URLs\n    for url in url_store.get_urls():\n        # check if URL is live\n        if is_live_page(url):\n            bufferlist.append(url)\n        else:\n            # add empty test\n            bufferlist.append('')\n    # check if bufferlist is empty\n    if not bufferlist:\n        # sleep\n        sleep(sleep_time)\n    return bufferlist, url_store", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    authors = authors.lower()\n    author_blacklist = {x.lower() for x in author_blacklist}\n    authors = [x.strip() for x in authors.split(';')]\n    new_authors = [x for x in authors if x not in author_blacklist]\n    if new_authors:\n        return '; '.join(new_authors)\n    return None", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    if \"_where\" in request.args:\n        if not datasette.permission_allowed(\n            request.actor, \"execute_sql\", resource=database\n        ):\n            raise DatasetteError(\"execute_sql permission denied\", status=403)\n        where_clauses = []\n        extra_wheres_for_ui = []\n        for i, where in enumerate(request.args.getlist(\"_where\")):\n            where_clauses.append(where)\n            extra_wheres_for_ui.append(\n                {\n                    \"value\": where,\n                    \"sql\": where,\n                    \"label\": where,\n                    \"hideable\": True,\n                    \"path_with_removed_args\": path_with_removed_args(\n                        request, \"_where\", where\n                    ),\n                }\n            )\n        return FilterArguments(where_clauses, extra_wheres_for_ui=extra_wheres_for_ui)\n\n", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    current = []\n    if isinstance(args, set):\n\n        def should_add(key, value):\n            return key in args\n\n    elif isinstance(args, dict):\n        # Must match key AND value\n        def should_add(key, value):\n            return args.get(key) == value\n\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if not should_add(key, value):\n            current.append((key, value))\n    current.extend([p for p in args if p[1] is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    if isinstance(args, dict):\n        args = args.items()\n    args_to_remove = {k for k, v in args if v is None}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_remove:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    units = [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]\n    for unit in units:\n        if bytes < 1024:\n            return f\"{bytes:.2f} {unit}\"\n        bytes /= 1024\n    return f\"{bytes:.2f} {units[-1]}\"", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if isinstance(allow, str):\n        return actor == allow\n    elif isinstance(allow, list):\n        return actor in allow\n    elif isinstance(allow, dict):\n        for key, value in allow.items():\n            if key not in actor:\n                return False\n            if actor[key] != value:\n                return False\n        return True\n    else:\n        return False", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        resolved_config = {}\n        for key, value in config.items():\n            if isinstance(value, dict) and \"$env\" in value:\n                resolved_config[key] = environ[value[\"$env\"]]\n            elif isinstance(value, dict) and \"$file\" in value:\n                with open(value[\"$file\"], \"r\") as f:\n                    resolved_config[key] = f.read()\n            elif isinstance(value, dict):\n                resolved_config[key] = resolve_env_secrets(value, environ)\n            else:\n                resolved_config[key] = value\n        return resolved_config\n    elif isinstance(config, list):\n        resolved_config = []\n        for item in config:\n            if isinstance(item, dict) and \"$env\" in item:\n                resolved_config.append(environ[item[\"$env\"]])\n            elif isinstance(item, dict) and \"$file\" in item:\n                with open(item[\"$file\"], \"r\") as f:\n                    resolved_config.append(f.read())\n            elif isinstance(item, dict):\n                resolved_config.append(resolve_env_secrets(item, environ))\n            else:\n                resolved_config.append(item)\n        return resolved_config\n    else:\n        return config", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    elif \"name\" in actor:\n        return actor[\"name\"]\n    elif \"username\" in actor:\n        return actor[\"username\"]\n    elif \"login\" in actor:\n        return actor[\"login\"]\n    elif \"id\" in actor:\n        return actor[\"id\"]\n    else:\n        return str(actor)", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    if len(datasette.databases) == 1:\n        return datasette.databases[0].path\n    if len(datasette.databases[0].tables) == 1:\n        return datasette.databases[0].tables[0].path\n    return datasette.path", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    s = s.replace(\"%\", \"~PERCENT~\")\n    s = urllib.parse.unquote(s)\n    s = s.replace(\"~PERCENT~\", \"%\")\n    return s\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = regex.match(path)\n        if match:\n            return match, view\n    return None", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    # Check if the URL ends with a file extension\n    file_extension = url.split(\".\")[-1]\n    if len(file_extension) > 0 and len(file_extension) <= 4 and \"/\" not in file_extension:\n        # Truncate the URL to the specified length minus the length of the file extension minus 1 (for the dot)\n        truncated_url = url[: length - len(file_extension) - 1]\n        # Add ellipsis and the file extension at the end\n        truncated_url += \"...\" + file_extension\n    else:\n        # Truncate the URL to the specified length minus 3 (for the ellipsis)\n        truncated_url = url[: length - 3]\n        # Add ellipsis at the end\n        truncated_url += \"...\"\n\n    return truncated_url\n\n", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    permission_backend = request.registry.permission\n    if permission_backend is None:\n        return []\n\n    # If the permission backend is configured, query it only once per request.\n    if not hasattr(request, \"_cached_principals\"):\n        request._cached_principals = permission_backend.get_user_principals(userid)\n\n    return request._cached_principals", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    if isinstance(message, str):\n        message = message.encode(encoding)\n    return hmac.new(secret, message, hashlib.sha256).hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    try:\n        services = request.registry.services\n        for service in services:\n            if service.match(request.path, request.method) is not None:\n                return service\n    except AttributeError:\n        pass\n    return None", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"system.Authenticated\" not in principals:\n        return principals\n\n    # Remove the unprefixed user id from the effective principals to avoid conflicts\n    unprefixed_userid = request.selected_userid\n    principals.remove(unprefixed_userid)\n\n    # Add the prefixed user id to the beginning of the list\n    prefixed_userid = prefixed_userid(request)\n    principals.insert(0, prefixed_userid)\n\n    return principals\n\n", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n    validation_enabled = settings.get(\"account_validation\", False)\n\n    if validation_enabled:\n        for obj in event.impacted_objects:\n            # Extract username and password from current user\n            username = obj[\"new\"][\"id\"]\n            activation_key = get_cached_validation_key(username, request.registry)\n            if activation_key is None:\n                continue\n\n            # Send email\n            emailer = Emailer(request)\n            emailer.send_activation(username, activation_key)", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    hashed = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt())\n    return hashed.decode(\"utf-8\")\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    if not object_uri.startswith(\"/buckets/\"):\n        return \"\"\n\n    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n\n    return \"/\".join(path[:3])", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorate(f):\n        _registry[name] = f\n        return f\n\n    return decorate", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db_name, collection_name = namespace.split(\".\", 1)\n    db_regex = re.compile(db_name.replace(\"*\", \"(.*)\"))\n    collection_regex = re.compile(collection_name.replace(\"*\", \"(.*)\"))\n    return re.compile(db_regex.pattern + \"\\.\" + collection_regex.pattern)", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    seconds = val >> 32\n    incrementor = val & 0xFFFFFFFF\n    return Timestamp(seconds, incrementor)", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == 'Windows':\n        dir_fd = None\n    else:\n        dir_fd = os.open(os.path.dirname(path), os.O_RDONLY)\n    fd = io.open(path, 'rb+')\n    fd.seek(0, io.SEEK_END)\n    if fd.tell() == 0:\n        # Empty file, write the page size in the header\n        fd.write(PAGE_SIZE.to_bytes(OTHERS_BYTES, ENDIAN))\n    return fd, dir_fd", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        chunk = tuple(itertools.islice(it, n))\n        if not chunk:\n            return\n        yield chunk, False\n    yield chunk, True", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        return obj.encode(encoding='utf-8')", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.pack(fmt, *args)", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height, width = pattern.data[2:4]\n    pattern_array = np.zeros((height, width, 4), dtype=np.float32)\n    for channel in pattern.data[4:]:\n        channel_id = channel.id\n        if channel_id == ChannelID.TRANSPARENCY_MASK:\n            pattern_array[:, :, 3] = _parse_array(channel.data, 1)\n        elif channel_id == ChannelID.USER_LAYER_MASK:\n            pattern_array[:, :, 2] = _parse_array(channel.data, 1)\n        elif channel_id == ChannelID.REAL_USER_LAYER_MASK:\n            pattern_array[:, :, 2] = _parse_array(channel.data, 1)\n        elif channel_id >= 0:\n            pattern_array[:, :, channel_id] = _parse_array(channel.data, 16)\n    return pattern_array", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n    while True:\n        try:\n            csv.field_size_limit(csv.field_size_limit() * 2)\n        except OverflowError:\n            break\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type is None:\n        return \"NULL\"\n    elif column_type in (\"text\", \"blob\"):\n        return column_type.upper()\n    elif column_type == \"integer\":\n        return \"INT\"\n    elif column_type == \"real\":\n        return \"REAL\"\n    elif column_type == \"numeric\":\n        return \"NUM\"\n    elif column_type == \"boolean\":\n        return \"INT\"\n    else:\n        return \"TEXT\"\n\n", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and \"$base64\" in value and \"encoded\" in value:\n            doc[key] = base64.b64decode(value[\"encoded\"])\n    return doc", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    it = iter(sequence)\n    while True:\n        chunk = tuple(itertools.islice(it, size))\n        if not chunk:\n            break\n        yield chunk", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    return hashlib.sha1(\n        \"\".join(\n            \"{}={}\".format(key, record.get(key, \"\"))\n            for key in sorted(keys)\n        ).encode(\"utf-8\")\n    ).hexdigest()\n\n", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if store:\n        if isinstance(store, (list, tuple)):\n            store = store[0]\n        return {'library': store.library_name,\n                'nodes': ','.join(store._mongo_client.nodes),\n                'host': store._mongo_client.primary}\n    return {}", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry, _retry_count\n        if _in_retry:\n            _retry_count += 1\n        else:\n            _retry_count = 0\n        _in_retry = True\n        try:\n            return f(*args, **kwargs)\n        except (AutoReconnect, OperationFailure, ServerSelectionTimeoutError) as e:\n            _handle_error(f, e, _retry_count, **kwargs)\n            return wrapper(*args, **kwargs)\n        except DuplicateKeyError as e:\n            _log_exception(f.__name__, e, _retry_count, **kwargs)\n            raise\n        except BulkWriteError as e:\n            _log_exception(f.__name__, e, _retry_count, **kwargs)\n            raise\n        finally:\n            _in_retry = False\n\n    return wrapper", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        else:\n            return o1 == o2\n    except Exception as e:\n        logger.exception(\"Exception in are_equals: %s\", e)\n        return False", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    return [array_2d[slices[i]:slices[i + 1]] for i in range(len(slices) - 1)]\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    # Convert the dictionary to a string representation\n    doc_str = str(doc)\n\n    # Concatenate the symbol and the dictionary string\n    concat_str = symbol + doc_str\n\n    # Calculate the SHA1 hash of the concatenated string\n    sha1_hash = hashlib.sha1(concat_str.encode('utf-8')).digest()\n\n    # Return the hash as a Binary object\n    return Binary(sha1_hash)\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return 'VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})'.format(\n            symbol=self.symbol,\n            library=self.library,\n            data=self.data,\n            version=self.version,\n            metadata=self.metadata,\n            host=self.host\n        )", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype(string, metadata=metadata)\n", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if dtype1.names is None:\n        return np.promote_types(dtype1, dtype2)\n    if dtype2.names is None:\n        return dtype1\n\n    # Check that dtype1 is a superset of dtype2\n    if not set(dtype1.names).issuperset(dtype2.names):\n        raise ValueError(\"dtype1 is not a superset of dtype2\")\n\n    # Promote the data types of the two structured arrays\n    dtype1_promoted = []\n    for name in dtype1.names:\n        if name in dtype2.names:\n            dtype1_promoted.append((name, np.promote_types(dtype1[name], dtype2[name])))\n        else:\n            dtype1_promoted.append((name, dtype1[name]))\n    return np.dtype(dtype1_promoted)", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        if isinstance(data, DataFrame):\n            return DataFrame()\n        elif isinstance(data, Series):\n            return Series()\n        else:\n            raise ValueError(\"Invalid data type. Expected DataFrame or Series.\")", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if isinstance(df, pd.Series):\n            df = df.to_frame()\n\n        if 'date' in df.columns:\n            df = df.sort_values('date')\n        else:\n            df = df.sort_index()\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if df.empty:\n            return\n\n        start_date = df.date.iloc[0]\n        end_date = df.date.iloc[-1]\n\n        for start, end in pd.date_range(start_date, end_date, freq=chunk_size):\n            if func:\n                yield start, end, chunk_size, func(df[(df.date >= start) & (df.date < end)], **kwargs)\n            else:\n                yield start, end, chunk_size, df[(df.date >= start) & (df.date < end)]", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~data.index.get_level_values('date').isin(pd.date_range(start, end))]\n        elif 'date' in data.columns:\n            return data[~data.date.isin(pd.date_range(start, end))]\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config.get(\"hostname\", None):\n        return None\n    proxy_string = \"{type}://{hostname}:{port}\".format(\n        type=proxy_config.get(\"type\", \"http\"),\n        hostname=proxy_config.get(\"hostname\"),\n        port=proxy_config.get(\"port\", \"80\"),\n    )\n    if auth and proxy_config.get(\"username\", None):\n        return re.sub(\n            \"//(.*)\",\n            r\"//\"\n            + proxy_config.get(\"username\", \"\")\n            + \":\"\n            + proxy_config.get(\"password\", \"\")\n            + r\"@\\1\",\n            proxy_string,\n        )\n    return proxy_string", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[data.index.get_level_values('date').isin(to_pandas_closed_closed(range_obj))]\n        elif 'date' in data.columns:\n            return data[data.date.isin(to_pandas_closed_closed(range_obj))]\n        else:\n            return data", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and value is None:\n        raise ValueError(\"value is required\")", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        names = \", \".join(repr(c) for c in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value} must be larger than {minimum}.\")", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value} must be smaller than {maximum}.\")", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    best = (3, None)\n    for choice in choices:\n        distance = _levenshtein(name, choice)\n        if distance < best[0]:\n            best = (distance, choice)\n\n    if best[0] <= 3:\n        return best[1]\n    return None", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(char, char.encode(encoding=\"unicode-escape\").decode())\n\n    return value", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is True:\n            return \"true\"\n        elif value is False or value is None:\n            return \"false\"\n        else:\n            raise ValueError(f\"{value} is not a boolean\")", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Check if the DataFrame is single-level\n    if isinstance(data.index, pd.MultiIndex):\n        raise ValueError(\"The DataFrame is Multi-level. Please use df2mat_multi instead.\")\n\n    # Get the column names\n    cols = data.columns\n\n    # Get the column types\n    col_types = data.dtypes\n\n    # Get the column labels\n    labels = []\n\n    # Loop through the columns\n    for col in cols:\n\n        # Check if the column is text\n        if col_types[col] == object:\n\n            # Get the unique values in the column\n            unique_vals = data[col].unique()\n\n            # Loop through the unique values\n            for val in unique_vals:\n\n                # Create a new column for each unique value\n                data[col + \"_\" + str(val)] = (data[col] == val).astype(int)\n\n                # Add the new column to the list of labels\n                labels.append(col + \"_\" + str(val))\n\n            # Drop the original text column\n            data = data.drop(col, axis=1)\n\n        else:\n\n            # Add the column to the list of labels\n            labels.append(col)\n\n    # Convert the DataFrame to a Numpy array\n    data = data.values\n\n    # Return the Numpy array and the list of labels\n    if return_labels:\n        return data, labels\n    else:\n        return data", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    m = np.mean(x_stacked)\n    return [i - m for i in x]", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get sorted set of unique values\n    vals_set = sorted(set(vals))\n\n    # return index of each value in the sorted set\n    return [vals_set.index(i) for i in vals]", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    # get color palette\n    palette = sns.color_palette(cmap, res)\n    # map values to colors\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [palette[val_set.index(val)] for val in vals]", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    bins = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return bins", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    x = np.linspace(0,len(arr),len(arr))\n    x_new = np.linspace(0,len(arr),interp_val)\n    f = pchip(x, arr)\n    return f(x_new)", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if isinstance(x, (list, tuple)):\n        if isinstance(args, (list, tuple)):\n            if len(x) == len(args):\n                return list(zip(x, *args))\n            else:\n                print('Error: Length of x and args must be the same')\n                sys.exit(1)\n        else:\n            return [(i, args) for i in x]\n    else:\n        return [(x, args)]", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i,item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    term = environ.get('TERM', '')\n    colorterm = environ.get('COLORTERM', '')\n    if 'truecolor' in term or 'truecolor' in colorterm:\n        return 'truecolor'\n    if '256' in term or '256' in colorterm:\n        return '256fgbg'\n    return 'nocolor'", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError('{} is not a valid pool size.'.format(val))\n    return val", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    r, g, b = 0, 0, 0\n    for i in range(x, x + cell_width):\n        for j in range(y, y + cell_height):\n            r += px[i, j][0]\n            g += px[i, j][1]\n            b += px[i, j][2]\n    r //= cell_width * cell_height\n    g //= cell_width * cell_height\n    b //= cell_width * cell_height\n    return [r, g, b]", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('-')[-1]\n        gif_url = f'https://media.tenor.com/images/gifs/gif_{gif_id}.gif'\n    else:\n        if os.path.isfile(input_source):\n            gif_url = input_source\n        else:\n            # Send a request to the Tenor GIF API to get the GIF URL based on the input source\n            response = requests.get(\n                f'https://api.tenor.com/v1/search?q={input_source}&key={api_key}&limit=1'\n            )\n            try:\n                gif_url = response.json()['results'][0]['media'][0]['gif']['url']\n            except (KeyError, IndexError, JSONDecodeError):\n                raise ValueError(\n                    'Invalid input source. Please provide a valid Tenor GIF URL, local file path, or search query.'\n                )\n\n    return gif_url", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    if hue is None:\n        return [x], labels\n    else:\n        if isinstance(hue, (list, np.ndarray)):\n            hue = np.array(hue)\n        if isinstance(x, (list, np.ndarray)):\n            x = np.array(x)\n        if isinstance(labels, (list, np.ndarray)):\n            labels = np.array(labels)\n        if len(x.shape) == 1:\n            x = x.reshape(-1, 1)\n        if len(hue.shape) == 1:\n            hue = hue.reshape(-1, 1)\n        if len(labels.shape) == 1:\n            labels = labels.reshape(-1, 1)\n        if x.shape[0] != hue.shape[0]:\n            raise ValueError('x and hue must have the same number of rows')\n        if x.shape[0] != labels.shape[0]:\n            raise ValueError('x and labels must have the same number of rows')\n        if x.shape[1] != labels.shape[1]:\n            raise ValueError('x and labels must have the same number of columns')\n        if hue.shape[1] != 1:\n            raise ValueError('hue must have only one column')\n        if labels.shape[1] != 1:\n            raise ValueError('labels must have only one column')\n        if hue.shape[0] != labels.shape[0]:\n            raise ValueError('hue and labels must have the same number of rows')\n        if hue.shape[0] != x.shape[0]:\n            raise ValueError('hue and x must have the same number of rows')\n        if hue.shape[1] != x.shape[1]:\n            raise ValueError('hue and x must have the same number of columns')\n        if hue.shape[1] != labels.shape[1]:\n            raise ValueError('hue and labels must have the same number of columns')\n        if hue.shape[0] != labels.", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    # Throw exception\n    if not hasattr(note, \"name\"):\n        return False\n\n    # Process the octave\n    if process_octaves:\n        octave = note.octave\n    else:\n        octave = 0\n\n    # Process the note\n    if note.name == \"r\":\n        result = \"r\"\n    else:\n        result = note.name\n        if note.diminished:\n            result += \"es\"\n        elif note.augmented:\n            result += \"is\"\n        if note.octave_tied:\n            result += \"~\"\n        if note.tie:\n            result += \"~\"\n        if note.dot:\n            result += \".\"\n        if note.accidental is not None:\n            if note.accidental == \"sharp\":\n                result += \"is\"\n            elif note.accidental == \"flat\":\n                result += \"es\"\n            elif note.accidental == \"double sharp\":\n                result += \"isis\"\n            elif note.accidental == \"double flat\":\n                result += \"eses\"\n        if note.octave_up:\n            for i in range(note.octave_up):\n                result += \"'\"\n        if note.octave_down:\n            for i in range(note.octave_down):\n                result += \",\"\n        if note.quarter_tone_up:\n            for i in range(note.quarter_tone_up):\n                result += \"+\"\n        if note.quarter_tone_down:\n            for i in range(note.quarter_tone_down):\n                result += \"-\"\n        if note.octave_up or note.octave_down:\n            result += str(octave)\n    if not standalone:\n        return result\n    else:\n        return \"{ %s }\" % result", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Calculate the maximum number of frets on the tuning\n    max_frets = max([fret for string, fret in tuning.tuning])\n\n    # Calculate the maximum number of digits in the fret numbers\n    max_digits = len(str(max_frets))\n\n    # Calculate the maximum number of characters needed to represent the fret numbers\n    max_chars = max_frets * max_digits\n\n    # Calculate the maximum number of characters that can be used for the fret numbers\n    max_width = width - max_chars - 1\n\n    # Calculate the maximum number of characters that can be used for the quarter note markers\n    max_qsize = max_width // 4\n\n    # Return the maximum number of characters that can be used for the quarter note markers\n    return max_qsize", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    return duration > 0 and (duration & (duration - 1)) == 0", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n\n", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]\n\n", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Initialize variables\n    roman_numeral = \"\"\n    accidentals = 0\n    chord_suffix = \"\"\n\n    # Parse the input string\n    for char in progression:\n        if char in [\"b\", \"#\"]:\n            accidentals += 1 if char == \"#\" else -1\n        elif char.isalpha():\n            roman_numeral += char\n        else:\n            chord_suffix += char\n\n    # Return the result as a tuple\n    return (roman_numeral, accidentals, chord_suffix)", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder=byteorder)", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', value)\n    return string", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    return re.sub(r'^\\[pid\\s+\\d+\\]\\s+', '', line)", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if not path1:\n        return path2\n    if not path2:\n        return path1\n    if path1[-1] == \"/\":\n        return path1 + path2\n    return path1 + \"/\" + path2", "idx": 163}
{"namespace": "fs.path.split", "completion": "    head, tail = splitdrive(path)\n    if head and tail and not tail.startswith(\"/\"):\n        tail = \"/\" + tail\n    head_parts = head.split(\"/\")\n    if tail == \"/\":\n        tail_parts: List[Text] = []\n    else:\n        tail_parts = tail.split(\"/\")\n    return head_parts, tail_parts", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1.startswith(_path2)  # longer one is parent", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val is None:\n        return None\n\n    if val.lower() in ('false', '0'):\n        return False\n    elif val.lower() in ('true', '1'):\n        return True\n    else:\n        raise ValueError('Invalid boolean value: {}'.format(val))", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        d['time'] = datetime.datetime.utcnow().isoformat()\n        d['pid'] = os.getpid()\n\n        return ' '.join(sorted(\n            ['{0}={1}'.format(k, v) for k, v in d.items()],\n            key=lambda x: x if x.startswith('time=') or x.startswith('pid=') else ''))\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'rb') as f:\n            os.fsync(f.fileno())\n        dirname = os.path.dirname(filename)\n        with open(dirname, 'rb') as f:\n            os.fsync(f.fileno())", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return \"/\".join(str(part).strip(\"/\") for part in path_parts)\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return os.path.abspath(inspect.getsourcefile(cls))\n        except TypeError:\n            return None\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = sorted(version_map.items(), key=lambda x: x[0])\n\n    if not version_map:\n        return default\n\n    version = LooseVersion(version)\n\n    for version_str, value in version_map:\n        if version < version_str:\n            return value\n\n    return version_map[-1][1]", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n\n    return None", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return u'\\t'.join(x for x in (key, value) if x is not None).encode('utf_8')", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split(u'\\t', 1)\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split(u'\\t', 1)\n\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    filename = filename.lstrip('.')\n    dot_index = filename.find('.')\n    if dot_index == -1:\n        return ''\n    else:\n        return filename[dot_index:]", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if not os.path.exists(archive_path):\n        raise IOError('No such file: %r' % (archive_path,))\n\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n\n    if is_zipfile(archive_path):\n        with ZipFile(archive_path) as zip_file:\n            for name in zip_file.namelist():\n                if name.endswith('/'):\n                    continue\n\n                zip_file.extract(name, dest)\n    else:\n        with tarfile.open(archive_path) as tar_file:\n            tar_file.extractall(dest)", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            yield item\n            seen.add(item)", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    if isinstance(urlstring, bytes):\n        urlstring = urlstring.decode('utf-8')\n    if isinstance(scheme, bytes):\n        scheme = scheme.decode('utf-8')\n    if isinstance(allow_fragments, bytes):\n        allow_fragments = allow_fragments.decode('utf-8')\n    if isinstance(args, bytes):\n        args = args.decode('utf-8')\n    if isinstance(kwargs, bytes):\n        kwargs = kwargs.decode('utf-8')\n\n    # Parse the URL\n    parsed_url = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # Split the fragment into its components\n    fragment_components = parsed_url.fragment.split('&')\n\n    # Reconstruct the fragment\n    parsed_url = parsed_url._replace(fragment='')\n    for component in fragment_components:\n        parsed_url = parsed_url._replace(fragment=parsed_url.fragment + '#' + component)\n\n    return parsed_url", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ['PATH']\n\n    for dir in path.split(os.pathsep):\n        path_to_cmd = os.path.join(dir, cmd)\n        if os.path.isfile(path_to_cmd) and os.access(path_to_cmd, os.X_OK):\n            return path_to_cmd\n\n    return None", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    if '@' in rhostport:\n        username, rhostport = rhostport.split('@', 1)\n        if ':' in username:\n            username, password = username.split(':', 1)\n        else:\n            password = None\n    else:\n        username = None\n        password = None\n\n    if ':' in rhostport:\n        host, port = rhostport.split(':', 1)\n        port = int(port)\n    else:\n        host = rhostport\n        port = None\n\n    if not host:\n        return None, None, None, None\n\n    if not re.match(r'^[a-zA-Z0-9._-]+$', host):\n        # host is not a simple hostname, so it might be an IP address\n        try:\n            ipaddress.ip_address(host)\n        except ValueError:\n            # host is not an IP address, so it might be a ssh alias\n            # from ~/.ssh/config\n            try:\n                ssh_config = os.path.expanduser('~/.ssh/config')\n                with open(ssh_config, 'r') as f:\n                    config = f.read()\n                match = re.search(r'Host\\s+%s\\s+HostName\\s+(\\S+)' % host, config)\n                if match:\n                    host = match.group(1)\n            except FileNotFoundError:\n                pass\n\n    return username, password, port, host", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    if not key or not value:\n        return False\n    if key not in str_dict:\n        return False\n    if str_dict.index(key) + len(key) + 2 >= len(str_dict):\n        return False\n    if str_dict[str_dict.index(key) + len(key) + 2] != \"'\":\n        return False\n    if str_dict.index(key) + len(key) + 3 + len(value) >= len(str_dict):\n        return False\n    if str_dict[str_dict.index(key) + len(key) + 3 + len(value)] != \"'\":\n        return False\n    return True", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n        return module.Method(method_name)\n    except ImportError:\n        raise Fatal(\"Unknown method '%s'.\" % method_name)", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    return set(open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt')).read().splitlines())", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filter(None, map(_parse_record, json_records)))", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b\"\"\n        is_negative = (v < 0)\n        if is_negative:\n            v = -v\n        s = bytearray()\n        while v:\n            s.append(v & 0xff)\n            v >>= 8\n        if s[-1] & 0x80:\n            s.append(0x00)\n        if is_negative:\n            s[-1] |= 0x80\n        s.reverse()\n        return bytes(s)\n", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    return [\n        _s3_key_prefix(prefix, date, account_id, region)\n        for date in dates\n        for account_id in account_ids\n        for region in regions\n    ]\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != b'\\x00':\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.append(stack.pop(-2))", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1 + v2)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # s = v / k + (se * r) / k\n    # so\n    # k * s = v + (se * r)\n    # so\n    # k * s * r = r * v + (se * r * r)\n    # so\n    # k * s * r - r * v = se * r * r\n    # so\n    # k * s * r - r * v = se * r^2\n    # so\n    # k * s * r - r * v = se * sig\n    # so\n    # k * s * r = r * v + sig\n    # so\n    # k * s = v + sig / r\n    # so\n    # k = (v + sig / r) / s\n\n    r, s = sig\n    return (signed_value + sig * r.inverse(generator.order())) * r.inverse(k) % generator.order()", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    return (((val1 - val2) % generator.order()) * (r1 - r2).inverse(generator.order())) % generator.order()\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for type_code, (parse_func, stream_func) in parsing_functions:\n        streamer.register_array_count_parse_func(type_code, parse_func)\n        streamer.register_array_count_stream_func(type_code, stream_func)\n    return streamer", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Split the path range into its components\n    path_range_parts = path_range.split(\"/\")\n\n    # Initialize an empty list to store the subpaths\n    subpaths = []\n\n    # Iterate over the path range parts\n    for part in path_range_parts:\n        # Check if the part contains a range\n        if \"-\" in part:\n            # Split the part into its start and end values\n            start, end = part.split(\"-\")\n            # Convert the start and end values to integers\n            start = int(start)\n            end = int(end)\n            # Generate a list of subpaths for the range\n            subpaths.append(list(range(start, end + 1)))\n        else:\n            # If the part does not contain a range, add it to the list of subpaths\n            subpaths.append([int(part)])\n\n    # Use the itertools.product function to generate all possible combinations of subpaths\n    for subpath in itertools.product(*subpaths):\n        # Join the subpaths into a single path string\n        path = \"/\".join(str(p) for p in subpath)\n        # Check if the path contains any hardening characters\n        if any(c in path for c in hardening_chars):\n            # If it does, yield the path with the hardening character replaced by a question mark\n            yield path.replace(hardening_chars[0], \"?\")\n        else:\n            # If it does not, yield the path as is\n            yield path\n\n", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    try:\n        return binascii.unhexlify(h)\n    except Exception:\n        raise ValueError(\"{} is not a valid hex string\".format(h))", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    degree_sum = 0\n    for node in graph:\n        degree_sum += len(graph[node])\n\n    return degree_sum / len(graph)\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if 0 <= k <= n:\n        ntok = 1\n        ktok = 1\n        for t in range(1, min(k, n - k) + 1):\n            ntok *= n\n            ktok *= t\n            n -= 1\n        return ntok // ktok\n    else:\n        return 0", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_chars = {}\n    for i in range(len(password)):\n        password_chars[password[i]] = True\n\n    subtable = {}\n    for key, values in table.items():\n        if password_chars.get(key, False):\n            subtable[key] = values\n\n    return subtable", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    return ' '.join([chr_map.get(chr, chr) for chr in string])", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    rval = db.GetNet(obj)\n    results.append((obj, rval))\n  return results", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    port, protocol = db.GetService(svc)\n    results.append((svc, '%s/%s' % (port, protocol)))\n  return results", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    ip = nacaddr.IP(ip)\n    for obj in options.cmp:\n      if ip in db.GetNet(obj):\n        results.append('%s is in %s' % (ip, obj))\n      else:\n        results.append('%s is not in %s' % (ip, obj))\n  return '\\n'.join(results)", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.services:\n    for p in db.services[svc]:\n      if p.port == port and p.protocol == protocol:\n        results.append(svc)\n  return port, protocol, results", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts.keys()) + [start_token, end_token, unk_token]\n    for cmd in cmds:\n        seq1_counts_ls[cmd] += 1\n        for cmd2 in cmds:\n            seq2_counts_ls[cmd][cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts_ls.keys()) + [unk_token]\n    for param in params:\n        for cmd in cmds:\n            if cmd in cmd_param_counts_ls or cmd == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n    if not allow_zero and epsilon == 0 and delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        if secure:\n            return secrets.SystemRandom()\n        else:\n            return np.random.mtrand._rand\n    if isinstance(seed, (int, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    if isinstance(seed, secrets.SystemRandom):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n                     ' instance' % seed)", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    multiplier = np.minimum(1, clip / norms)\n    return array * multiplier[:, np.newaxis]", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        return self.fit(X).transform(X)\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for cls in cls.__mro__:\n        yield from cls.__slots__\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    qualname = func.__qualname__\n    if '<locals>' in qualname:\n        return True\n    return False", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, \"frozen\", False):\n        # frozen\n        if \"PyInstaller\" in sys.modules:\n            # PyInstaller\n            return str(Path(sys._MEIPASS).joinpath(*module.__package__.split(\".\")))\n        else:\n            # others\n            return str(Path(sys._MEIPASS).joinpath(*module.__name__.split(\".\")))\n    else:\n        # unfrozen\n        if module.__file__ is None:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n        return str(Path(module.__file__).parent)", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    def digits_of(n: float) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d * 2))\n    return checksum % 10", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain.from_iterable(d.items() for d in odicts))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = (7, 3, 1, 0, 7, 3, 1, 7, 3)\n    return weighted_modulus_digit(characters, weights, 10, True)\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum: int = 0\n    for i in range(len(value)):\n        checksum += int(value[i]) * factors[i]\n    return str(checksum % 11 % 10)", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit > 9:\n        check_digit = 0\n\n    return check_digit", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    sum_ = 0\n    for i in range(13):\n        sum_ += digits[i] * weights[i]\n    check_digit = sum_ % 11\n    if check_digit < 2:\n        return [0]\n    else:\n        return [11 - check_digit]", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        # Based on http://stackoverflow.com/q/41186818\n        return os.urandom(length)\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = max_chars\n        if min_chars > max_chars:\n            raise ValueError(\"Min must not be greater than max\")\n        if min_chars < 0:\n            raise ValueError(\"Min must not be less than zero\")\n        if max_chars < 0:\n            raise ValueError(\"Max must not be less than zero\")\n\n        return prefix + self.generator.lexify(self.generator.lexify(self.generator.lexify(\"?\" * self.random_int(min_chars, max_chars)))) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not isinstance(names, (list, tuple)):\n            names = [names]\n        for name in names:\n            object.__setattr__(self, name, msg)\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            if name in self:\n                return self[name]\n\n        if not names:\n            return list(self.values())[0]\n\n        return None", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path.rstrip(\"/\") + \"/\" + path.lstrip(\"/\")\n    else:\n        return config.requests_pathname_prefix.rstrip(\"/\") + \"/assets/\" + path.lstrip(\"/\")", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    G = {}\n    for model in models:\n        G[model] = set()\n        for fk, rel_model in model._meta.backrefs.items():\n            if rel_model in models:\n                G[model].add(rel_model)\n\n    def visit(n, visited, stack):\n        if n in visited:\n            return\n        visited.add(n)\n        for m in G[n]:\n            visit(m, visited, stack)\n        stack.append(n)\n\n    visited = set()\n    stack = []\n    for model in models:\n        visit(model, visited, stack)\n    return stack\n\n", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    if isinstance(grouping, (tuple, list)):\n        return sum(grouping_len(g) for g in grouping)\n\n    if isinstance(grouping, dict):\n        return sum(grouping_len(g) for g in grouping.values())\n\n    return 1", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n    ", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    return sha256(certificate.public_key().public_bytes(Encoding.DER, PublicFormat.PKCS1)).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return f\"Comparison of {', '.join(titles)}\"", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f} Yi{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if math.isnan(value):\n            return \"NaN%\"\n        if math.isinf(value):\n            if value > 0:\n                return \"+Inf%\"\n            else:\n                return \"-Inf%\"\n        if value == 1:\n            return \"100%\"\n        if value == 0:\n            return \"0%\"\n    return f\"{value:.1%}\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:,.{precision}f}\"\n\n", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    with np.printoptions(threshold=threshold, edgeitems=threshold):\n        return str(value)", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"<span class='badge' style='background-color:#337ab7;'>0</span>\"\n    elif value > 0:\n        return f\"<span class='badge' style='background-color:#5cb85c;'>+{value}</span>\"\n    else:\n        return f\"<span class='badge' style='background-color:#d9534f;'>{value}</span>\"\n\n", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    # Plot bars\n    wedges, _, _ = ax.pie(\n        data,\n        labels=labels,\n        autopct=\"%1.1f%%\",\n        startangle=90,\n        colors=colors,\n    )\n\n    # Make the labels on the small side, legend if necessary\n    plt.setp(wedges, width=0.3)\n    if not hide_legend:\n        ax.legend(\n            wedges,\n            labels,\n            fontsize=\"x-large\",\n            title_fontsize=\"xx-large\",\n            loc=\"center left\",\n            bbox_to_anchor=(0.9, 0, 0.5, 1),\n        )\n\n    return ax, None", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if sortby is not None:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        df = dataframe.sort_values(sortby)\n    else:\n        df = dataframe\n\n    if selected_entities is not None:\n        df = df[df[entity_column].isin(selected_entities)]\n    else:\n        df = df.head(max_entities)\n\n    df = df.pivot(\n        index=entity_column,\n        columns=df.index,\n        values=df.columns[0],\n    )\n    return df", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    ax = sns.heatmap(df, cmap=color, linewidths=0.5, linecolor=\"lightgray\", cbar=False)\n    ax.set_aspect(1)\n    ax.set_facecolor(\"white\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, ha=\"right\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n    ax.set_title(\"\")\n    return ax", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in batch.columns:\n        raise ValueError(f\"Column {name} not found in batch.\")\n\n    batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"n_distinct\"] == summary[\"n\"]:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    if summary[\"n_distinct\"] > 1:\n        batch.expect_column_values_to_be_in_set(\n            name, set(summary[\"value_counts_without_nan\"].keys())\n        )\n\n    if summary[\"n_distinct\"] > 10:\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary.get(\"min\"),\n            max_value=summary.get(\"max\"),\n        )\n\n    return name, summary, batch", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] < summary[\"p_distinct\"] * summary[\"n_values\"]:\n        batch.expect_column_values_to_be_in_set(\n            name, set(summary[\"value_counts_without_nan\"].keys())\n        )\n\n    return name, summary, batch", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if \"min\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary[\"min\"],\n            parse_strings_as_datetimes=True,\n        )\n\n    if \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            max_value=summary[\"max\"],\n            parse_strings_as_datetimes=True,\n        )\n\n    return name, summary, batch", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # Get the word counts\n    word_counts = get_word_counts(vc, stop_words)\n\n    # Sort the word counts\n    word_counts_series = word_counts\n    summary = {\n        \"n_words_distinct\": len(word_counts_series),\n        \"n_words\": np.sum(word_counts_series.values),\n        \"word_counts\": word_counts_series,\n    }\n\n    return summary\n\n", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes == 1:\n        return 0\n    else:\n        return entropy(value_counts, base=2) / log2(n_classes)", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(dict(self).values(), [])\n        return list(self)\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    try:\n        module = importlib_find(package.__name__ + \".\" + module_name)\n        return module is not None\n    except ImportError:\n        return False", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    return timezone(timedelta(minutes=offset), \"FixedOffset%(offset)d\" % {\"offset\": offset})", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    return escape_uri_path(path)", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    return Path(value)", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # Determine the number of comma-separated sections and number of words in each section for this sentence.\n    sections = random.randint(1, 10)\n    words = random.randint(3, 12)\n    # Now build this sentence.\n    s = \"\"\n    for i in range(sections):\n        if i > 0:\n            s += \", \"\n        s += \" \".join(random.sample(WORDS, words))\n    # Convert to sentence case and add end punctuation.\n    return \"%s%s%s\" % (s[0].upper(), s[1:], random.choice(\"?.\"))", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    elif sort == \"ascending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0]))\n    elif sort == \"descending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0], reverse=True))\n    else:\n        raise ValueError(\n            f\"Invalid sorting order: {sort}. It should be 'ascending', 'descending' or None.\"\n        )", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n    except ValueError:\n        return False\n    return True", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.replace(\"-\", \"+\").replace(\"_\", \"/\")\n    s = s.encode(\"ascii\")\n    try:\n        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b\"=\"))\n    except (TypeError, ValueError):\n        raise ValueError(\"Invalid base64-encoded string: %s\" % s)\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    etag_str = etag_str.strip()\n    if etag_str == \"*\":\n        return [\"*\"]\n    etags = etag_str.split(\",\")\n    return [quote_etag(etag.strip()) for etag in etags]", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n    elif pattern.startswith(\"*.\") and host.endswith(pattern[1:]):\n        return True\n    else:\n        return False\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return 'attachment; filename=\"%s\"' % filename\n    else:\n        return 'inline; filename=\"%s\"' % filename\n\n", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        ellipsis = '...'\n        half_length = (max_length - len(ellipsis)) // 2\n        return ellipsis + string[:half_length] + ellipsis + string[-half_length:]", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    try:\n        return compile(source, '<variable>', 'eval') != compile('({})'.format(source), '<variable>', 'eval')\n    except SyntaxError:\n        return True", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    orig_sys_path = sys.path\n    sys.path = orig_sys_path + list(paths)\n    try:\n        yield\n    finally:\n        sys.path = orig_sys_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    mean = np.array(mean, dtype=np.float32)\n    mean *= denominator\n\n    if img.ndim == 3 and img.shape[-1] == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        img = img[..., np.newaxis]\n\n    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img -= mean\n    img *= denominator\n    return img", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        invGamma = 1.0 / gamma\n        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n        return cv2.LUT(img, table)\n    else:\n        return np.power(img, gamma)\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    if image.dtype != np.uint8:\n        raise TypeError(\"Image must be uint8\")\n    if not is_rgb_image(image):\n        raise TypeError(\"Image must be RGB\")\n\n    img_shape = image.shape\n    tile_shape = (img_shape[0] // 8, img_shape[1] // 8)\n    for tile in tiles:\n        current_tile = image[tile[0] : tile[0] + tile_shape[0], tile[1] : tile[1] + tile_shape[1]]\n        old_tile = image[tile[2] : tile[2] + tile_shape[0], tile[3] : tile[3] + tile_shape[1]]\n        image[tile[0] : tile[0] + tile_shape[0], tile[1] : tile[1] + tile_shape[1]] = old_tile\n        image[tile[2] : tile[2] + tile_shape[0], tile[3] : tile[3] + tile_shape[1]] = current_tile\n    return image", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    height, width = rows, cols\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    matrix[0, 2] += (x - center[0]) - x\n    matrix[1, 2] += (y - center[1]) - y\n\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    angle = a + math.radians(angle)\n    scale = s * scale\n\n    return x, y, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    center = (width / 2, height / 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, angle + math.radians(angle), scale\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    two_pi = 2 * math.pi\n    angle = angle % two_pi\n    return angle\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter n must be in set {0, 1, 2, 3}\")\n    img = np.rot90(img, factor)\n    return img", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n    if param is None:\n        return tuple()\n\n    if isinstance(param, (int, float, np.ndarray)):\n        if low is None:\n            low = -param\n        if bias is None:\n            bias = param\n\n        param = cast(NumType, param + bias)\n        return (low, param) if low > param else (param, low)\n    elif isinstance(param, Sequence):\n        if low is not None:\n            return tuple(low + p for p in param)\n        elif bias is not None:\n            return tuple(p + bias for p in param)\n        else:\n            return tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar (int, float) or tuple\")", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        if not isinstance(saved_augmentations, dict):\n            raise ValueError(\"saved_augmentations must be a dictionary\")\n        if not saved_augmentations:\n            raise ValueError(\"saved_augmentations is empty\")\n        if \"transforms\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations does not contain 'transforms' key\")\n        if \"params\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations does not contain 'params' key\")\n        if \"applied\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations does not contain 'applied' key\")\n\n        transforms = saved_augmentations[\"transforms\"]\n        params = saved_augmentations[\"params\"]\n        applied = saved_augmentations[\"applied\"]\n\n        if not isinstance(transforms, list):\n            raise ValueError(\"saved_augmentations['transforms'] must be a list\")\n        if not isinstance(params, dict):\n            raise ValueError(\"saved_augmentations['params'] must be a dictionary\")\n        if not isinstance(applied, bool):\n            raise ValueError(\"saved_augmentations['applied'] must be a boolean\")\n\n        if not applied:\n            return kwargs\n\n        for transform_dict in transforms:\n            transform = ReplayCompose._restore_for_replay(transform_dict)\n            transform.params = params.get(transform_dict.get(\"id\"))\n            kwargs = transform(**kwargs)\n\n        return kwargs\n", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    return path.replace(\"\\\\\", \"/\")\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    safe_chars = RE_ARTIFACT_NAME_CHARS.sub(\"_\", name.lower())\n    deduped = RE_ARTIFACT_NAME_SEPARATOR_REPEAT.sub(\"_\", safe_chars)\n    trimmed_start = RE_ARTIFACT_NAME_SEPARATOR_START.sub(\"\", deduped)\n    trimmed = RE_ARTIFACT_NAME_SEPARATOR_END.sub(\"\", trimmed_start)\n    if len(trimmed) > 128:\n        trimmed = RE_ARTIFACT_NAME_TRIM.sub(\"...\", trimmed)\n    return trimmed if trimmed else \"artifact\"\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    d_copy = d.copy()\n    for key in unsafe_keys:\n        if key in d_copy:\n            d_copy[key] = redact_str\n    return d_copy\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    py_version = sys.version_info.major\n    py_major = str(py_version)\n    return py_major, py_version", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError\n\n    ", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    return \"\".join(secrets.choice(string.ascii_lowercase + string.digits) for _ in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        intervals = []\n        for i in sorted(console):\n            if not intervals or i > intervals[-1][1] + 1:\n                intervals.append([i, i])\n            else:\n                intervals[-1][1] = i\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n        except Exception as e:\n            wandb.termwarn(f\"Failed to get IPU devices: {e}\")\n            return\n\n        for device in devices:\n            device_metrics: Dict[str, str] = dict(device)\n            device_id = device_metrics.get(\"id\")\n            if device_id is None:\n                continue\n\n            device_name = self.name.format(device_id, device_metrics.get(\"board type\"))\n            if device_name in self._devices_called:\n                continue\n\n            metrics = device_metrics.get(\"metrics\")\n            if metrics is None:\n                continue\n\n            metrics_dict = {}\n            for key, value in metrics.items():\n                parsed_metric = self.parse_metric(key, value)\n                if parsed_metric is not None:\n                    metrics_dict[parsed_metric[0]] = parsed_metric[1]\n\n            if not metrics_dict:\n                continue\n\n            if device_name not in self._devices_called:\n                self._devices_called.add(device_name)\n                wandb.log(metrics_dict, step=0, commit=False)\n\n            if any(key in metrics_dict for key in self.variable_metric_keys):\n                wandb.log(metrics_dict, step=0, commit=False)\n\n            self.samples.append(metrics_dict)\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if not rows:\n        return []\n\n    rows = list(rows)\n    fixed_row = rows[0]\n\n    for row in rows[1:]:\n        if len(row) < len(fixed_row):\n            fixed_row = fixed_row[:len(row)]\n\n        for i, cell in enumerate(row):\n            if cell:\n                fixed_row[i] = joiner.join([fixed_row[i], cell])\n\n    return fixed_row", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    if filename.endswith('.csv'):\n        return 'csv'\n    elif filename.endswith('.dbf'):\n        return 'dbf'\n    elif filename.endswith('.fixed'):\n        return 'fixed'\n    elif filename.endswith('.xls'):\n        return 'xls'\n    elif filename.endswith('.xlsx'):\n        return 'xlsx'\n    elif filename.endswith('.js'):\n        return 'json'\n    else:\n        return None", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    return re.sub(r\"\\s+\", \"\", rendered)", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.statistics = {'generation': 'INVALID',\n                             'mutation_count': 0,\n                             'crossover_count': 0,\n                             'predecessor': 'INVALID'}", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    # remove --env <env_name> or --env=<env_name> from the command line arguments\n    cmd_args = [arg for arg in cmd_args if not re.match(r\"--env(=|\\s)\", arg)]\n    return cmd_args", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if os.name == \"nt\":\n        return \"file:///\" + path.replace(\"\\\\\", \"/\")\n    else:\n        return \"file://\" + quote(path)", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme not in [\"file\", \"\"]:\n        raise ValueError(f\"Unsupported URI scheme: {parsed_uri.scheme}\")\n    path = url2pathname(unquote(parsed_uri.path))\n    if psutil.WINDOWS:\n        path = path.lstrip(\"/\")\n    return path", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dict\")\n\n    for key, val in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"labels keys must be strings\")\n        if not isinstance(val, str):\n            raise ValueError(\"labels values must be strings\")", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [pd.DataFrame(batch.iloc[indices[i] : indices[i + 1]]) for i in range(len(indices) - 1)]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch = list(itertools.chain.from_iterable(batches))\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected string or bytes, got %s\" % type(value))", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    columns = shutil.get_terminal_size().columns\n    max_width = int(columns * scale)\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = f\" \u21b3 |{bar}| {percent}%\\r\"\n    sys.stdout.write(text)\n    sys.stdout.flush()", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    target = target or os.getcwd()\n    filename = filename or safe_filename(stream.title)\n    file_path = os.path.join(target, f\"{filename}.{stream.subtype}\")\n    print(f\"\\n{filename}.{stream.subtype}\")\n    print(f\"File size: {round(stream.filesize / 1000000, 2)} MB\")\n    print(f\"Downloading to: {file_path}\")\n    stream.download(filename=filename, output_path=target)\n    print(\"Download complete\")", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    streams = youtube.streams.filter(progressive=True)\n    for stream in streams:\n        print(stream)", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if target is None:\n        target = os.getcwd()\n    if not os.path.exists(target):\n        os.makedirs(target)\n    if media_type == \"audio\":\n        base = f\"{base}-audio\"\n    if media_type == \"video\":\n        base = f\"{base}-video\"\n    filename = f\"{base}.{subtype}\"\n    if not os.path.exists(os.path.join(target, filename)):\n        return filename\n    n = 1\n    while True:\n        new_filename = f\"{base}-{n}.{subtype}\"\n        if not os.path.exists(os.path.join(target, new_filename)):\n            return new_filename\n        n += 1", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    if not captions:\n        print(\"No captions available for this video.\")\n        return\n\n    print(\"Available captions:\")\n    for caption in captions:\n        print(f\"{caption.code} - {caption.name}\")", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable[..., GenericType]) -> Callable[..., GenericType]:\n        @functools.wraps(func)\n        def new_func(*args, **kwargs) -> GenericType:\n            warnings.simplefilter('always', DeprecationWarning)\n            warnings.warn(\n                f\"Call to deprecated function {func.__name__}. {reason}\",\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n        return new_func\n    return decorator", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]\n\n", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        output_path = os.getcwd()\n    output_path = os.path.abspath(output_path)\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    return output_path", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    ix = np.indices(shape, dtype=int)\n    ix = ix.reshape(len(arrays), -1).T\n    for n, _ in enumerate(arrays):\n        ix[:, n] = arrays[n][ix[:, n]]\n    return ix\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return pt.switch(\n            pt.lt(x, -0.6931471805599453),\n            pt.log1p(-pt.exp(x)),\n            pt.log(-pt.expm1(x)),\n        )\n    else:\n        return pt.switch(\n            pt.gt(x, 0.6931471805599453),\n            pt.log1p(-pt.exp(-x)),\n            pt.log(-pt.expm1(-x)),\n        )", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    if x < 0.693:\n        return np.log(-np.expm1(-x))\n    else:\n        return np.log1p(-np.exp(-x))", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Create a new InferenceData object with the same data and attributes as the input\n    new_idata = arviz.InferenceData(**idata.to_dict())\n\n    # Iterate over the groups in the new InferenceData object\n    for group in new_idata.groups():\n        # Check if the group has a \"sample_stats\" attribute\n        if hasattr(new_idata[group], \"sample_stats\"):\n            # Get the \"sample_stats\" attribute\n            sample_stats = getattr(new_idata[group], \"sample_stats\")\n            # Check if the \"sample_stats\" attribute has a \"warning\" column\n            if \"warning\" in sample_stats.columns:\n                # Drop the \"warning\" column from the \"sample_stats\" attribute\n                setattr(new_idata[group], \"sample_stats\", sample_stats.drop(columns=[\"warning\"]))\n\n    # Return the new InferenceData object\n    return new_idata", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    stop_at_vars = set(stop_at_vars) if stop_at_vars is not None else set()\n    seen = set()\n    for graph in graphs:\n        for var in walk(\n            [graph],\n            expand=expand_fn,\n            stop_at=stop_at_vars,\n            include_start=True,\n            stop_at_subgraphs=True,\n        ):\n            if var not in seen:\n                seen.add(var)\n                yield var", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    else:\n        return float32", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def new_func(x, args):\n        if args is None:\n            return func(x)\n        else:\n            return func(x, *args)\n\n    return new_func", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    if X.ndim == 1:\n        X = X[:, None]\n    if X.ndim != 2:\n        raise ValueError(\"X must be 1D or 2D\")\n\n    # Initialize inducing points\n    if \"obs\" in kmeans_kwargs:\n        warnings.warn(\n            \"Observed data is not used for initializing inducing points. \"\n            \"Remove `obs` from `kmeans_kwargs`.\",\n            UserWarning,\n        )\n    kmeans_kwargs[\"obs\"] = X\n    if \"minit\" in kmeans_kwargs:\n        warnings.warn(\n            \"Initialization points are not used for initializing inducing points. \"\n            \"Remove `minit` from `kmeans_kwargs`.\",\n            UserWarning,\n        )\n    kmeans_kwargs[\"minit\"] = \"points\"\n    if \"maxiter\" not in kmeans_kwargs:\n        kmeans_kwargs[\"maxiter\"] = 100\n    if \"thresh\" not in kmeans_kwargs:\n        kmeans_kwargs[\"thresh\"] = 1e-05\n\n    # Run kmeans\n    centroids, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n\n    # Scale centroids\n    centroids *= np.sqrt(X.shape[0] / n_inducing)\n\n    return centroids", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    return X.astype(pytensor.config.floatX)", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p <= 0:\n        raise ValueError(\"p must be greater than 0\")\n    if p == 1:\n        return pt.sum(pt.log(a))\n    else:\n        return pt.sum(pt.log(pt.gamma(a))) - pt.log(pt.gamma(pt.sum(a)))", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    # Find the deterministics that depend directly on observed variables\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        for input_node in deterministic.owner.inputs:\n            if input_node in observed_rvs or input_node in basic_rvs:\n                observed_dependent_deterministics.append(deterministic)\n                break\n\n    return observed_dependent_deterministics", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    n = len(weights)\n    positions = (rng.random() + np.arange(n)) / n\n    index = 0\n    new_indices = np.zeros(n, dtype=int)\n    cumulative_sum = weights[0]\n    for i in range(n):\n        while positions[i] > cumulative_sum:\n            index += 1\n            cumulative_sum += weights[index]\n        new_indices[i] = index\n    return new_indices", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results, axis=0)\n    if squeeze:\n        results = np.squeeze(results)\n    return results", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        log_value = pt.log(value)\n        sum_log_value = pt.sum(log_value, -1, keepdims=True)\n        return log_value - sum_log_value", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1] + 1\n        N = N.astype(value.dtype)\n        sum_value = pt.sum(value, -1, keepdims=True)\n        value_sum_expanded = value + sum_value\n        value_sum_expanded = pt.concatenate([value_sum_expanded, pt.zeros(sum_value.shape)], -1)\n        logsumexp_value_expanded = pt.logsumexp(value_sum_expanded, -1, keepdims=True)\n        res = pt.exp(value_sum_expanded - logsumexp_value_expanded)\n        return res[..., :-1]", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    stop_at_vars = set() if stop_at_vars is None else set(stop_at_vars)\n    stop_at_vars.update(graph_inputs(graphs))\n\n    def expand(var):\n        if var in stop_at_vars:\n            return []\n        if not walk_past_rvs and isinstance(var.owner.op, MeasurableVariable):\n            return []\n        return expand_fn(var)\n\n    for var in walk(graphs, expand, False):\n        yield var\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\n                \"steps\": [],\n                \"timestamps\": [],\n                \"values\": [],\n            }\n        metrics[metric.name][\"steps\"].append(metric.step)\n        metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n        metrics[metric.name][\"values\"].append(metric.value)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    if not path:\n        return\n    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = {}\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # TODO: T\n    # This is a really dirty approach, because it effectively assumes that all\n    # sampling is being removed, and, thus, all shared updates relating to\n    # `RandomType`s.  Instead, we should be more precise and only remove the\n    # `RandomType`s associated with `values`.\n    scan_args.outer_in_shared = [\n        i for i in scan_args.outer_in_shared if not isinstance(i.type, RandomType)\n    ]\n    scan_args.inner_in_shared = [\n        i for i in scan_args.inner_in_shared if not isinstance(i.type, RandomType)\n    ]\n    scan_args.inner_out_shared = [\n        i for i in scan_args.inner_out_shared if not isinstance(i.type, RandomType)\n    ]\n    # XXX TODO: Remove this properly\n    # scan_args.outer_out_shared = []\n\n    scan_op = Scan(\n        scan_args.inner_inputs,\n        scan_args.inner_outputs,\n        scan_args.info,\n        mode=scan_args.mode,\n    )\n    scan_node = scan_op.make_node(*scan_args.outer_inputs)\n\n    return scan_node.outputs, scan_node.updates", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    if pre_path == path:\n        return True\n    if pre_path.startswith(path):\n        return pre_path[len(path)] == \".\"\n    return False", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n    return subclasses", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    return pkgutil.find_loader(modname) is not None", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.splitlines(True)\n    for i, line in enumerate(lines):\n        while \"\\b\" in line:\n            idx = line.index(\"\\b\")\n            line = line[:idx - 1] + line[idx + 1:]\n        while \"\\n\" in line:\n            idx = line.index(\"\\n\")\n            line = line[:idx] + line[idx + 1:]\n        lines[i] = line\n    return \"\".join(lines)", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.render_doc(command, renderer=pydoc.plaintext)\n    help_text = re.sub(r\"\\x08+\", \"\", help_text)\n    return help_text", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            pass\n    return False, None", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\"):\n        return pyc_name\n    elif pyc_name.endswith(\".so\") or pyc_name.endswith(\".pyd\") or pyc_name.endswith(\n        \".ipynb\"\n    ):\n        return pyc_name[:-1]\n    elif os.path.exists(pyc_name[:-1]):\n        return pyc_name[:-1]\n    else:\n        return pyc_name", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for k, v in iterable.items():\n                    self[k] = v\n            else:\n                for k, v in iterable:\n                    self[k] = v\n        for k, v in kwargs.items():\n            self[k] = v", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return line == \"\" or line.startswith(\"#\")", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError('orig must be a function, not %r' % orig)\n    if not copy_dict:\n        return orig\n    return FunctionType(orig.__code__, orig.__globals__, orig.__name__, orig.__defaults__, orig.__closure__)", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    else:\n        return line", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kw:\n        raise TypeError('unexpected kwargs: %r' % kw.keys())\n\n    if args and kwargs:\n        raise ValueError('cannot specify both args and kwargs')\n\n    if args:\n        args_str = ', '.join(repr(arg) for arg in args)\n    else:\n        args_str = ''\n\n    if kwargs:\n        kwargs_str = ', '.join('%s=%r' % (k, v) for k, v in kwargs)\n    else:\n        kwargs_str = ''\n\n    if args_str and kwargs_str:\n        args_str += ', '\n\n    return '%s(%s%s)' % (name, args_str, kwargs_str)", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        self.insert(dest_index, self.pop(item_index))\n\n    ", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    out = StringIO()\n    with GzipFile(fileobj=out, mode='wb', compresslevel=level) as f:\n        f.write(bytestring)\n    return out.getvalue()", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version\n    try:\n        return uuid.UUID(obj, version=version).version == version\n    except ValueError:\n        return False\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    # Split the input string into a list of integers and ranges.\n    range_list = range_string.split(delim)\n\n    # Initialize an empty list to store the integers.\n    int_list = []\n\n    # Iterate over the list of integers and ranges.\n    for range_item in range_list:\n        # Check if the range item contains a range delimiter.\n        if range_delim in range_item:\n            # Split the range item into a list of start and end integers.\n            start, end = range_item.split(range_delim)\n            # Convert the start and end integers to integers.\n            start = int(start)\n            end = int(end)\n            # Add the integers in the range to the list.\n            int_list.extend(range(start, end + 1))\n        else:\n            # Convert the range item to an integer and add it to the list.\n            int_list.append(int(range_item))\n\n    # Return the sorted list of integers.\n    return sorted(int_list)", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not step:\n        raise ValueError('step must be non-zero')\n    if start is None:\n        start, stop = 0.0, stop * 1.0\n    else:\n        # swap when all args are used\n        stop, start = start * 1.0, stop * 1.0\n    cur = start\n    while cur < stop:\n        yield cur\n        cur += step\n\n    if count is None:\n        count = int(math.ceil((stop - start) / step))\n    elif count == 'repeat':\n        count = None\n    else:\n        count = int(count)\n\n    if jitter:\n        if jitter is True:\n            jitter = 1.0\n        elif jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n\n    if count is not None:\n        for i in xrange(count):\n            if jitter:\n                cur = cur + (cur * jitter)\n            yield cur\n            cur = cur * factor\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def cached_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_decorator", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return td.days * 86400.0 + td.seconds + td.microseconds / 1000000.0", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    if not isinstance(type_obj, type):\n        raise TypeError('type_obj must be a type')\n\n    if not include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n\n    # TODO: this is a bit of a hack, but it's the only way to get\n    #       subclasses of builtins, and it's fast enough\n    if type_obj.__module__ == '__builtin__':\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]\n\n    return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if step == 0:\n        raise ValueError('step must not be zero')\n    if stop is None:\n        while True:\n            yield start\n            start += step\n    else:\n        if step > 0:\n            op = operator.lt\n        else:\n            op = operator.gt\n        while op(start, stop):\n            yield start\n            start += step\n        if inclusive and start == stop:\n            yield start\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(min(x, upper), lower)\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceiling options greater than or equal to: %r\" % x)\n    return options[i]", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    pos_args, kw_args = [], []\n    for lit, field_str in split_format_str(fstr):\n        if field_str is None:\n            continue\n        field = BaseFormatField(*field_str[1:-1].split(':'))\n        if field.is_positional:\n            pos_args.append((field.fname, field.type_func))\n        else:\n            kw_args.append((field.fname, field.type_func))\n    return pos_args, kw_args", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key in self:\n            return self[key]\n        self[key] = default\n        return default\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if type(dict_or_iterable) is dict:\n            for key, val in dict_or_iterable.items():\n                self[key] = val\n        else:\n            for key, val in dict_or_iterable:\n                self[key] = val\n        for key, val in kw.items():\n            self[key] = val\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return frozenset(self.data.get(key, default))\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        new = self.copy()\n        new.update(*a, **kw)\n        return new\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict([(k, d[k]) for k in keep if k not in drop])", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return f'{cn}({dict_repr})'\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    if isinstance(val, str):\n        try:\n            module_name, obj_name = val.split(':', 1)\n        except ValueError:\n            raise TypeError(\"Value must be a string in the form 'module:object'\")\n\n        try:\n            module = importlib.import_module(module_name)\n        except ImportError:\n            raise TypeError(\"Module %s not found\" % module_name)\n\n        try:\n            obj = getattr(module, obj_name)\n        except AttributeError:\n            raise TypeError(\"Object %s not found in module %s\" % (obj_name, module_name))\n\n        val = obj\n\n    if not callable(val):\n        raise TypeError(\"Value must be a callable object\")\n\n    if arity != -1 and util.get_arity(val) != arity:\n        raise TypeError(\"Value must have an arity of: %d\" % arity)\n\n    return val", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    path = os.path.join(util.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(path):\n        return path\n    return None\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if os.environ.get('LISTEN_PID') != str(os.getpid()):\n        return 0\n\n    if unset_environment:\n        os.environ.pop('LISTEN_PID')\n        os.environ.pop('LISTEN_FDS')\n\n    return int(os.environ.get('LISTEN_FDS', 0))", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith(\"unix:\"):\n        # unix socket\n        path = netloc[5:]\n        if not os.path.exists(path):\n            raise ValueError(\"unix socket path %r does not exist\" % path)\n        return path\n    elif netloc.startswith(\"fd:\"):\n        # file descriptor\n        fd = int(netloc[3:])\n        return fd\n    elif netloc.startswith(\"fd://\"):\n        # file descriptor URI\n        fd = int(netloc[5:])\n        return fd\n    elif netloc.startswith(\"tcp:\"):\n        # tcp address\n        netloc = netloc[4:]\n    elif netloc.startswith(\"tcp://\"):\n        # tcp address URI\n        netloc = netloc[6:]\n    else:\n        # tcp address with no prefix\n        pass\n\n    # parse the address\n    if \":\" in netloc:\n        host, port = netloc.split(\":\", 1)\n        if not port.isdigit():\n            raise ValueError(\"port must be an integer\")\n        port = int(port)\n    else:\n        host = netloc\n        port = int(default_port)\n\n    if not host:\n        host = \"0.0.0.0\"\n\n    return host, port", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, str):\n        return value.encode(encoding)\n    return value", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    warnings.warn(msg, RuntimeWarning, stacklevel=3)", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n\n    return urllib.parse.urlsplit(uri)", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if self.has_next_page:\n            return self.end_cursor\n        return None\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    known_permissions = set(known_permissions)\n    permitted_permissions = set(permissions)\n    if not permitted_permissions.issubset(known_permissions):\n        raise ValueError(\n            \"The following permissions are unknown: {}\".format(\n                permitted_permissions.difference(known_permissions)\n            )\n        )\n    permitted_permissions = OrderedDict.fromkeys(permissions)\n    if permitted_permissions.pop(\"+all\", None):\n        return \"+all\"\n    if permitted_permissions.pop(\"-all\", None):\n        return \"-all\"\n    for permission in list(permitted_permissions):\n        if permission.startswith(\"+\"):\n            permitted_permissions[permission[1:]] = True\n        elif permission.startswith(\"-\"):\n            permitted_permissions[permission[1:]] = False\n        else:\n            permitted_permissions[permission] = True\n    if permitted_permissions.pop(\"+all\", None):\n        return \"+all\"\n    if permitted_permissions.pop(\"-all\", None):\n        return \"-all\"\n    return \",\".join(\n        \"+{}\".format(permission) if value else \"-{}\".format(permission)\n        for permission, value in permitted_permissions.items()\n    )", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            return json.dumps(self.data_out, indent=self.json_indent, separators=self.json_separators, ensure_ascii=self.ascii_only)\n\n        return json.dumps(self.data_out, separators=self.json_separators, ensure_ascii=self.ascii_only)", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        # This is a URL.\n        # Transform to pip-compatible format:\n        # \"https://github.com/pyinstaller/pyinstaller/archive/develop.zip\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@develop\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@v3.3\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip#egg=pyinstaller\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@v3.3#egg=pyinstaller\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip#egg=pyinstaller-v3.3\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@v3.3#egg=pyinstaller-v3.3\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip#egg=pyinstaller-v3.3-py3.6\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@v3.3#egg=pyinstaller-v3.3-py3.6\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip#egg=pyinstaller-v3.3-py3.6-win32\"\n        # -> \"git+https://github.com/pyinstaller/pyinstaller@v3.3#egg=pyinstaller-v3.3-py3.6-win32\"\n        # \"https://github.com/pyinstaller/pyinstaller/archive/v3.3.zip#egg=pyinstaller-", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    deps = [dep.lower() if isinstance(dep, str) else dep for dep in deps]\n    deps = [dep if isinstance(dep, tuple) else tuple(dep) for dep in deps]\n    return deps", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        # Remove invalid directories\n        for invalid_dir in invalid_dir_names:\n            if invalid_dir in dirs:\n                dirs.remove(invalid_dir)\n\n        # Yield valid files\n        for file in files:\n            full_path = join(root, file)\n            if not any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                yield full_path", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    a_priority = default_recipe_priorities.index(a) if a in default_recipe_priorities else 0\n    b_priority = default_recipe_priorities.index(b) if b in default_recipe_priorities else 0\n    if a_priority != b_priority:\n        return a_priority - b_priority\n    return cmp(a, b)", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstrap_dir = join(cls.ctx.root_dir, 'bootstraps')\n        bootstrap_files = [\n            f for f in listdir(bootstrap_dir)\n            if isdir(join(bootstrap_dir, f)) and not f.startswith('_')\n        ]\n        return set(bootstrap_files)\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError(f'The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    print(message)\n    sys.exit(1)", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.engine.dialect.server_version_info\n    if server_version is None:\n        return 'pid'\n    version = '%s.%s' % (server_version[0], server_version[1])\n    if version == '9.2':\n        return 'procpid'\n    else:\n        return 'pid'", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if not isinstance(s, str):\n        return s\n\n    res = []\n    b64_buffer = []\n    for c in s:\n        if ord(c) & 0x80:\n            if b64_buffer:\n                res.append(base64_utf7_encode(b64_buffer))\n                b64_buffer = []\n            res.append(b\"&\" + c.encode(\"utf-16be\").hex().upper().encode(\"ascii\"))\n        elif c in (\"&\", \"+\", \"-\"):\n            if b64_buffer:\n                res.append(base64_utf7_encode(b64_buffer))\n                b64_buffer = []\n            res.append(b\"&\" + c.encode(\"ascii\"))\n        elif c == \"\\\\\":\n            res.append(b\"&-\")\n        else:\n            b64_buffer.append(c)\n\n    if b64_buffer:\n        res.append(base64_utf7_encode(b64_buffer))\n\n    return b\"\".join(res)", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    v = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        v += \"-\" + releaselevel\n    return v", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = server_nonce.to_bytes(16, 'little')\n    new_nonce = new_nonce.to_bytes(32, 'little')\n\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\n\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"msg\" in response and \"result\" in response and response[\"result\"] == \"error\":\n        if hasattr(controller, \"view\"):\n            controller.view.report_error(response[\"msg\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n\n    ", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        validation_error = self._validate_narrow_link(parsed_link)\n        if validation_error:\n            self.view.footer.show_error_message(validation_error)\n        else:\n            self._switch_narrow_to(parsed_link)", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    class NewEnum(Enum):\n        pass\n\n    for color in colors:\n        for p in prop:\n            setattr(NewEnum, f\"{color.name}_{p}\", f\"{color.value}_{p}\")\n\n    return NewEnum", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    try:\n        return Decimal(d, context=BasicContext)\n    except (TypeError, ValueError):\n        return d", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except (TypeError, ValueError):\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    try:\n        return json.dumps(obj)\n    except (TypeError, ValueError):\n        return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    if lst == values.unset:\n        return lst\n    return [serialize_func(e) for e in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                \"{} is a deprecated method. Please use {} instead.\".format(\n                    func.__name__, new_func.__name__\n                ),\n                DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if new_func is None:\n        return deprecated_method_wrapper\n    else:\n        return deprecated_method_wrapper(new_func)", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    return sample(array, nb_items)", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text == 'True':\n        return True\n    elif text == 'False':\n        return False\n    else:\n        raise ValueError(\"The input string is not 'True' or 'False'.\")", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if word.endswith('\"') and (len(word) < 2 or word[-2] != '\\\\'):\n            return True\n        return False\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        if len(self.command_tokens) < 2:\n            self.print_wrapper.print_error(\n                \"Not enough arguments for the command.\"\n            )\n            return\n        unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.print_error(\n                \"Unknown unit type: \" + self.command_tokens[1]\n            )\n            return\n        if len(self.command_tokens) == 2:\n            self.execute_on_all_units(unit_type)\n        else:\n            unit_name = self.command_tokens[2]\n            if unit_name.startswith('\"'):\n                unit_name = CommandStrategy.remove_quotes(unit_name)\n            regex = self.get_regex_name(unit_name)\n            if regex is not None:\n                for unit_name in self.next_matching_unit_name(\n                    unit_type, regex\n                ):\n                    self.execute_on_unit(unit_type, unit_name)\n            else:\n                splitted_unit_name = CommandStrategy.split_exact_unit_name(\n                    unit_name\n                )\n                self.execute_on_unit(\n                    unit_type, splitted_unit_name[0], splitted_unit_name[1]\n                )\n        self.finish_execution()\n", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    records = sorted(all_records, key=lambda x: x[0])\n    groups = itertools.groupby(records, key=lambda x: x[0])\n    for prio, group in groups:\n        group = list(group)\n        rng.shuffle(group)\n        for _, _, (host, port) in group:\n            yield host, port", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except IndexError:\n            return default", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            if hasattr(ssl_context, \"set_alpn_protocols\"):\n                ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            verifier.setup_context(ssl_context, transport)\n            return ssl_context\n\n        return context_factory", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        path.append(el.tag)\n        el = el.getparent()\n    path.reverse()\n    return \"/\".join(path)", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if s[0] == \"@\":\n            raise ValueError(\"JID must not start with @\")\n\n        if s[-1] == \"/\":\n            raise ValueError(\"JID must not end with /\")\n\n        if \"@\" in s:\n            localpart, s = s.split(\"@\", 1)\n        else:\n            localpart = None\n\n        if \"/\" in s:\n            domain, resource = s.split(\"/\", 1)\n        else:\n            domain = s\n            resource = None\n\n        return cls(localpart, domain, resource, strict=strict)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    subject = x509.get_subject()\n    subject_dict = {}\n    for i in range(subject.get_components()):\n        component = subject.get_component(i)\n        subject_dict[component.get_short_name().decode(\"utf-8\")] = component.get_data().decode(\"utf-8\")\n\n    subject_alt_name = x509.get_ext_by_oid(b\"subjectAltName\")\n    subject_alt_name_dict = {}\n    if subject_alt_name is not None:\n        subject_alt_name_dict = {}\n        for i in range(subject_alt_name.get_components()):\n            component = subject_alt_name.get_component(i)\n            subject_alt_name_dict[component.get_short_name().decode(\"utf-8\")] = component.get_data().decode(\"utf-8\")\n\n    return {\n        \"subject\": subject_dict,\n        \"subjectAltName\": subject_alt_name_dict\n    }", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return x509.to_cryptography().public_bytes(\n        encoding=serialization.Encoding.DER\n    )", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(\n        blob,\n        asn1Spec=pyasn1_modules.rfc2459.Certificate()\n    )", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1_struct.getComponentByName(\"tbsCertificate\").getComponentByName(\"subjectPublicKeyInfo\").getComponentByName(\"subjectPublicKey\").asOctets()", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        loop = asyncio.get_event_loop() if loop is None else loop\n        return functools.partial(cls._async_wrapper, f, loop)\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be coroutine, got {f}\".format(f=f))\n            return functools.partial(cls._spawn_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut.set_result)\n    return fut", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        if not __groups:\n            __groups = {()}\n        if self.default_limit is not None:\n            __groups.add(())\n        for group in __groups:\n            limit = self.get_limit(group)\n            if limit is not None and self.get_task_count(group) >= limit:\n                raise RuntimeError(\n                    \"limit for group {group} exhausted\"\n                    .format(group=group)\n                )\n        task = asyncio.create_task(__coro_fun(*args, **kwargs))\n        task.add_done_callback(self._task_done)\n        for group in __groups:\n            self._group_tasks.setdefault(group, set()).add(task)\n        return task\n", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    fut = xmlstream.error_future()\n    xmlstream.send_xso(send)\n    if timeout is not None:\n        try:\n            result = await asyncio.wait_for(fut, timeout=timeout)\n        except asyncio.TimeoutError:\n            raise TimeoutError from None\n    else:\n        result = await fut\n    if cb is not None:\n        cb(result)\n    return result", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n    coroutine_task = loop.create_task(coroutine)\n    peer_coroutine_task = loop.create_task(peer_coroutine)\n    done, pending = loop.run_until_complete(\n        asyncio.wait(\n            [coroutine_task, peer_coroutine_task],\n            timeout=timeout,\n            return_when=asyncio.ALL_COMPLETED))\n    if not done:\n        raise asyncio.TimeoutError()\n    if coroutine_task.exception():\n        raise coroutine_task.exception()\n    if peer_coroutine_task.exception():\n        raise peer_coroutine_task.exception()\n    return coroutine_task.result()", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for signal_name in dir(instance):\n        signal = getattr(instance, signal_name)\n        if isinstance(signal, callbacks.Signal):\n            setattr(listener, signal_name, unittest.mock.Mock())\n    return listener", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.deepcopy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(self.eval(expr))", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        return self.eval_leaf(ec)", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n        if depth == 0:\n            return", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        try:\n            ev = yield\n        except GeneratorExit:\n            try:\n                dest.close()\n            except StopIteration:\n                pass\n            raise\n        except:\n            dest.close()\n            raise\n        else:\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            dest.send(ev)\n    return (yield from dest)", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            ev = yield\n            dest.append(ev)\n            try:\n                receiver.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        dest.clear()", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for event in events:\n        if event[0] == \"start\":\n            dest.startElementNS(event[1], event[2], event[3])\n        elif event[0] == \"text\":\n            dest.characters(event[1])\n        elif event[0] == \"end\":\n            dest.endElementNS(event[1], event[2])\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = list(\n        escape(identity.category).encode(\"utf-8\") + b\"/\" +\n        escape(identity.type_).encode(\"utf-8\") + b\"/\" +\n        escape(identity.lang).encode(\"utf-8\") + b\"/\" +\n        escape(identity.name).encode(\"utf-8\")\n        for identity in identities\n    )\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [escape(feature).encode(\"utf-8\") for feature in features]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = list(escape(form).encode(\"utf-8\") for form in forms)\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\"hashes\") / urllib.parse.quote(self.node, safe=\"\")\n\n    ", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    return aioxmpp.hashes.calculate_hash(algo, hash_input)", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"urn:xmpp:caps:{}#{}\".format(\n            self.algo,\n            base64.b32encode(self.digest).decode(\"ascii\").rstrip(\"=\").lower()\n        )\n\n    ", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return pathlib.PurePosixPath(\n            \"caps\",\n            \"{}.{}\".format(\n                self.algo,\n                base64.b64encode(self.digest).decode(\"ascii\")\n            )\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is None:\n            return\n        for algo, digest in presence.xep0390_caps.digests.items():\n            yield Key(algo, digest)", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        del self._value\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    _random_states.clear()\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        return numpy.float32\n    if isinstance(dtype, numpy.dtype):\n        if dtype.kind not in '?bBiuf':\n            raise TypeError('Unsupported data type: {}'.format(dtype))\n        return dtype\n    if isinstance(dtype, type):\n        if dtype is bool:\n            return numpy.bool_\n        if dtype is int:\n            return numpy.int32\n        if dtype is float:\n            return numpy.float32\n        if dtype is numpy.bool_:\n            return numpy.bool_\n        if dtype is numpy.int32:\n            return numpy.int32\n        if dtype is numpy.float32:\n            return numpy.float32\n        raise TypeError('Unsupported data type: {}'.format(dtype))\n    raise TypeError('Unsupported data type: {}'.format(dtype))\n\n", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    sources_filtered = []\n    sources_rest = []\n    for src in sources:\n        if src.endswith(extension):\n            sources_filtered.append(src)\n        else:\n            sources_rest.append(src)\n    return sources_filtered, sources_rest", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    with open(filename, \"rb\") as f:\n        return pa.ipc.open_file(f).read_all()\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    in_memory_stream = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(in_memory_stream)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if not arr:\n        raise IndexError(\"Array is empty\")\n    if x < arr[0]:\n        raise IndexError(\"Query is smaller than the smallest element of the array\")\n    if x >= arr[-1]:\n        raise IndexError(\"Query is greater than or equal to the largest element of the array\")\n    lo, hi = 0, len(arr) - 1\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if x < arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return lo - 1\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    # We just need to check if every special part from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the path and the pattern have the same number of special parts.\n    special_directories_in_path = [\n        part for part in PurePath(matched_rel_path).parts if part.startswith(\"__\") and not set(part) == {\"__\"}\n    ]\n    special_directories_in_pattern = [\n        part for part in PurePath(pattern).parts if part.startswith(\"__\") and not set(part) == {\"__\"}\n    ]\n    return len(special_directories_in_path) != len(special_directories_in_pattern)", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every hidden directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of hidden directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    return [{col: example[col] for col in batch} for example in zip(*batch.values())]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = OrderedDict()\n    for example in examples:\n        for column_name, value in example.items():\n            if column_name not in columns:\n                columns[column_name] = []\n            columns[column_name].append(value)\n    return {column_name: values for column_name, values in columns.items()}\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.choice(num_sources, size=random_batch_size, p=p))\n", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n\n        if self._info:\n            info = copy.deepcopy(self._info)\n            if self._info.features is not None:\n                for column_name in column_names:\n                    if column_name not in self._info.features:\n                        raise ValueError(\n                            f\"Column name {column_name} not in the \"\n                            \"dataset. Columns in the dataset: \"\n                            f\"{list(self._info.features.keys())}.\"\n                        )\n                info.features = Features({c: info.features[c] for c in info.features if c not in column_names})\n                # check that it's still valid, especially with regard to task templates\n                try:\n                    info.copy()\n                except ValueError:\n                    info.task_templates = None\n\n        ex_iterable = RemoveColumnsIterable(self._ex_iterable, column_names)\n        return IterableDataset(\n            ex_iterable=ex_iterable,\n            info=info,\n            split=self._split,\n            formatting=self._formatting,\n            shuffling=self._shuffling,\n            distributed=self._distributed,\n            token_per_repo_id=self._token_per_repo_id,\n        )", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        for dataset in self.values():\n            dataset.set_format(type, columns, output_all_columns, **format_kwargs)\n        return self\n\n    ", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n\n    ", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n\n    ", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if \"://\" in dataset_path:\n        dataset_path = dataset_path.split(\"://\")[1]\n    return dataset_path", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    if isinstance(fs, fsspec.FSMap):\n        fs = fs.fs\n    if isinstance(fs, fsspec.FunctionFileSystem):\n        fs = fs.fs\n    if isinstance(fs, fsspec.CachedFileSystem):\n        fs = fs.target\n    if isinstance(fs, fsspec.SubFileSystem):\n        fs = fs.target\n    if isinstance(fs, fsspec.core.url_to_fs):\n        # Special case where the protocol is passed after `://`\n        # For instance `gcs://<your-bucket-name>`\n        protocol = fs.fs_class.protocol\n    else:\n        protocol = fs.protocol\n    return protocol in [\"http\", \"https\", \"webdav+http\", \"webdav+https\", \"s3\", \"gs\"]", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n\n    return filename", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        # Old version of huggingface_hub don't handle special characters in path\n        path = quote(path, safe=\"/:\")\n    url = hfh.hf_hub_url(repo_id=repo_id, filename=path, revision=revision)\n    return url", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    num_shards = None\n    for value in gen_kwargs.values():\n        if isinstance(value, list):\n            if num_shards is None:\n                num_shards = len(value)\n            elif num_shards != len(value):\n                raise ValueError(\n                    \"The input gen_kwargs contain lists of different lengths. Please provide a gen_kwargs with lists of the same length.\"\n                )\n    return num_shards or 1", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards < max_num_jobs:\n        return [range(i, i + 1) for i in range(num_shards)]\n    else:\n        shards_per_job = num_shards // max_num_jobs\n        remaining_shards = num_shards % max_num_jobs\n        shard_indices_per_group = [\n            range(i * shards_per_job + min(i, remaining_shards), (i + 1) * shards_per_job + min(i + 1, remaining_shards))\n            for i in range(max_num_jobs)\n        ]\n        return shard_indices_per_group", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    try:\n        old_value = getattr(obj, attr)\n        setattr(obj, attr, value)\n        yield\n    finally:\n        setattr(obj, attr, old_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        with tarfile.open(input_path, \"r:*\") as archive:\n            archive.extractall(output_path, members=cls.safemembers(archive, output_path))", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n\n    ", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return asdict(obj)\n    elif isinstance(obj, dict):\n        return {k: asdict(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [asdict(v) for v in obj]\n    elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n        return type(obj)(*[asdict(v) for v in obj])\n    elif isinstance(obj, tuple):\n        return tuple(asdict(v) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME not in dataset_card_data:\n            return cls()\n        metadata_configs = dataset_card_data[cls.FIELD_NAME]\n        if not isinstance(metadata_configs, list):\n            raise ValueError(f\"Expected {cls.FIELD_NAME} in dataset card data to be a list, but got {metadata_configs}\")\n        metadata_configs_dict = {}\n        for metadata_config in metadata_configs:\n            if not isinstance(metadata_config, dict):\n                raise ValueError(\n                    f\"Expected {cls.FIELD_NAME} in dataset card data to be a list of dicts, but got {metadata_configs}\"\n                )\n            config_name = metadata_config.pop(\"config_name\", None)\n            if config_name is None:\n                raise ValueError(f\"Expected {cls.FIELD_NAME} in dataset card data to have a 'config_name' key\")\n            metadata_configs_dict[config_name] = metadata_config\n        return cls(metadata_configs_dict)\n", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_dict_paths = _lang_dict_paths()\n    if lang in lang_dict_paths:\n        return lang_dict_paths[lang]\n    else:\n        raise ValueError(\n            \"Unknown language code: %r. \"\n            \"Please use one of the following codes: %r\" % (\n                lang, list(lang_dict_paths.keys())\n            )\n        )", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\n            \"DAWG-Python is not available. \"\n            \"Install the Cython version to get 100x speedup.\"\n        )", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = None\n    paradigm = []\n\n    for word_form, tag in lexeme:\n        prefix = ''\n        suffix = ''\n        for i in range(1, len(word_form)):\n            if word_form[:i] in paradigm_prefixes:\n                prefix = word_form[:i]\n                suffix = word_form[i:]\n                break\n        else:\n            stem = ''\n            break\n\n        if not stem:\n            stem = suffix\n\n        paradigm.append((suffix, tag, prefix))\n\n    if not paradigm:\n        paradigm = [('', '', '')]\n\n    return stem, paradigm", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result\n\n    ", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    item = d\n    for key in keys:\n        try:\n            item_key, item_value = _get_item_key_and_value(item, key)\n            item = item_value\n        except (IndexError, KeyError):\n            return (None, None, None)\n    return (item, key, item_value)", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys)\n    _set_item_value(item, keys[-1], value)", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not key or not key.endswith(\"]\"):\n        return [key]\n    if not key.startswith(\"[\"):\n        return [key]\n    indexes = re.findall(KEY_INDEX_RE, key)\n    if not indexes:\n        return [key]\n    keys = []\n    for index in indexes:\n        key = key.replace(f\"[{index}]\", \"\")\n        keys.append(key)\n        keys.append(int(index))\n    return keys", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not base:\n        return rel\n    if not rel:\n        return base\n    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n    uri = _urljoin(base, rel)\n    scheme = urllib.parse.urlsplit(uri)[0]\n    if scheme and scheme.lower() not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, io.IOBase):\n        return url_file_stream_or_string\n\n    if isinstance(url_file_stream_or_string, str):\n        if url_file_stream_or_string.startswith('http://') or url_file_stream_or_string.startswith('https://'):\n            # URL\n            url = url_file_stream_or_string\n            if not handlers:\n                handlers = []\n            opener = urllib.request.build_opener(*handlers)\n            request = urllib.request.Request(url)\n            if etag:\n                request.add_header('If-None-Match', etag)\n            if modified:\n                if isinstance(modified, tuple):\n                    modified = _parse_date(modified)\n                request.add_header('If-Modified-Since', modified)\n            if agent:\n                request.add_header('User-Agent', agent)\n            if referrer:\n                request.add_header('Referer', referrer)\n            if request_headers:\n                for header, value in request_headers.items():\n                    request.add_header(header, value)\n            try:\n                response = opener.open(request)\n            except urllib.error.HTTPError as e:\n                if e.code == 304:\n                    # Not Modified\n                    result['status'] = 304\n                    result['etag'] = e.headers.get('etag')\n                    result['modified'] = e.headers.get('last-modified')\n                    return None\n                else:\n                    raise\n            result['status'] = response.status\n            result['etag'] = response.headers.get('etag')\n            result['modified'] = response.headers.get('last-modified')\n            result['encoding'] = response.headers.get_content_charset()\n            result['headers'] = dict(response.headers)\n            return response\n        else:\n            # File or string\n            try:\n                return open(url_file", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', 'Basic ' + auth)\n    for key, value in request_headers.items():\n        request.add_header(key, value)\n    return request", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n    elif callable(mapper):\n        mapper = [mapper]\n\n    if not isinstance(mapper, list):\n        raise ValueError(\"mapper must be a callable or a list of callables\")\n\n    l_ = []\n    for item in l:\n        for m in mapper:\n            item = m(item)\n        l_.append(_latex_item_to_string(item, escape=escape, as_content=as_content))\n\n    return NoEscape(token.join(l_))", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n\n    s = str(item)\n\n    if escape:\n        s = escape_latex(s)\n\n    return s", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'rb') as f:\n            s = f.read()\n\n        s = s.decode(encoding)\n        return self.parse(s, state)\n\n    ", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    if plugins:\n        plugins = [import_plugin(name)() for name in plugins]\n    return Markdown(renderer=renderer, plugins=plugins)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if not regex:\n        return []\n    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\")\n    if replace_entities:\n        text = w3lib_replace_entities(text)\n    return _extract_regex(regex, text)\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    sb = []\n    self._render(sb, 0, indent, pretty, xhtml)\n    return ''.join(sb)", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as f:\n    return f.read()", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  def unescape_entity(m):\n    try:\n      return unichr(_unescape[m.group(1)])\n    except KeyError:\n      return m.group(0)\n\n  return re.sub(r'&(#?[^;]+);', unescape_entity, data)", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t\\r\\n') == (line[r] in ' \\t\\r\\n'):\n            r += 1\n        if line[l] in ' \\t\\r\\n':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    if font_bold is None:\n        font_bold = lambda s: s\n    if font_dim is None:\n        font_dim = lambda s: s\n    if font_red is None:\n        font_red = lambda s: s\n    if font_blue is None:\n        font_blue = lambda s: s\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    def render_token(token: _PrettyToken) -> str:\n        if token.type == _PrettyTokenType.BODY:\n            return font_normal(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            return font_red(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            return font_blue(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            return font_bold(token.value)\n        elif token.type == _PrettyTokenType.OTHERS:\n            return font_normal(token.value)\n        else:\n            assert False\n\n    return ''.join(map(render_token, tokens))\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens, text = _decode_with_recovery(content=content)\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        return self._load_template(name, globals)\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if isinstance(source, str):\n            source = self._parse(source, None, None)\n        return self.template_class.from_code(\n            self, self.compile(source, None, None, True, True), globals, None, template_class\n        )\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            import asyncio\n\n            async def to_string() -> str:\n                return self.environment.concat(  # type: ignore\n                    [x async for x in self.generate_async(*args, **kwargs)]  # type: ignore\n                )\n\n            return asyncio.run(to_string())\n\n        ctx = self.new_context(dict(*args, **kwargs))\n\n        try:\n            return self.environment.concat(  # type: ignore\n                [x for x in self.root_render_func(ctx)]  # type: ignore\n            )\n        except Exception:\n            return self.environment.handle_exception()", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    # Define the list of words to use in the Lorem Ipsum text\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"dolor\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"dolore\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    # Generate the Lorem Ipsum text\n    lorem_ipsum = \"\"\n    for i in range(n):\n        # Generate a random number of words between min and max", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        return [(key, self._mapping[key]) for key in reversed(self._queue)]\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for target, (instr, _) in self.loads.items():\n                if instr == VAR_LOAD_STORE:\n                    rv[target] = target\n\n            node = node.parent\n\n        return rv\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers\n\n", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if os.sep in template or (os.altsep and os.altsep in template):\n        raise TemplateNotFound(template)\n    return template.split(\"/\")", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            value = self.client.get(key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n            return\n        if value is not None:\n            bucket.bytecode_from_string(value)\n\n    ", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    path = join(language, \"stopwords.txt\")\n    try:\n        data = pkgutil.get_data(\"sumy\", path)\n    except IOError:\n        raise LookupError(\"Stop-words are not available for language %s.\" % language)\n\n    return parse_stop_words(data)", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        words = map(self.normalize_word, document.words)\n        unique_words = frozenset(self.stem_word(w) for w in words if w not in self._stop_words)\n\n        return dict((w, i) for i, w in enumerate(unique_words))\n\n    ", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        content_words = self._filter_out_stop_words(sentence.words)\n        normalized_content_words = self._normalize_words(content_words)\n        stemmed_content_words = self._stem_words(normalized_content_words)\n        return stemmed_content_words\n\n    ", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        normalized_content_words = self._filter_out_stop_words(all_words)\n        return self._stem_words(normalized_content_words)\n\n    ", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((w, f / content_words_count) for w, f in content_words_freq.items())\n        return content_word_tf\n\n    ", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        for i in range(len(sentences)):\n            index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            ratings[sentences[index]] = i\n            word_freq = self._update_tf(word_freq, sentences_as_words[index])\n            del sentences_as_words[index]\n            del sentences[index]\n        return ratings\n", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        summarization_method = self._build_cue_method_instance()\n        return summarization_method(document, sentences_count, bonus_word_value, stigma_word_value)\n\n    ", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n\n    ", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        sentences_words = [(s, self._to_words_set(s)) for s in document.sentences]\n        ratings = defaultdict(float)\n\n        for (sentence1, words1), (sentence2, words2) in combinations(sentences_words, 2):\n            rank = self._rate_sentences_edge(words1, words2)\n            ratings[sentence1] += rank\n            ratings[sentence2] += rank\n\n        return ratings\n\n    ", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [w for w in words if w not in self._stop_words]\n\n    ", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]\n\n    ", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n\n        # Normalize word frequencies\n        for w in content_words_freq:\n            content_words_freq[w] = content_words_freq[w] / content_words_count\n\n        return content_words_freq\n\n    ", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert (n > 0)\n    assert (len(sentences) > 0)\n\n    # words = _split_into_words(sentences)\n    words = sum(\n        [_split_into_words([s]) for s in sentences], [])\n    return _get_ngrams(n, words)", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    i, j = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n    return recon_tuple", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_word_count = len(_split_into_words(lcs_union))\n    return union_lcs_word_count / combined_lcs_length", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"rb\") as file:\n            html_content = file.read()\n            return cls(html_content, tokenizer, url)\n\n    ", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        current_paragraph = []\n        paragraphs = []\n\n        lines = (line.strip() for line in self._text.splitlines() if line.strip())\n        for line in lines:\n            if line.isupper():\n                heading = line\n                if current_paragraph:\n                    paragraphs.append(current_paragraph)\n                    current_paragraph = []\n            else:\n                if current_paragraph and current_paragraph[-1].endswith((\".\", \":\", \";\")):\n                    paragraphs.append(current_paragraph)\n                    current_paragraph = []\n\n                current_paragraph.append(line)\n\n        if current_paragraph:\n            paragraphs.append(current_paragraph)\n\n        return ObjectDocumentModel(\n            tuple(Paragraph(self._to_sentences(lines), self._tokenizer) for lines in paragraphs)\n        )\n\n    ", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])\n        self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(map(unicode.strip, sentences))\n\n    ", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return unicode(object).lower()", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if isinstance(value, six.binary_type):\n                return b64encode(value).decode(\"ascii\")\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if value.lower() in cls.TRUE_VALUES:\n            return True\n        elif value.lower() in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return value.strftime(cls.OUTPUT_FORMAT)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            value = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        except ValueError:\n            value_error(value, cls)\n        return datetime.date(value.year, value.month, value.day)", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None:\n            return None\n        else:\n            match = cls.EMAIL_REGEXP.match(value)\n            if match:\n                return match.group(0)\n            else:\n                raise ValueError(\"Invalid email\")\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            return json.loads(value)\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            handler = _DEFAULT_JSON_HANDLER\n\n        return handler.dumps(self.to_dict(obj_type=dict))", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    middleware = inspect_middleware(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    asgi = isinstance(app, App)\n    return AppInfo(routes, middleware, static_routes, sinks, error_handlers, asgi)", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    if router is None:\n        return []\n\n    router_class = type(router)\n    if router_class not in _supported_routers:\n        raise RuntimeError(\n            'Router {} is not supported. '\n            'Please register a function to inspect it with '\n            '@register_router(router_class)'.format(router_class)\n        )\n\n    return _supported_routers[router_class](router)", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for prefix, directory, fallback_filename in app._static_routes:\n        static_routes.append(StaticRouteInfo(prefix, directory, fallback_filename))\n    return static_routes", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for prefix, sink in app._sinks.items():\n        source_info, name = _get_source_info_and_name(sink)\n        info = SinkInfo(prefix, name, source_info)\n        sinks.append(info)\n    return sinks", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error, handlers in app._error_handlers.items():\n        for handler in handlers:\n            source_info, name = _get_source_info_and_name(handler)\n            info = ErrorHandlerInfo(error, name, source_info, _is_internal(handler))\n            error_handlers.append(info)\n    return error_handlers", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_tree = _prepare_middleware_tree(app)\n    middleware_classes = _prepare_middleware_classes(app)\n    return MiddlewareInfo(middleware_tree, middleware_classes, app._independent_middleware)\n\n", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = getattr(self, 'visit_{}'.format(visit_name), None)\n        if visit_method is None:\n            raise RuntimeError(\n                'No visit method for {}'.format(instance.__class__.__name__)\n            )\n        return visit_method(instance)", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            header_value = self.env.get('HTTP_FORWARDED')\n            if header_value:\n                self._cached_forwarded = helpers._parse_forwarded_header(\n                    header_value\n                )\n            else:\n                self._cached_forwarded = []\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts(\n            'application/msgpack'\n        )\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            return int(self.env['CONTENT_LENGTH'])\n        except KeyError:\n            return None\n        except ValueError:\n            msg = 'The value must be a positive integer.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = helpers.BoundedStream(\n                self.stream, self.content_length\n            )\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            # NOTE(kgriffs): For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.relative_uri\n\n            self._cached_uri = value\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            self._cached_forwarded_uri = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n            )\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # NOTE(kgriffs): For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.app + self.path\n\n            if self.query_string:\n                value += '?' + self.query_string\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            # PERF(kgriffs): For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.forwarded_scheme + '://' + self.forwarded_host + self.app\n\n            self._cached_forwarded_prefix = value\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            value = self.env['HTTP_HOST']\n        except KeyError:\n            value = self.env['SERVER_NAME']\n\n        return value\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        subdomain = host.partition('.')[0]\n        return subdomain if subdomain != host else None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {\n                # NOTE(kgriffs): We use a dict comprehension here\n                # instead of a dict constructor because it is\n                # faster.\n                k.replace('_', '-'): v for k, v in self.env.items() if k.startswith('HTTP_')\n            }\n\n        return self._cached_headers\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        # NOTE(kgriffs): best_match will return '' if no match is found\n        return mimeparse.best_match([media_type], self.accept) == media_type", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if (accept == media_types) or (accept == '*/*'):\n            return media_types\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # NOTE(kgriffs): Normalize to uppercase and replace hyphens\n        # with underscores, since that is what Falcon uses internally.\n        name = name.upper().replace('-', '_')\n\n        # NOTE(kgriffs): try..except is faster than get() when we\n        # expect the key to be present most of the time.\n        try:\n            return self.env['HTTP_' + name]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(\n                    'Missing required header', '{0}'.format(name)\n                )\n\n            return default\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            self._cookies = helpers.parse_cookie_header(self.get_header('Cookie'))\n\n        return self._cookies.get(name, [])\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if not is_ascii_encodable(name):\n            raise KeyError('name is not ascii encodable')\n\n        if self._cookies is None:\n            return\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n        self._cookies[name]['max-age'] = '0'\n\n        # NOTE(tbug): we never actually need to\n        # know that GMT is named GMT when formatting cookies.\n        # It is a function call less to just write \"GMT\" in the fmt string:\n        fmt = '%a, %d %b %Y %H:%M:%S GMT'\n        self._cookies[name]['expires'] = datetime.datetime.utcnow().strftime(fmt)\n\n        self._cookies[name]['max-age'] = '0'\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n        self._cookies[name]['max-age'] = '0'\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n        self._cookies[name]['max-age'] = '0'\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        if name.lower() == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to retrieve cookies')\n\n        value = self._headers.get(name.lower(), default)\n        if value is None:\n            return default\n\n        if isinstance(value, list):\n            return ', '.join(value)\n\n        return value", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): uwsgi fails with a TypeError if any header\n        # is not a str, so do the conversion here. It's actually\n        # faster to not do an isinstance check. str() will encode\n        # to US-ASCII.\n        value = str(value)\n\n        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        self._headers[name] = value", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        try:\n            del self._headers[name]\n        except KeyError:\n            pass", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\n        'The \"falcon-print-routes\" command is deprecated. ',\n        'Please use \"falcon-inspect-app\"',\n        file=sys.stderr,\n    )\n    main()", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return encoded_uri\n\n    if not isinstance(encoded_uri, str):\n        raise TypeError('encoded_uri must be a string')\n\n    if '%' not in encoded_uri:\n        return encoded_uri\n\n    # PERF(kgriffs): This was found to be faster than using a regex, for\n    # both short and long query strings. Tested on CPython 3.4.\n    tokens = encoded_uri.split('%')\n    if len(tokens) == 1:\n        return tokens[0]\n\n    decoded_uri = _join_tokens(tokens)\n\n    if unquote_plus:\n        decoded_uri = decoded_uri.replace('+', ' ')\n\n    return decoded_uri", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/' + self\n\n        return self", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if not etag_str:\n            raise ValueError('Invalid ETag string')\n\n        if etag_str[0] == 'W/':\n            if len(etag_str) < 3 or etag_str[-1] != '\"':\n                raise ValueError('Invalid ETag string')\n\n            return cls(etag_str[2:-1], is_weak=True)\n\n        if etag_str[0] != '\"' or etag_str[-1] != '\"':\n            raise ValueError('Invalid ETag string')\n\n        return cls(etag_str[1:-1])\n", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')\n    filename = _UNSAFE_CHARS.sub('_', filename)\n    if filename[0] == '.':\n        filename = '_' + filename[1:]\n    return filename", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            return self._buffer[self._buffer_pos :]\n\n        if size <= 0:\n            return b''\n\n        if size <= self._buffer_len - self._buffer_pos:\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        remaining = size - (self._buffer_len - self._buffer_pos)\n        if remaining <= self._max_join_size:\n            result = []\n            async for chunk in self._source:\n                chunk_len = len(chunk)\n                if remaining < chunk_len:\n                    result.append(chunk[:remaining])\n                    self._prepend_buffer(chunk[remaining:])\n                    break\n\n                result.append(chunk)\n                remaining -= chunk_len\n                if remaining == 0:  # pragma: no py39,py310 cover\n                    break\n\n            # PERF(vytas) Don't join unless necessary.\n            return self._buffer[self._buffer_pos :] + (\n                result[0] if len(result) == 1 else b''.join(result)\n            )\n\n        # NOTE(vytas): size > self._max_join_size\n        result = io.BytesIO()\n        result.write(self._buffer[self._buffer_pos :])\n        remaining -= self._buffer_len - self._buffer_pos\n        self._buffer = b''\n        self._buffer_len = 0\n        self._buffer_pos = 0\n\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if remaining < chunk_len:\n                result.write(chunk[:remaining])\n                self._prepend_buffer(chunk[remaining:])\n                break\n\n            result.write(chunk)\n            remaining -= chunk_len\n            if remaining == 0:  # pragma: no py39,py310 cover\n                break\n\n        return result.getvalue()\n\n    ", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0),\n            size,\n        )\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if value.isspace():\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        responder = getattr(resource, 'on_' + method.lower(), None)\n\n        if responder is not None:\n            if suffix:\n                responder.__name__ = responder.__name__ + suffix\n\n            method_map[method] = responder\n\n    return method_map", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        if size > self.remaining:\n            size = self.remaining\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return ' '.join([to_unicode(s) for s in scope])\n    if scope is None:\n        return None\n    return to_unicode(scope)", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth = headers.get('Authorization')\n    if not auth or ' ' not in auth:\n        return None, None\n    auth_type, auth_token = auth.split(None, 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n    try:\n        auth_token = base64.b64decode(to_unicode(auth_token).encode('ascii'))\n    except (binascii.Error, ValueError):\n        return None, None\n    try:\n        username, password = auth_token.decode('utf-8').split(':', 1)\n    except ValueError:\n        username = auth_token.decode('utf-8')\n        password = None\n    return username, password", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = [('client_id', client_id), ('response_type', response_type)]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k]:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_qs(uri, params)\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    params = dict(urlparse.parse_qsl(urlparse.urlparse(uri).query))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json.dumps(text)\n    return urlsafe_b64encode(text.encode('utf-8')).decode('utf-8')\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_data = extract_segment(header_segment, error_cls, 'header')\n    header = json_loads(to_unicode(header_data))\n    if not isinstance(header, dict):\n        raise error_cls('Invalid header type')\n    return header", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n        for key, value in self.__dict__.items():\n            if key.startswith('_'):\n                continue\n            if isinstance(value, (list, tuple, set)):\n                if all(isinstance(v, dict) for v in value):\n                    data[key] = [v for v in value]\n                else:\n                    data[key] = value\n            elif isinstance(value, TwitterModel):\n                data[key] = value.AsDict()\n            else:\n                data[key] = value\n        return data", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        data = dict(data, **kwargs)\n        return cls(**data)\n\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        if len(status) > char_lim:\n            raise TwitterError({'message': \"Status message is too long\"})\n\n        words = status.split()\n\n        for word in words:\n            if len(word) > char_lim:\n                raise TwitterError({'message': \"Status message contains a word that is too long\"})\n\n        tweets = []\n        line = \"\"\n\n        for word in words:\n            if len(line) + len(word) > char_lim:\n                tweets.append(line)\n                line = word\n            else:\n                line += word + \" \"\n\n        tweets.append(line)\n\n        return tweets", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    if not isinstance(import_str, str):\n        raise ImportFromStringError(\n            f\"Import string '{import_str}' is not a string.\"\n        )\n\n    module_str, _, attrs_str = import_str.partition(\":\")\n    if not module_str or not attrs_str:\n        raise ImportFromStringError(\n            f\"Import string '{import_str}' must be in format '<module>:<attribute>'.\"\n        )\n\n    try:\n        module = importlib.import_module(module_str)\n    except ImportError as e:\n        if \".\" not in module_str:\n            raise ImportFromStringError(\n                f\"Could not import module '{module_str}'. Are you sure it is installed?\"\n            ) from e\n\n        module_name, _, obj_name = module_str.rpartition(\".\")\n        modules = module_name.split(\".\")\n\n        obj = None\n        for module in (\n            importlib.import_module(module_name) for module in modules\n        ):\n            if hasattr(module, obj_name):\n                obj = getattr(module, obj_name)\n                break\n\n        if obj is None:\n            raise ImportFromStringError(\n                f\"Could not import object '{obj_name}' from module '{module_name}'.\"\n            )\n\n        return obj\n\n    instance = module\n    try:\n        for attr_str in attrs_str.split(\".\"):\n            instance = getattr(instance, attr_str)\n    except AttributeError:\n        raise ImportFromStringError(\n            f\"Attribute '{attrs_str}' not found in module '{module_str}'.\"\n        )\n\n    return instance", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    scheme = get_url_scheme(api_settings)\n    url = _reverse(\n        viewname,\n        args=args,\n        kwargs=kwargs,\n        request=request,\n        format=format,\n        **extra\n    )\n\n    if request is not None:\n        return build_absolute_uri(url, request=request)\n\n    return build_absolute_uri(url, request=request)", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        fields = OrderedDict()\n        for key, value in self._declared_fields.items():\n            fields[key] = value\n            if isinstance(value, Serializer):\n                value.bind(field_name=key, parent=self)\n        return fields\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parse_constant = json.strict_constant if self.strict else None\n            return json.load(decoded_stream, parse_constant=parse_constant)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        view = parser_context['view']\n        request = parser_context['request']\n        filename = view.kwargs.get('filename', request.GET.get('filename'))\n        if not filename:\n            content_disposition = request.META.get('HTTP_CONTENT_DISPOSITION')\n            if content_disposition:\n                match = re.search(r'filename=[\\'\"](.+)[\\'\"]', content_disposition)\n                if match:\n                    filename = match.group(1)\n        return filename", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if isinstance(obj, type):\n        # Check if the object is a built-in function.\n        if obj.__module__ == 'builtins':\n            raise BuiltinSignatureError\n\n        # Check if the object is a function, method, or a functools.partial object.\n        if isinstance(obj, (FunctionType, MethodType, partial)):\n            # Get the signature of the object.\n            signature = inspect.signature(obj)\n\n            # Check if all the parameters have a default value or are variable positional or keyword parameters.\n            for param in signature.parameters.values():\n                if param.default is param.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n                    return False\n\n            return True\n\n    return False\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n\n        # In case this is a nested serializer, we also bind the child\n        # serializer fields.\n        if isinstance(self, NestedSerializer):\n            self.bind_fields(self.fields)\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (empty, data) = self.validate_empty_values(data)\n        if empty:\n            return data\n\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and not self.allow_blank:\n            self.fail('blank')\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            raise TypeError(\n                'BooleanField should not be used to represent a boolean. '\n                'Use a BooleanField instead.'\n            )\n        if not isinstance(data, (str, int, float)):\n            raise TypeError(\n                'CharField should not be used to represent a non-string value. '\n                'Use a different field type instead.'\n            )\n        if self.trim_whitespace:\n            data = data.strip()\n        return str(data)\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            return decimal.Decimal(data)\n        except (TypeError, decimal.InvalidOperation):\n            self.fail('invalid')\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n\n        if output_format.lower() == ISO_8601:\n            value = value.isoformat()\n            if value.endswith('+00:00'):\n                value = value[:-6] + 'Z'\n            return value\n\n        return value.strftime(output_format)", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        choices = self.choices\n        for value, label in choices:\n            yield {\n                'value': value,\n                'display_text': label,\n                'display_name': label,\n                'selected': value in value,\n            }\n\n        if self.html_cutoff is not None and len(choices) > self.html_cutoff:\n            yield {\n                'value': '',\n                'display_text': self.html_cutoff_text.format(count=self.html_cutoff),\n                'display_name': self.html_cutoff_text.format(count=self.html_cutoff),\n                'selected': False,\n            }\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return []\n            self.fail('required')\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            return dictionary.getlist(self.field_name, [])\n        return dictionary.get(self.field_name, [])\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, list):\n        ret = [\n            _get_error_details(item, default_code) for item in data\n        ]\n        if isinstance(data, ReturnList):\n            return ReturnList(ret, serializer=data.serializer)\n        return ret\n    elif isinstance(data, dict):\n        ret = {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n        if isinstance(data, ReturnDict):\n            return ReturnDict(ret, serializer=data.serializer)\n        return ret\n\n    text = force_str(data)\n    code = getattr(data, 'code', default_code)\n    return ErrorDetail(text, code)", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        queryset = self.get_queryset()\n        if queryset is None:\n            # Ensure that field.choices returns something sensible\n            # even when accessed with a read-only field.\n            return\n\n        for item in queryset:\n            yield self.get_option_value(item), self.get_option_label(item)\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n        queryset = self.get_queryset()\n        try:\n            return queryset.get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    dict_ = request.GET.copy()\n    dict_[key] = val\n    return iri_to_uri(request.path) + '?' + dict_.urlencode()\n\n", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.full_type == '*/*':\n            return True\n        if self.main_type == '*' and self.sub_type == '*':\n            return True\n        if self.main_type == other.main_type and self.sub_type == '*':\n            return True\n        if self.main_type == other.main_type and self.sub_type == other.sub_type:\n            return True\n        return False", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.params:\n            return 2\n        else:\n            return 3\n\n    ", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        return self.orig\n", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            self.loop.default_exception_handler(context)\n            msg = context.get('message')\n            if re.search(msg_re, msg):\n                self.__unhandled_exceptions.append(context)\n\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(self.loop_exception_handler)\n\n        if not self.__unhandled_exceptions:\n            raise AssertionError(\n                'no loop error handler called with message matching '\n                '{}'.format(msg_re))\n\n        self.__unhandled_exceptions = []\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    for column, (table, value_column) in foreign_keys.items():\n        lookup_table = LookupTable(conn, table, value_column, index_fts)\n        for df in dataframes:\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_table.id_for_value)\n\n    return dataframes", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        for key, value in items:\n            self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n        if self.autocommit:\n            self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        self.conn.commit(blocking=blocking)\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close(do_log=do_log, force=True)\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    if retry_on_404:\n        return urllib.request.urlopen(url, timeout=timeout).read()\n    else:\n        try:\n            return urllib.request.urlopen(url, timeout=timeout).read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404:\n                return None\n            else:\n                raise", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data/')\n        data = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if data:\n            if sep:\n                userdata = {}\n                for line in data.split('\\n'):\n                    if line:\n                        key, value = line.split(sep, 1)\n                        userdata[key] = value\n                return userdata\n            else:\n                return data\n        else:\n            return None\n    except urllib.error.URLError:\n        return None", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    if not name:\n        return name\n    if name.isupper():\n        return name.lower()\n    if name.islower():\n        return name\n    if name.isdigit():\n        return name\n    if name.isalnum():\n        return name\n    if name.isspace():\n        return name\n    if name.isprintable():\n        return name\n    if name.isidentifier():\n        return name\n    if name.isdecimal():\n        return name\n    if name.isnumeric():\n        return name\n    if name.isalpha():\n        return name\n    if name.isascii():\n        return name\n    if name.islower():\n        return name\n    if name.isupper():\n        return name\n    if name.istitle():\n        return name\n    if name.isalnum():\n        return name\n    if name.isalpha():\n        return name\n    if name.isdigit():\n        return name\n    if name.isdecimal():\n        return name\n    if name.isnumeric():\n        return name\n    if name.isspace():\n        return name\n    if name.isprintable():\n        return name\n    if name.isidentifier():\n        return name\n    if name.isalnum():\n        return name\n    if name.isalpha():\n        return name\n    if name.isdigit():\n        return name\n    if name.isdecimal():\n        return name\n    if name.isnumeric():\n        return name\n    if name.isspace():\n        return name\n    if name.isprintable():\n        return name\n    if name.isidentifier():\n        return name\n    if name.isalnum():\n        return name\n    if name.isalpha():\n        return name\n    if name.isdigit():\n        return name\n    if name.isdecimal():\n        return name\n    if name.isnumeric():\n        return name\n    if name.isspace():\n        return name\n    if name.isprintable():\n        return name\n    if name.isidentifier():\n        return name\n    if name.isalnum():", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=RegionInfo(name=region_name,\n                                                        endpoint='cloudsearchdomain.%s.amazonaws.com' % region_name),\n                                        **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    return RedshiftConnection(region=RegionInfo(name=region_name,\n                                                endpoint='redshift.%s.amazonaws.com' % region_name),\n                              **kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=RegionInfo(name=region_name,\n                                               endpoint='support.%s.amazonaws.com' % region_name),\n                             **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='config.%s.amazonaws.com' % region_name),\n                                    **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region=region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1(region=RegionInfo(name=region_name,\n                                    endpoint='cloudsearch.%s.amazonaws.com' % region_name),\n                  **kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * DEFAULT_PART_SIZE:\n        raise ValueError(\"File size exceeds maximum allowed archive size of 40GB\")\n    if size_in_bytes > default_part_size:\n        return int(math.ceil(size_in_bytes / float(DEFAULT_PART_SIZE))) * DEFAULT_PART_SIZE\n    return default_part_size", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not bytestring:\n        return [hashlib.sha256(b'').digest()]\n    hashes = []\n    for i in range(0, len(bytestring), chunk_size):\n        chunk = bytestring[i:i + chunk_size]\n        hashes.append(hashlib.sha256(chunk).digest())\n    return hashes", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    tree_hashes = []\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n        linear_hash.update(chunk)\n        tree_hashes.append(hashlib.sha256(chunk).digest())\n    return linear_hash.hexdigest(), tree_hash(tree_hashes).hex()", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        min_part_size = int(math.pow(2, 20))\n        if part_size < min_part_size:\n            part_size = min_part_size\n        total_parts = int(math.ceil(float(total_size) / part_size))\n        if total_parts * part_size != total_size:\n            part_size = int(math.ceil(float(total_size) / total_parts))\n        return (total_parts, part_size)", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region=RegionInfo(name=region_name,\n                                    endpoint='glacier.%s.amazonaws.com' % region_name),\n                  **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        updated = self.connection.get_all_network_interfaces(\n            [self.id],\n            dry_run=dry_run\n        )[0]\n        if not updated:\n            if validate:\n                raise ValueError('%s is not a valid NetworkInterface ID' % self.id)\n            else:\n                return None\n        self._update(updated)\n        return self.status", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        return self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        return self.connection.release_address(\n            public_ip=self.public_ip,\n            dry_run=dry_run\n        )", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        return self.connection.disassociate_address(\n            public_ip=self.public_ip,\n            dry_run=dry_run\n        )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key, value in tags.items():\n            self.tags[key] = value", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        updated = self.connection.get_all_volumes(\n            volume_ids=[self.id],\n            filters={'dry-run': dry_run}\n        )\n        if len(updated) == 0:\n            if validate:\n                raise ValueError('%s is not a valid Volume ID' % self.id)\n            return None\n        updated = updated[0]\n        self._update(updated)\n        return self.status", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            instance_id=instance_id,\n            device=device,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(\n            self.id,\n            description,\n            dry_run=dry_run\n        )", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        return self.status", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if not self.rules:\n            self.rules = IPPermissionsList()\n        rule = IPPermissions(self.rules)\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        rule.add_grant(src_group_name, src_group_owner_id, cidr_ip,\n                       src_group_group_id, dry_run=dry_run)\n        self.rules.append(rule)\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    for region in regions(**kw_params):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    return connect_to_region(region_name, **kw_params)", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    region = RegionInfo(name=region_name,\n                        endpoint=RegionData[region_name]['endpoint'])\n    return ELBConnection(region=region, **kw_params)", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)], verb='GET')\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='awslambda.us-east-1.amazonaws.com'),\n                               **kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection(region=RegionInfo(name=region_name,\n                                                      endpoint='cognito-identity.%s.amazonaws.com' % (region_name,)))", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(name=region_name,\n                                                   endpoint='cognito-sync.%s.amazonaws.com' % region_name),\n                                 **kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                identifier = (identifier[0], identifier[1])\n            except:\n                identifier = (identifier[0], identifier[1])\n        records = self.route53connection.get_all_rrsets(self.id, name, type,\n                                                       identifier=identifier)\n        if len(records) == 0:\n            return None\n        if len(records) > desired:\n            raise TooManyRecordsException(desired, len(records))\n        if all:\n            return records\n        else:\n            return records[0]", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return Route53DomainsConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='route53domains.amazonaws.com'),\n                                    **kw_params)", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent,\n                                      version_id, res_download_handler,\n                                      response_headers)\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method=allowed_method,\n                       allowed_origin=allowed_origin,\n                       id=id,\n                       allowed_header=allowed_header,\n                       max_age_seconds=max_age_seconds,\n                       expose_header=expose_header)\n        self.append(rule)", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        return self._get_key_internal(key_name, headers, [])[0]\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        return self.key_class(self, key_name)", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if version_id:\n            query_args = 'versionId=%s' % version_id\n        else:\n            query_args = None\n        if mfa_token:\n            if not headers:\n                headers = {}\n            headers[self.connection.provider.mfa_header] = ' '.join(mfa_token)\n        response = self.connection.make_request('DELETE', self.name, key_name,\n                                                headers=headers,\n                                                query_args=query_args)\n        body = response.read()\n        if response.status != 204:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n        else:\n            # return a key object with information on what was deleted.\n            k = self.key_class(self)\n            k.name = key_name\n            k.handle_version_headers(response)\n            k.handle_addl_headers(response.getheaders())\n            return k", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.provider.security_token:\n            return ['hmac-v4-s3']\n        else:\n            return super(S3Connection, self)._required_auth_capability()\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if self._auth_handler.capability[0] != 'hmac-v4-s3':\n            raise BotoClientError(\"This method is only available for SigV4\")\n\n        headers = headers or {}\n        if iso_date is None:\n            iso_date = time.strftime(boto.utils.ISO8601, time.gmtime())\n        if response_headers:\n            for k, v in response_headers.items():\n                headers[k] = v\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if version_id is not None:\n            headers['x-amz-version-id'] = version_id\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        auth_path = self.get_path(auth_path)\n        # optional version_id and response_headers need to be added to\n        # the query param list.\n        extra_qp = []\n        if version_id is not None:\n            extra_qp.append(\"versionId=%s\" % version_id)\n        if response_headers:\n            for k, v in response_headers.items():\n                extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if extra_qp:\n            delimiter = '?' if '?' not in auth_path else '&'\n            auth_path += delimiter + '&'.join(extra_qp)\n        self.calling_format.build_path_base(bucket, key)\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if extra_qp:\n            delimiter = '?' if not query_part else '&'\n            query_part += delimiter + '&'.join(extra_qp)\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = []\n        if self.suffix:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key:\n            parts.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        return ''.join(parts)\n", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        return tag('RoutingRules', '\\n'.join([rule.to_xml() for rule in self]))", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix,\n                              http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                 replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix,\n                                 http_redirect_code=http_redirect_code)\n        return self", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        from boto.s3.connection import S3Connection\n        region = S3RegionInfo(\n            name=region_name,\n            endpoint=kw_params['host'],\n            connection_cls=S3Connection\n        )\n        del kw_params['host']\n        return region.connect(**kw_params)\n    else:\n        return connect_to_region(region_name, **kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='directconnect.%s.amazonaws.com' % (region_name,)),\n                                    **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region=region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        d = {}\n        d['Keys'] = []\n        for key in self.keys:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            d['Keys'].append(k)\n        if self.attributes_to_get:\n            d['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            d['ConsistentRead'] = self.consistent_read\n        return d", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        d = {}\n        for batch in self:\n            table_name, batch_dict = batch.to_dict()\n            d[table_name] = batch_dict\n        return d", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        try:\n            encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            return attr\n        return {dynamodb_type: encoder(attr)}", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) > 1:\n            return attr\n        if isinstance(attr, six.string_types):\n            return attr\n        dynamodb_type = list(attr.keys())[0]\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                             dynamodb_type)\n        return decoder(attr[dynamodb_type])", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=RegionInfo(name=region_name, endpoint='elasticbeanstalk.us-east-1.amazonaws.com'), **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    return boto.swf.layer1.Layer1(region=RegionInfo(name=region_name,\n                                                    endpoint=REGION_ENDPOINTS[region_name]))", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    return RegionInfo.regions(service_name='opsworks')", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=RegionInfo(name=region_name,\n                                               endpoint='opsworks.%s.amazonaws.com' % (region_name)))", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='cloudtrail.%s.amazonaws.com' % (region_name)))", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=RegionInfo(name=region_name,\n                                                   endpoint=region_name + '.elasticache.amazonaws.com'),\n                                 **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='codedeploy.%s.amazonaws.com' % region_name),\n                                **kw_params)", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id,\n        }\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=RegionInfo(name=region_name,\n                                                       endpoint='machinelearning.us-east-1.amazonaws.com'),\n                                      **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    for region in RegionData:\n        if region_name == region.name:\n            return region.connect(**kw_params)\n    return None", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, \n                                   'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, \n                             [('item', VpcPeeringConnection)])", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=RegionInfo(name=region_name,\n                                               endpoint='kinesis.%s.amazonaws.com' % region_name),\n                             **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region=region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n\n        for field in raw_indexes:\n            index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('ALL')\n            kwargs = {\n                'parts': []\n            }\n\n            if field['Projection']['ProjectionType'] == 'ALL':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('ALL')\n            elif field['Projection']['ProjectionType'] == 'KEYS_ONLY':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('KEYS_ONLY')\n            elif field['Projection']['ProjectionType'] == 'INCLUDE':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('INCLUDE')\n                kwargs['includes'] = field['Projection']['NonKeyAttributes']\n            else:\n                raise exceptions.UnknownIndexFieldError(\n                    \"%s was seen, but is unknown. Please report this at \"\n                    \"https://github.com/boto/boto/issues.\" % \\\n                    field['Projection']['ProjectionType']\n                )\n\n            name = field['IndexName']\n            kwargs['parts'] = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(index_klass(name, **kwargs))\n\n        return indexes", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_data = self.connection.describe_table(self.table_name)\n        raw_schema = raw_data['Table']['KeySchema']\n        raw_indexes = raw_data['Table'].get('LocalSecondaryIndexes', [])\n        raw_global_indexes = raw_data['Table'].get('GlobalSecondaryIndexes', [])\n        raw_attributes = raw_data['Table']['AttributeDefinitions']\n        raw_throughput = raw_data['Table']['ProvisionedThroughput']\n\n        self.schema = self._introspect_schema(raw_schema, raw_attributes)\n        self.indexes = self._introspect_indexes(raw_indexes)\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n        self.throughput = {\n            'read': raw_throughput['ReadCapacityUnits'],\n            'write': raw_throughput['WriteCapacityUnits'],\n        }\n\n        return raw_data", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput:\n            self.throughput['read'] = int(throughput['read'])\n            self.throughput['write'] = int(throughput['write'])\n\n        if global_indexes:\n            for index_name, index_throughput in global_indexes.items():\n                for gsi in self.global_indexes:\n                    if gsi.name == index_name:\n                        gsi.throughput['read'] = int(index_throughput['read'])\n                        gsi.throughput['write'] = int(index_throughput['write'])\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n\n        kwargs = {}\n\n        kwarg_map = {\n            'indexes': 'local_secondary_indexes',\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('indexes', 'global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n\n                kwargs[kwarg_map[index_attr]] = raw_indexes\n\n        self.connection.update_table(\n            self.table_name,\n            provisioned_throughput=raw_throughput,\n            **kwargs\n        )\n\n        return True\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if global_index:\n            raw_index = global_index.schema()\n            raw_index['ProvisionedThroughput'] = {\n                'ReadCapacityUnits': int(self.throughput['read']),\n                'WriteCapacityUnits': int(self.throughput['write']),\n            }\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=[\n                    {\n                        \"Create\": raw_index\n                    }\n                ],\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index to ' \\\n                  'create_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        if not self.schema:\n            self.describe()\n        for x, arg in enumerate(args):\n            kwargs[self.schema[x].name] = arg\n        raw_key = self._encode_keys(kwargs)\n        raw_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            consistent_read=consistent,\n            attributes_to_get=attributes,\n        )\n        item = Item(self)\n        item.load(raw_data)\n        return item", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            return False\n        return True", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        raw_key = self._encode_keys(item_data.keys())\n        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, raw_key, item_data, **kwargs)\n        return True\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        return [field.name for field in self.schema]\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            if '__' not in key:\n                raise exceptions.QueryError(\n                    \"Invalid filter key: %s. Must be in the format \"\n                    \"<fieldname>__<filter_operation>\" % key\n                )\n\n            field, op = key.split('__', 1)\n\n            if op not in using:\n                raise exceptions.QueryError(\n                    \"Invalid filter operation: %s. Must be one of: %s\" % (\n                        op, ', '.join(using.keys())\n                    )\n                )\n\n            if field not in filters:\n                filters[field] = {}\n\n            filters[field][using[op]] = self._dynamizer.encode(value)\n\n        return filters", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = BatchGetResultSet(\n            self,\n            keys,\n            consistent=consistent,\n            attributes=attributes,\n        )\n        return results", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.describe_table(self.table_name)['ItemCount']", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        # We're going to send 25 items at a time.\n        to_put = self._to_put[:25]\n        to_delete = self._to_delete[:25]\n        # Remove them from the list.\n        self._to_put = self._to_put[25:]\n        self._to_delete = self._to_delete[25:]\n        batch_data = {\n            self.table.table_name: []\n        }\n\n        for item in to_put:\n            item_data = {}\n\n            for key, value in item.items():\n                item_data[key] = self.table._dynamizer.encode(value)\n\n            batch_data[self.table.table_name].append({\n                'PutRequest': {\n                    'Item': item_data,\n                },\n            })\n\n        for key_data in to_delete:\n            raw_key = {}\n\n            for key, value in key_data.items():\n                raw_key[key] = self.table._dynamizer.encode(value)\n\n            batch_data[self.table.table_name].append({\n                'DeleteRequest': {\n                    'Key': raw_key,\n                },\n            })\n\n        resp = self.table.connection.batch_write_item(batch_data)\n        self.handle_unprocessed(resp)\n        return True", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: [\n                    # We'll insert data here shortly.\n                ],\n            }\n\n            for item in self._unprocessed:\n                if 'PutRequest' in item:\n                    batch_data[self.table.table_name].append(item)\n                elif 'DeleteRequest' in item:\n                    batch_data[self.table.table_name].append(item)\n\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        attribute_definitions = []\n        for part in self.parts:\n            attribute_definitions.append(part.definition())\n        return attribute_definitions", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        key_schema = []\n\n        for part in self.parts:\n            key_schema.append({\n                'AttributeName': part.name,\n                'KeyType': part.attr_type,\n            })\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = self.throughput\n        return schema_data", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['Projection']['NonKeyAttributes'] = self.includes_fields\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        key_data = {}\n        for key in self.table.key_schema:\n            key_data[key] = self[key]\n        return key_data", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        if fields is None:\n            fields = self.keys()\n\n        expects = {}\n\n        for field in fields:\n            if field not in self._orig_data:\n                # New field.\n                expects[field] = {\n                    'Exists': False,\n                }\n            elif self[field] == self._orig_data[field]:\n                # Unchanged field.\n                expects[field] = {\n                    'Exists': True,\n                    'Value': self._dynamizer.encode(self[field]),\n                }\n            elif self[field] is None:\n                # Deleted field.\n                expects[field] = {\n                    'Exists': False,\n                }\n            else:\n                # Modified field.\n                expects[field] = {\n                    'Exists': True,\n                    'Value': self._dynamizer.encode(self[field]),\n                }\n\n        return expects", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        final_data = {}\n        for key, value in self._data.items():\n            final_data[key] = self._dynamizer.encode(value)\n        return final_data", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        fields = set()\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            if key in self._orig_data:\n                if self._orig_data[key] == value:\n                    # Unchanged field.\n                    continue\n\n            final_data[key] = self._dynamizer.encode(value)\n            fields.add(key)\n\n        for key in self._orig_data.keys():\n            if not key in self._data:\n                # Deleted field.\n                final_data[key] = self._dynamizer.encode(self._orig_data[key])\n                fields.add(key)\n\n        return final_data, fields", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n        expects = self.build_expects(fields=fields)\n\n        returned = self.table._update_item(final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not overwrite:\n            if not self.needs_save():\n                return False\n\n        key_data = self.get_keys()\n        final_data = self.prepare_full()\n        # Build expectations of only the fields we're planning to update.\n        expects = self.build_expects()\n        returned = self.table._put_item(key_data, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        self.table._delete_item(key)\n        return True", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    stream = extract_from_part(sqlparse.parse(sql)[0])\n    return list(extract_table_identifiers(stream))", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    return any(query_starts_with(query, prefixes) for query in queries)", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\"drop\", \"shutdown\", \"delete\", \"truncate\", \"alter\"]\n    return queries_start_with(queries, destructive_keywords)", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    if full_text.lstrip().startswith(\"\\\\\"):\n        return suggest_special(full_text)\n\n    # Extract the last token from the text before the cursor\n    last_token = last_word(text_before_cursor, \"all_punctuations\").lower()\n\n    # Extract the last identifier from the text before the cursor\n    identifier = extract_identifier(text_before_cursor)\n\n    # Suggestions for the last token\n    if last_token in (\"select\", \"from\", \"where\", \"group\", \"order\", \"limit\", \"on\"):\n        return suggest_based_on_last_token(\n            last_token, text_before_cursor, full_text, identifier\n        )\n\n    # Suggestions for the last identifier\n    if identifier:\n        return suggest_based_on_last_token(\n            identifier, text_before_cursor, full_text, identifier\n        )\n\n    # Suggestions for the last keyword\n    if last_token in (\"create\", \"drop\", \"alter\", \"insert\", \"update\", \"delete\"):\n        return [{\"type\": \"keyword\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"into\", \"values\"):\n        return [{\"type\": \"table\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"begin\", \"commit\", \"rollback\"):\n        return [{\"type\": \"keyword\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"grant\", \"revoke\"):\n        return [{\"type\": \"keyword\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"set\", \"reset\"):\n        return [{\"type\": \"keyword\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"show\"):\n        return [{\"type\": \"show\"}]\n\n    # Suggestions for the last token\n    if last_token in (\"describe\", \"desc\"):\n        return [{\"type\": \"table\"}]\n\n    # Suggestions for the last token\n   ", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin_info in pkg_resources.iter_entry_points(\"datasette\"):\n        plugin = plugin_info.load()\n        if hasattr(plugin, \"__file__\"):\n            plugin_name = plugin.__name__\n            static_path = plugin_info.load().static_files()\n            templates_path = plugin_info.load().templates_directory()\n            hooks = [\n                hook.name\n                for hook in pm.hook.get_hookimpls(plugin)\n                if hook.name.startswith(\"render_\")\n            ]\n            version = getattr(plugin, \"__version__\", None)\n            project_name = getattr(plugin, \"__datasette_project_name__\", None)\n            plugins.append(\n                {\n                    \"name\": plugin_name,\n                    \"static_path\": static_path,\n                    \"templates_path\": templates_path,\n                    \"hooks\": hooks,\n                    \"version\": version,\n                    \"project_name\": project_name,\n                }\n            )\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        facet_size = self.get_facet_size()\n        for column in columns:\n            if column in already_enabled:\n                continue\n            # Does this column have between 1 and row_count distinct values?\n            suggested_facet_sql = \"\"\"\n                select count(distinct {col}) from (\n                    {sql}\n                ) where {col} is not null\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                count = results.rows[0][0]\n                if 1 <= count <= await self.get_row_count():\n                    # Does this column have at least one value with a count > 1?\n                    suggested_facet_sql = \"\"\"\n                        select count(*) from (\n                            select {col} from (\n                                {sql}\n                            ) where {col} is not null\n                            group by {col}\n                        ) where count > 1\n                    \"\"\".format(\n                        col=escape_sqlite(column), sql=self.sql\n                    )\n                    try:\n                        results = await self.ds.execute(\n                            self.database,\n                            suggested_facet_sql,\n                            self.params,\n                            truncate=False,\n                            custom_time_limit=self.ds.setting(\n                                \"facet_suggest_time_limit_ms\"\n                            ),\n                            log_sql_errors=False,\n                        )\n                        if results.rows[0][0] > 0:\n                            suggested_facets.append(\n                                {\n                                    \"name\": column,\n                                    \"type\": \"", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        args = dict(self.get_querystring_pairs())\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # TODO: does this query break if inner sql produces value or count columns?\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet\": column}\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = str(args.get(f\"{column}__exact\")) == str(row[\"value\"])\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    # Check that every value is either null or a JSON array\n                    if all(\n                        r[\"value\"] is None or self._is_json_array_of_strings(r[\"value\"])\n                        for r in distinct_values\n                    ):\n                        # Check that the first 100 arrays contain only strings\n                        if all(\n                            self._is_json_array_of_strings(r[\"value\"])\n                            for r in distinct_values[:100]\n                        ):\n                            suggested_facets.append(\n                                {\n                                    \"name\": column,\n                                    \"type\": self.type,\n                                    \"toggle_url\": self.ds.abs", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        qs_pairs = self.get_querystring_pairs()\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select json_each.value as value, count(*) as count from (\n                    {sql}\n                ) join json_each({column})\n                where json_each.value is not null\n                group by json_each.value order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                ) where {col} glob \"????-??-*\"\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        await self.refresh_schemas()\n        # pylint: disable=no-member\n        for hook in pm.hook.startup(datasette=self):\n            await await_me_maybe(hook)\n", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        if name:\n            return self.databases[name]\n        for db in self.databases.values():\n            if db.name != \"_internal\":\n                return db\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        if name is None:\n            name = db.name\n        if name in self.databases:\n            i = 1\n            while True:\n                new_name = \"{}-{}\".format(name, i)\n                if new_name not in self.databases:\n                    name = new_name\n                    break\n                i += 1\n        if route is None:\n            route = name\n        db.name = name\n        db.route = route\n        self.databases = self.databases.copy()\n        self.databases[name] = db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden(\n                    \"Permission denied: {} on {}\".format(action, resource or \"\")\n                )\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        if permissions is None:\n            permissions = [(action, resource)]\n        try:\n            await self.ensure_permissions(actor, permissions)\n        except Forbidden:\n            return False, False\n        return True, False\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\n                \"Datasette.render_template() called before startup has been invoked\"\n            )\n        if context is None:\n            context = {}\n        context.update(\n            {\n                \"datasette\": self,\n                \"request\": request,\n                \"urls\": self.urls,\n                \"settings\": self.settings_dict(),\n                \"version\": __version__,\n                \"crumbs\": self._crumb_items(request),\n                \"messages\": self._show_messages(request),\n                \"root_token\": self._root_token,\n                \"csrftoken\": request.scope.get(\"csrftoken\") if request else None,\n                \"view_name\": view_name,\n            }\n        )\n        for hook in pm.hook.get_template_vars(\n            datasette=self, request=request, view_name=view_name\n        ):\n            context.update(await await_me_maybe(hook))\n        for hook in pm.hook.extra_body_scripts(\n            datasette=self, request=request, view_name=view_name\n        ):\n            context.update(await await_me_maybe(hook))\n        context[\"extra_body_scripts\"] = Markup(\n            \"\\n\".join(\n                [\n                    '<script src=\"{}\"></script>'.format(escape(script[\"url\"]))\n                    for script in context.get(\"extra_body_scripts\", [])\n                ]\n            )\n        )\n        context[\"extra_head_scripts\"] = Markup(\n            \"\\n\".join(\n                [\n                    '<script src=\"{}\"></script>'.format(escape(script[\"url\"]))\n                    for script in context.get(\"extra_head_scripts\", [])\n                ]\n            )\n        )\n        context[\"extra_css_urls\"] = [\n            self.absolute_url(request, url[\"url\"])\n            for url in context.get(\"extra_css_urls\", [])\n        ]\n        context[\"", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        return self.path + (\"?\" + self.query_string if self.query_string else \"\")\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        return cls(\n            {\n                \"type\": \"http\",\n                \"method\": method,\n                \"scheme\": scheme,\n                \"path\": path_with_query_string.split(\"?\")[0],\n                \"query_string\": path_with_query_string.split(\"?\")[1].encode(\"utf-8\"),\n                \"url_route\": {\"kwargs\": url_vars or {}},\n            },\n            receive=None,\n        )\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        headers = {k.lower(): v for k, v in self.headers.items()}\n        headers[\"content-type\"] = self.content_type\n        await asgi_start(send, self.status, headers, self.content_type)\n        for header in self._set_cookie_headers:\n            await send(\n                {\n                    \"type\": \"http.response.header\",\n                    \"name\": b\"set-cookie\",\n                    \"value\": header.encode(\"latin-1\"),\n                }\n            )\n        await send({\"type\": \"http.response.body\", \"body\": self.body.encode(\"utf-8\")})", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        cookie = Morsel()\n        cookie.set(key, value, value)\n        if max_age is not None:\n            cookie[\"max-age\"] = max_age\n        if expires is not None:\n            cookie[\"expires\"] = expires\n        cookie[\"path\"] = path\n        if domain is not None:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = True\n        if httponly:\n            cookie[\"httponly\"] = True\n        if samesite in SAMESITE_VALUES:\n            cookie[\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie.output(header=\"\"))\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = sql.strip().lower()\n    for allowed_sql_re in allowed_sql_res:\n        if allowed_sql_re.match(sql):\n            return\n    for disallowed_sql_re, error in disallawed_sql_res:\n        if disallowed_sql_re.search(sql):\n            raise InvalidSql(error)\n    raise InvalidSql(\"Invalid SQL\")\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    s = s.replace(\"\\n\", \"\\n\")\n    return _css_re.sub(lambda m: \"\\\\\" + hex(ord(m.group(0)))[1:], s)\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    fts_tables = conn.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' AND sql LIKE '%USING FTS%'\"\n    ).fetchall()\n    for fts_table in fts_tables:\n        if fts_table[0] == table:\n            return fts_table[0]\n    return None", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not value.startswith(\"http://\") and not value.startswith(\"https://\"):\n        return False\n    if whitespace_re.search(value):\n        return False\n    return True\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    return f\"{css_invalid_chars_re.sub('', s)}-{hashlib.md5(s.encode('utf-8')).hexdigest()[:6]}\"\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    terms = _escape_fts_re.findall(query)\n    terms = [term for term in terms if term and term not in ('\"\"', \"'\")]\n    if len(terms) % 2 == 0:\n        terms.append('\"\"')\n    return \" \".join(terms)\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    except sqlite3.OperationalError as e:\n        if \"no such table: sqlite_master\" in str(e):\n            raise SpatialiteConnectionProblem(\n                \"This database does not appear to be a Spatialite database\"\n            )\n        else:\n            raise ConnectionProblem(\n                \"Error while checking connection to Spatialite database: {}\".format(e)\n            )\n    for table in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\"):\n        try:\n            conn.execute(f\"PRAGMA table_info({table[0]})\")\n        except sqlite3.OperationalError as e:\n            if \"no such table\" in str(e):\n                raise SpatialiteConnectionProblem(\n                    \"This database does not appear to be a Spatialite database\"\n                )\n            else:\n                raise ConnectionProblem(\n                    \"Error while checking connection to Spatialite database: {}\".format(e)\n                )", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        return json.loads(content)\n    except ValueError:\n        pass\n    try:\n        return yaml.safe_load(content)\n    except yaml.YAMLError:\n        pass\n    raise BadMetadataError(\"Invalid metadata.json - must be valid JSON or YAML\")\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    sql = sql.rstrip(\";\")\n    named_parameters = _re_named_parameter.findall(sql)\n    if not named_parameters:\n        return named_parameters\n    explain_sql = \"EXPLAIN \" + sql\n    try:\n        explain_results = await db.execute(\n            explain_sql,\n            {named_parameter: None for named_parameter in named_parameters},\n        )\n    except Exception:\n        return named_parameters\n    explain_results = explain_results.fetchall()\n    explain_results = [row[0] for row in explain_results]\n    explain_results = \" \".join(explain_results)\n    explain_results = _re_named_parameter.findall(explain_results)\n    return [named_parameter[1:] for named_parameter in explain_results]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package.__name__", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if not isinstance(dotted, str):\n            raise ValueError('dotted must be a string')\n        if dotted.startswith('.'):\n            if self.package is CALLER_PACKAGE:\n                package = caller_package()\n            else:\n                package = self.package\n            if package is None:\n                raise ValueError(\n                    'relative name %r irresolveable without package' % (dotted,)\n                )\n            dotted = package.__name__ + dotted\n        return self._resolve(dotted, self.package)", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if not isinstance(dotted, str):\n            return dotted\n        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return self._resolve(dotted, package)", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n    if response is None:\n        response = request.response\n    result = helper.render_to_response(value, None, request=request)\n    return result", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        settings = self.registry.settings.get('renderers', {})\n        if settings is None:\n            settings = {}\n        return settings\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        renderer = self.renderer\n        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        registry = self.registry\n        registry.notify(BeforeRender(system, value=None))\n        result = renderer(value=None, system=system)\n        if result is not None:\n            if isinstance(result, str):\n                response.text = result\n            elif isinstance(result, bytes):\n                response.body = result\n            elif hasattr(result, '__iter__'):\n                response.app_iter = result\n            else:\n                response.body = result\n\n        return response", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system_values = system_values.copy()\n        system_values.update(\n            {\n                'view': None,\n                'renderer_name': self.name,\n                'renderer_info': self,\n                'context': None,\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n        )\n        self.registry.notify(SystemValues, system_values)\n        return self.renderer(value, system_values)", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        response = self._make_response(result, request)\n        return response", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name=name, package=package, registry=registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        return self.routelist", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(\n            name, pattern, factory, predicates, pregenerator\n        )\n        self.routes[name] = route\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routes[name] = route\n            self.routelist.append(route)\n        return route\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(\n                    'Key %r not found in renderer' % k\n                )\n            if self._received[k] != v:\n                raise AssertionError(\n                    'Value for key %r does not match: %r != %r'\n                    % (k, self._received[k], v)\n                )\n        return True", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        clone = copy.copy(self)\n        clone.__name__ = __name__\n        clone.__parent__ = __parent__\n        clone.kw = kw\n        clone.__dict__.update(**kw)\n        return clone", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        if '_csrft_' not in self:\n            return self.new_csrf_token()\n        return self['_csrft_']\n", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        from pyramid.response import Response\n\n        return Response()\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        allowed = set()\n        for location in reversed(list(lineage(context))):\n            try:\n                acl = location.__acl__\n            except AttributeError:\n                continue\n\n            if acl and callable(acl):\n                acl = acl()\n\n            for ace_action, ace_principal, ace_permissions in acl:\n                if not is_nonstr_iter(ace_permissions):\n                    ace_permissions = [ace_permissions]\n                if (ace_action == Allow) and (permission in ace_permissions):\n                    allowed.add(ace_principal)\n                if (ace_action == Deny) and (permission in ace_permissions):\n                    if ace_principal in allowed:\n                        allowed.remove(ace_principal)\n                    if ace_principal == Everyone:\n                        allowed = set()\n                        break\n        return allowed", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        route = self.registry.introspector.get('routes', route_name)\n        if route is None:\n            raise KeyError(route_name)\n        route_path = route['pattern']\n        if elements:\n            route_path += '/' + '/'.join(elements)\n        return self.route_url(route_name, route_path, **kw)", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        return getattr(self.func, '__text__', 'custom predicate: %s' % self.func)", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        return self.default()", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity was None; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n        userid = identity['repoze.who.userid']\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid was None; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'unauthenticated_userid returned disallowed %r; returning '\n                    '%r as if it was None' % (userid, None)\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n        if self.callback is None:\n            self.debug and self._log(\n                'security policy groups callback is None; returning %r'\n                % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        callback_ok = self.callback(identity, request)\n        if callback_ok is not None:  # is not None\n            self.debug and self._log(\n                'security policy groups callback returned %r; returning %r'\n                % (callback_ok, userid),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        self.debug and self._log(\n            'security policy groups callback returned None; returning None',\n            'authenticated_userid',\n            request,\n        )\n        return None", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity['repoze.who.userid']", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self.cookie.identify(request)\n        if identity is None:\n            return None\n        return identity['userid']", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            return credentials.username", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        if self.registry.session_factory is None:\n            raise ConfigurationError(\n                'No session factory registered.  Use the '\n                '\"pyramid.session_factory\" configuration directive to register '\n                'one.'\n            )\n        return self.registry.session_factory(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n\n        if request in self._store:\n            return self._store[request]\n\n        value = creator(request)\n        self.set(request, value)\n        return value", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request in self._store:\n            self._store[request] = value\n        else:\n            self._store[request] = value\n            request.add_finished_callback(self._remove)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if dirname is None:\n            dirname = os.path.realpath(os.curdir)\n        if locales is None:\n            locales = []\n        if not isinstance(locales, (list, tuple)):\n            locales = [locales]\n        if not isinstance(domain, str):\n            raise TypeError('domain must be a string')\n\n        for locale in locales:\n            if isinstance(locale, Locale):\n                locale = str(locale)\n            if locale:\n                locale = locale.replace('-', '_')\n            if locale:\n                locale = locale.replace('@', '_')\n            if locale:\n                locale = locale.replace('.', '_')\n            if locale:\n                locale = locale.replace(' ', '')\n            if locale:\n                locale = locale.lower()\n            if locale:\n                locale = locale.replace('utf8', 'UTF-8')\n            if locale:\n                locale = locale.replace('utf-8', 'UTF-8')\n            if locale:\n                locale = locale.replace('utf_8', 'UTF-8')\n            if locale:\n                locale = locale.replace('utf-32', 'UTF-32')\n            if locale:\n                locale = locale.replace('utf_32', 'UTF-32')\n            if locale:\n                locale = locale.replace('utf32', 'UTF-32')\n            if locale:\n                locale = locale.replace('utf_32', 'UTF-32')\n            if locale:\n                locale = locale.replace('utf-16', 'UTF-16')\n            if locale:\n                locale = locale.replace('utf_16', 'UTF-16')\n            if locale:\n                locale = locale.replace('utf16', 'UTF-16')\n            if locale:\n                locale = locale.replace('utf_16', 'UTF-16')\n            if locale:\n                locale = locale.replace('utf-16be', 'UTF-16BE')\n            if locale:\n                locale = locale.replace('utf_16be', 'UTF-16BE", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if not isinstance(translations, Translations):\n            raise TypeError(\n                'translations must be an instance of Translations'\n            )\n        if merge:\n            self._catalog.update(translations._catalog)\n        else:\n            self._catalog.clear()\n            self._catalog.update(translations._catalog)\n        self.files.extend(translations.files)\n        self._domains[translations.domain] = translations\n        return self", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for key, value in translations._catalog.items():\n            if key in self._catalog:\n                self._catalog[key] = value\n            else:\n                self._catalog[key] = value\n\n        self.files.extend(translations.files)\n        return self\n", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)\n", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.add_response_callback(\n            lambda request, response: self.cookie_profile.bind(request).apply(\n                response\n            )\n        )\n        return token", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<%s instance at %s with msg %r>' % (\n            self.__class__.__name__,\n            id(self),\n            self.msg,\n        )\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            callable = reify(callable)\n        return name, callable\n\n    ", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, {name: fn})", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            del self.name2val[name]\n            if name in self.name2before:\n                del self.name2before[name]\n            if name in self.name2after:\n                del self.name2after[name]\n            for a, b in self.order:\n                if a == name or b == name:\n                    self.order.remove((a, b))\n\n    ", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is not None:\n            if after is FIRST:\n                after = [self.first]\n            else:\n                after = [after]\n            self.req_after.update(after)\n            for a in after:\n                self.order.append((a, name))\n            self.name2after[name] = after\n        if before is not None:\n            if before is LAST:\n                before = [self.last]\n            else:\n                before = [before]\n            self.req_before.update(before)\n            for b in before:\n                self.order.append((name, b))\n            self.name2before[name] = before\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if is_nonstr_iter(path):\n        # the traverser factory expects PATH_INFO to be a string and it\n        # expects path segments to be utf-8 and\n        # urlencoded (it's the same traverser which accepts PATH_INFO\n        # from user agents; user agents always send strings).\n        if path:\n            path = _join_path_tuple(tuple(path))\n        else:\n            path = ''\n\n    # The user is supposed to pass us a string object, never Unicode.  In\n    # practice, however, users indeed pass Unicode to this API.  If they do\n    # pass a Unicode object, its data *must* be entirely encodeable to ASCII,\n    # so we encode it here as a convenience to the user and to prevent\n    # second-order failures from cropping up (all failures will occur at this\n    # step rather than later down the line as the result of calling\n    # ``traversal_path``).\n\n    path = ascii_(path)\n\n    if path and path[0] == '/':\n        resource = find_root(resource)\n\n    reg = get_current_registry()\n\n    request_factory = reg.queryUtility(IRequestFactory)\n    if request_factory is None:\n        from pyramid.request import Request  # avoid circdep\n\n        request_factory = Request\n\n    request = request_factory.blank(path)\n    request.registry = reg\n    traverser = reg.queryAdapter(resource, ITraverser)\n    if traverser is None:\n        traverser = ResourceTreeTraverser(resource)\n\n    return traverser(request)", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime != self._mtime:\n                    self._manifest = self.get_manifest()\n                    self._mtime = mtime\n            else:\n                self._manifest = {}\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if self.has_listeners:\n            Components.notify(self, *events)", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        if not IIntrospectable.providedBy(intr):\n            raise ValueError(\n                'object must provide IIntrospectable: %r' % intr\n            )\n        self._counter += 1\n        intr.order = self._counter\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        self._refs[intr] = []\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        return category.get(discriminator, default)", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.setdefault(category_name, {})\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        return sorted(\n            [\n                {'introspectable': intr, 'related': self.related(intr)}\n                for intr in category.values()\n            ],\n            key=sort_key,\n        )\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        return [\n            (category_name, self.get_category(category_name, sort_key=sort_key))\n            for category_name in sorted(self._categories.keys())\n        ]\n", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator)\n        if intr is not None:\n            del category[discriminator]\n            del category[intr.discriminator_hash]\n            del self._refs[intr]\n            for k, v in self._refs.items():\n                if intr in v:\n                    v.remove(intr)\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if y not in L:\n                L.append(y)", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        return self._refs.get(intr, [])", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.getUtility(IRoutesMapper)", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            shell_name = self.args.python_shell.lower()\n            if shell_name in shells:\n                return shells[shell_name]\n            else:\n                raise ValueError('could not find a shell named \"%s\"' % shell_name)\n        for shell_name in self.preferred_shells:\n            if shell_name in shells:\n                return shells[shell_name]\n        if shells:\n            return list(shells.values())[0]\n        return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith('/'):\n            self.overrides.insert(0, DirectoryOverride(path, source))\n        else:\n            self.overrides.insert(0, FileOverride(path, source))\n        return self.overrides[0]", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            result = override(resource_name)\n            if result is not None:\n                yield result\n\n    ", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                'This PackageOverrides instance has no real loader'\n            )\n        return self._real_loader\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            self.views.append((order, view, phash))\n            self.views.sort()\n            return\n        if accept not in self.media_views:\n            self.media_views[accept] = []\n        self.media_views[accept].append((order, view, phash))\n        self.media_views[accept].sort()\n        if accept not in self.accepts:\n            self.accepts.append(accept)\n            self.accepts.sort(key=accept_order)\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for accept in request.accept.offers(self.accepts):\n                views.extend(self.media_views.get(accept, []))\n            views.extend(self.views)\n            return views\n        return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if hasattr(view, '__predicated__'):\n                if view.__predicated__(context, request):\n                    return view\n            else:\n                return view\n        raise PredicateMismatch(self.name)\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                includepath=includepath,\n                info=info,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.src is None:\n            return ''\n        if self.line is None:\n            return self.src\n        if self.file is None:\n            return 'Line %s:\\n%s' % (self.line, self.src)\n        return 'Line %s of file %s:\\n%s' % (self.line, self.file, self.src)", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if name in self.registry._directives:\n            c, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                c = self.action_state.wrap(c)\n            return types.MethodType(c, self)\n        raise AttributeError(name)", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        if relative_spec.startswith('.'):\n            relative_spec = self.package_name + relative_spec\n        return self._make_spec(relative_spec)\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.get('request', _marker)\n        self.manager.push({'registry': self.registry, 'request': request})\n\n    ", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        if isinstance(package, str):\n            package = self.name_resolver.maybe_resolve(package)\n        scanner = self.venusian.Scanner(\n            categories=categories, onerror=onerror, ignore=ignore, **kw\n        )\n        scanner.scan(package)\n", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n\n        self.registry.registerUtility(self.registry, IRegistry)\n\n        self.registry.registerUtility(self.registry, IDebugLogger)\n\n        self.registry.registerUtility(self.registry, IIntrospector)\n\n        self.registry.registerUtility(self.registry, ISettings)\n\n        self.registry.registerUtility(self.registry, IAuthorizationPolicy)\n\n        self.registry.registerUtility(self.registry, IAuthenticationPolicy)\n\n        self.registry.registerUtility(self.registry, ISecurityPolicy)\n\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n\n        self.registry.registerUtility(self.registry, IViewMapper)\n\n        self.registry.registerUtility(self.registry, IView)\n\n        self.registry.registerUtility(self.registry, IViewClassifier)\n\n        self.registry.registerUtility(self.registry, IViewRenderer)\n\n        self.registry.registerUtility(self.registry, IViewRendererFactory)\n\n        self.registry.registerUtility(self.registry, IViewRendererInfo)\n\n        self.registry.registerUtility(self.registry, IViewMapper)\n\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n\n        self.registry.registerUtility(self.registry, IViewMapper)\n\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n\n        self.registry.registerUtility(self.registry, IViewMapper)\n\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n\n        self.registry.registerUtility(self.registry, IViewMapper)\n\n        self.registry.registerUtility(self.registry, IViewMapperFactory)", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(lambda c: '_' + c.group(0).lower(), name).lstrip('_')", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    for i in range(len(obj_parts) - 1, 0, -2):\n        parent_uri = \"/\".join(obj_parts[:i])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if parent_resource_name == resource_name:\n            return parent_uri\n\n    raise ValueError(f\"Could not find parent URI for {resource_name} in {object_uri}\")", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition[\"scopes\"]\n", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.settings[\"http_api_url\"],\n            \"schemes\": [self.settings[\"http_api_scheme\"]],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super().generate(swagger=base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    credentials = f\"{user}:{password}\".encode(\"utf-8\")\n    encoded = base64.b64encode(credentials).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {encoded}\"}", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if self.shared_ids is None:\n            self.shared_ids = self._get_accessible_objects(\n                principals,\n                self._object_id_match,\n                perm,\n                get_bound_permissions=get_bound_permissions,\n            )\n        return self.shared_ids", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if self.on_plural_endpoint:\n            if object_id is None:\n                # On a plural endpoint, the object id is the current\n                # resource name.\n                object_id = self.resource_name\n            else:\n                # If an object id is specified, we need to find the\n                # corresponding object URI.\n                # XXX: Rewrite using kinto.core.utils.view_lookup() and matchdict['id']\n                object_id = object_id.split(\"/\")[-1]\n                object_id = self._resource.get_object_uri(object_id)\n        else:\n            # On an object endpoint, the object id is the current object id.\n            if object_id is None:\n                object_id = self._resource.object_id\n            else:\n                object_id = self._resource.get_object_uri(object_id)\n        return object_id", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if isinstance(value, dict):\n            if key not in root:\n                root[key] = {}\n            recursive_update_dict(root[key], value, ignores)\n        elif key in ignores:\n            root.pop(key, None)\n        else:\n            root[key] = value\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            return value\n    return value", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n    for key in keys:\n        if \".\" in key:\n            # If the key contains a dot, it is a nested key\n            # Extract the root key and the nested key\n            root_key, nested_key = key.split(\".\", 1)\n            # Recursively call the function to get the nested value\n            nested_value = dict_subset(d.get(root_key, {}), [nested_key])\n            # Add the nested value to the result\n            result[key] = nested_value\n        else:\n            # If the key does not contain a dot, it is a root key\n            # Add the value to the result\n            result[key] = d.get(key)\n    return result\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    result = a.copy()\n    for key, value in b.items():\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = dict_merge(result[key], value)\n        else:\n            result[key] = value\n    return result", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    parts = path.split(\".\")\n    root_keys = [\".\".join(parts[:i]) for i in range(len(parts), 0, -1)]\n    for root_key in root_keys:\n        if root_key in d:\n            subpath = path[len(root_key) + 1 :]\n            return find_nested_value(d[root_key], subpath, default)\n\n    return default", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + resource_name\n\n    q = registry.queryUtility\n    routes_mapper = q(IRoutesMapper)\n\n    fakerequest = Request.blank(path=path)\n    info = routes_mapper(fakerequest)\n    matchdict, route = info[\"match\"], info[\"route\"]\n    if route is None:\n        raise ValueError(\"URI has no route\")\n\n    resource_name = route.name.replace(\"-object\", \"\").replace(\"-plural\", \"\")\n    return resource_name, matchdict", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\n            \"Please install Kinto with monitoring dependencies (e.g. \"\n            \"'pip install Kinto[monitoring]' or add statsd to the \"\n            \"requirements.txt of your project.\"\n        )\n\n    settings = config.get_settings()\n    uri = settings[\"statsd_url\"]\n    uri = urlparse(uri)\n\n    return Client(uri.hostname, uri.port, prefix=settings[\"statsd_prefix\"])", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n    if code is None:\n        code = httpexception.code\n    if error is None:\n        error = httpexception.title\n\n    response = httpexceptions.HTTPException(httpexception, code=code)\n    response.content_type = \"application/json\"\n    response.json = {\n        \"code\": code,\n        \"errno\": errno,\n        \"error\": error,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details,\n    }\n    return response", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default schemas for the endpoint type and method\n        default_schemas = getattr(self, f\"default_{endpoint_type}_schemas\")\n        default_method_schemas = getattr(self, f\"default_{method}_schemas\")\n\n        # Get the endpoint-specific schemas for the endpoint type\n        endpoint_schemas = getattr(self, f\"{endpoint_type}_schemas\", {})\n\n        # Get the method-specific schemas for the method\n        method_schemas = getattr(self, f\"{endpoint_type}_{method}_schemas\", {})\n\n        # Merge the default and endpoint-specific schemas\n        schemas = {**default_schemas, **endpoint_schemas}\n\n        # Merge the method-specific schemas into the merged schemas\n        schemas.update(method_schemas)\n\n        # Merge the default method schemas into the merged schemas\n        schemas.update(default_method_schemas)\n\n        # Create a dictionary of status codes mapping cloned and binded responses\n        responses = {}\n        for status, schema in schemas.items():\n            # Clone the schema\n            cloned_schema = schema.clone()\n\n            # Bind the schema with the method arguments and endpoint arguments\n            cloned_schema.bind(**kwargs)\n\n            # Add the cloned and binded schema to the responses dictionary\n            responses[status] = cloned_schema\n\n        return responses", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp()\n        except HTTPServiceUnavailable as e:\n            self.request.response.status_code = e.status_code\n            self.request.response.body = json.dumps(e.json)\n            self.request.response.content_type = \"application/json\"\n            raise e\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        # Plural endpoint are considered resources that always exist\n        self._raise_412_if_modified(obj={})\n\n        # If `data` is not provided, use empty object (creation)\n        post_object = self.request.validated[\"body\"].get(\"data\", {})\n\n        # If `id` is not provided, generate one\n        object_id = post_object.setdefault(self.model.id_field, self.id_generator())\n        self._raise_400_if_invalid_id(object_id)\n\n        new_object = self.process_object(post_object)\n\n        try:\n            obj = self.model.create_object(new_object)\n        except storage_exceptions.UnicityError:\n            # If the object already exists, return it.\n            obj = self.model.get_object(object_id)\n            self.request.response.status_code = 200\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(obj, action=ACTIONS.CREATE)", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        try:\n            obj = self._get_object_or_404(self.object_id)\n        except HTTPNotFound:\n            raise self._404_for_object(self.object_id)\n\n        self._raise_304_if_not_modified(obj)\n\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        # If `data` is not provided, use existing object (or empty if creation)\n        post_object = self.request.validated[\"body\"].get(\"data\", existing) or {}\n\n        object_id = post_object.setdefault(self.model.id_field, self.object_id)\n        self._raise_400_if_id_mismatch(object_id, self.object_id)\n\n        new_object = self.process_object(post_object, old=existing)\n\n        if existing:\n            obj = self.model.update_object(new_object)\n        else:\n            obj = self.model.create_object(new_object)\n            self.request.response.status_code = 201\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        action = existing and ACTIONS.UPDATE or ACTIONS.CREATE\n        return self.postprocess(obj, action=action, old=existing)", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals\n\n    ", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id) is not None", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        assert self.name is not None\n        assert self.schema_version is not None\n        assert self.schema_file is not None\n        assert self.migrations_directory is not None\n\n        current_version = self.get_installed_version()\n        if current_version is None:\n            self.create_schema(dry_run)\n        elif current_version == self.schema_version:\n            logger.info(\n                f\"PostgreSQL {self.name} schema is up-to-date at version {self.schema_version}.\"\n            )\n        else:\n            self.migrate_schema(current_version, dry_run)", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        cstruct = super().deserialize(cstruct)\n        defaults = cstruct.get(\"defaults\", {})\n        requests = cstruct[\"requests\"]\n        for request in requests:\n            request.update(defaults)\n        return cstruct", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    if not event.request.registry.settings.get(\"account_validation.enable\"):\n        return\n\n    for impacted in event.impacted_objects:\n        if (\n            impacted.get(\"old\", {}).get(\"validated\")\n            or not impacted.get(\"new\", {}).get(\"validated\")\n        ):\n            continue\n\n        Emailer(event.request, impacted[\"new\"]).send_confirmation_email()\n\n", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch userinfo\n        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        try:\n            resp = requests.get(userinfo_endpoint, headers=headers)\n            resp.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            logger.debug(f\"Failed to verify token: {e}\")\n            return None\n\n        return resp.json()", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.get_all(resource_name=\"bucket\", parent_id=\"\"):\n        bucket_id = bucket[\"id\"]\n        bucket_record_count = 0\n        bucket_storage_size = record_size(bucket)\n        bucket_collection_count = 0\n        for collection in storage.get_all(\n            resource_name=\"collection\", parent_id=f\"/buckets/{bucket_id}\"\n        ):\n            collection_record_count, collection_storage_size = rebuild_quotas_collection(\n                storage, bucket_id, collection, dry_run\n            )\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n            bucket_collection_count += 1\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_record_count} records, {bucket_storage_size} bytes, {bucket_collection_count} collections.\"\n        )\n        new_quota_info = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=f\"/buckets/{bucket_id}\",\n                object_id=\"\",\n                obj=new_quota_info,\n            )", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    template = os.path.join(HERE, template)\n    folder = os.path.dirname(destination)\n    if folder and not os.path.exists(folder):\n        os.makedirs(folder)\n\n    logger.info(f\"Created config {os.path.abspath(destination)}\")\n\n    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        raw_template = f.read()\n        rendered = raw_template.format_map(kwargs)\n        with codecs.open(destination, \"w+\", encoding=\"utf-8\") as output:\n            output.write(rendered)", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # extract\n        for match in HREFLANG_REGEX.findall(self.content):\n            # process middle part of the match tuple\n            if match == self.target_lang:\n                self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        for match in (m[0] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        if not is_plausible_sitemap(self.sitemap_url, self.content):\n            return\n\n        # try to extract links from a TXT file\n        if self.content.startswith('http'):\n            self.urls = DETECT_LINKS.findall(self.content)\n            LOGGER.debug('%s links found for %s', len(self.urls), self.sitemap_url)\n            return\n\n        # iterate through the content to find links and handle each link\n        if not self.sitemap_urls and not self.urls:\n            for link in DETECT_LINKS.findall(self.content):\n                self.handle_link(link)\n\n        # extract language links from the sitemap\n        if self.sitemap_urls or self.urls:\n            return\n\n        if self.target_lang:\n            self.extract_sitemap_langlinks()\n\n        # extract sitemap links from the sitemap\n        if not self.sitemap_urls and not self.urls:\n            self.extract_sitemap_links()", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    if contents is None:\n        return False\n    if not SITEMAP_FORMAT.match(contents):\n        return False\n    if not url.endswith(('.gz', 'sitemap', '.xml')):\n        return False\n    if not url.startswith(('http://', 'https://')):\n        return False\n    if not POTENTIAL_SITEMAP.search(url):\n        return False\n    return True", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemaps = []\n    for line in robotstxt.splitlines():\n        line = line.strip()\n        if line.startswith('#'):\n            continue\n        if line.startswith('Sitemap:'):\n            sitemap_url = line.split(':', 1)[1].strip()\n            if not sitemap_url.startswith('http'):\n                sitemap_url = baseurl + sitemap_url\n            sitemaps.append(sitemap_url)\n    return sitemaps", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    # check links\n    checked_links = [check_url(link, baseurl) for link in linklist]\n    # filter links\n    filtered_links = filter_urls(checked_links, domainname)\n    # fix relative URLs\n    fixed_links = [fix_relative_urls(baseurl, link) for link in filtered_links]\n    # remove duplicates\n    unique_links = list(set(fixed_links))\n    # remove links that are too short\n    long_links = [link for link in unique_links if len(link) > 10]\n    # remove links that are too long\n    short_links = [link for link in long_links if len(link) < 200]\n    # remove links that are not from the same domain\n    domain_links = [link for link in short_links if domainname in link]\n    # remove links that are not from the same language\n    if target_lang is not None:\n        lang_links = [link for link in domain_links if target_lang in link]\n    else:\n        lang_links = domain_links\n    # return the final list of links\n    return lang_links", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # get domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch the page\n    htmlstring = load_html(url)\n    # safeguard\n    if htmlstring is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', baseurl)\n        return []\n    # check if it's a feed\n    if '<feed' in htmlstring[:100] or '<rss' in htmlstring[:100]:\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    # could be a web page\n    else:\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n        feed_links = []\n        for feed_url in feed_urls:\n            feed_string = load_html(feed_url)\n            feed_links.extend(extract_links(feed_string, domainname, baseurl, url, target_lang))\n    # refine\n    output_links = handle_link_list(feed_links, domainname, baseurl, target_lang)\n    output_links = filter_urls(output_links, source_url=url, target_lang=target_lang)\n    # log result\n    if feed_links:\n        LOGGER.debug('Links found: %s of which %s valid', len(feed_links), len(output_links))\n    else:\n        LOGGER.debug('No feed links found: %s', baseurl)\n    return sorted(set(output_links))", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    content = CLEAN_XML.sub(\" \", content)\n    return urlsafe_b64encode(generate_bow_hash(content, length=length)).decode()", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # initialize download queue\n    if url_store.total_url_number() > MAX_FILES_PER_DIRECTORY:\n        counter = 0\n    else:\n        counter = None\n    # download strategy\n    errors = []\n    while url_store.done is False:\n        bufferlist, url_store = load_download_buffer(url_store, config.getfloat('DEFAULT', 'SLEEP_TIME'))\n        # start several threads\n        for url, result in buffered_downloads(bufferlist, args.parallel, decode=False):\n            counter = process_result(result, args, url, counter, config)\n            # just in case a crawl delay is specified in robots.txt\n            # sleep(spider.get_crawl_delay(spider.URL_STORE.get_rules(base_url)))\n        # early exit if maximum count is reached\n        if any(c >= args.max_count for c in url_store.get_all_counts()):\n            break\n    # print results\n    if args.list:\n        url_store.print_unvisited_urls()\n    # return errors\n    return errors, counter", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # GZip and Brotli test\n    filecontent = handle_compressed_file(response.data)\n    # encoding\n    for guessed_encoding in detect_encoding(filecontent):\n        try:\n            response = filecontent.decode(guessed_encoding)\n        except (LookupError, UnicodeDecodeError): # VISCII: lookup\n            LOGGER.warning('wrong encoding detected: %s', guessed_encoding)\n            response = None\n        else:\n            break\n    # return original content if nothing else succeeded\n    return response or str(filecontent, encoding='utf-8', errors='replace')", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # init\n    csv_data = []\n    # add header\n    csv_data.append('url\\tfingerprint\\thostname\\ttitle\\timage\\tdate\\tlicense\\tpagetype\\tid')\n    # add data\n    csv_data.append(f\"{docmeta['url']}\\t{docmeta['fingerprint']}\\t{docmeta['hostname']}\\t{docmeta['title']}\\t{docmeta['image']}\\t{docmeta['date']}\\t{docmeta['license']}\\t{docmeta['pagetype']}\\t{docmeta['id']}\")\n    # add comments\n    if comments:\n        csv_data.append(f\"\\n# {comments}\")\n    # add text\n    if text:\n        csv_data.append(f\"\\n{text}\")\n    # return\n    return '\\n'.join(csv_data)", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n    return similarity_ratio >= threshold\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter('*'):\n        if elem.tag not in TEI_REMOVE_TAIL and not elem.getchildren() and not elem.text and not elem.tail:\n            parent = elem.getparent()\n            if parent is not None:\n                parent.remove(elem)\n    return tree", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in reversed(tree.iter()):\n        if element.tag in NESTING_WHITELIST:\n            for child in element:\n                if child.tag in NESTING_WHITELIST:\n                    element.remove(child)\n                    element.extend(child.iterchildren())\n    return tree", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # remove unwanted elements\n    strip_tags(xmldoc, 'div', 'main', 'span')\n    # iterate and convert to list of strings\n    for element in xmldoc.iter('*'):\n        if element.text is None and element.tail is None:\n            if element.tag == 'graphic':\n                # add source, default to ''\n                text = f'{element.get(\"title\", \"\")} {element.get(\"alt\", \"\")}'\n                returnlist.extend(['![', text.strip(), ']', '(', element.get('src', ''), ')'])\n            # newlines for textless elements\n            if element.tag in ('graphic', 'row', 'table'):\n                returnlist.append('\\n')\n            continue\n        # process text\n        textelement = replace_element_text(element, include_formatting)\n        # common elements\n        if element.tag in NEWLINE_ELEMS:\n            returnlist.extend([NEWLINE_ELEMS[element.tag], textelement, '\\n'])\n        elif element.tag == 'comments':\n            returnlist.append('\\n\\n')\n        else:\n            if element.tag not in SPECIAL_FORMATTING:\n                LOGGER.debug('unprocessed element in output: %s', element.tag)\n            returnlist.extend([textelement, ' '])\n    return unescape(sanitize(''.join(returnlist)))\n\n", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as file:\n            TEI_RELAXNG = load_pickle(file)\n    result = TEI_RELAXNG.validate(xmldoc)\n    if result is False:\n        LOGGER.warning('TEI validation failed: %s', url)\n    return result", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is None:\n        return\n    if element.text is not None:\n        parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n    if element.tail is not None:\n        parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n    parent.remove(element)", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    if headers is None:\n        headers = DEFAULT_HEADERS\n    # load user-agents\n    myagents, mycookie = _parse_config(config)\n    # set user-agent\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    # set cookie\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    gc.collect()\n    reset_caches_courlan()\n    reset_caches_htmldate()\n    define_stoplist(stoplist=None, language='en')", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # init\n    newtable = Element('table')\n    newrow = Element('row')\n    # iterate\n    for child in table_elem.iter('*'):\n        # rows\n        if child.tag == 'tr':\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        # cells\n        elif child.tag in TABLE_ELEMS:\n            cell_element = define_cell_type(child)\n            # handle child elements\n            for subelem in child.iter('*'):\n                if subelem.tag not in potential_tags:\n                    LOGGER.debug('unexpected in table: %s %s %s', subelem.tag, subelem.text, subelem.tail)\n                    continue\n                # handle text\n                if subelem.tag not in TABLE_ALL:\n                    processed_subchild = process_node(subelem, options)\n                    if processed_subchild is not None:\n                        cell_element.text = processed_subchild.text\n                        if processed_subchild.tail is not None and processed_subchild.tail.strip():\n                            cell_element.text += \" \" + processed_subchild.tail\n                # handle table\n                elif subelem.tag == 'table':\n                    processed_subchild = handle_table(subelem, potential_tags, options)\n                    if processed_subchild is not None:\n                        cell_element.append(processed_subchild)\n                # handle other elements\n                else:\n                    processed_subchild = handle_textnode(subelem, options, comments_fix=False)\n                    if processed_subchild is not None:\n                        subchildelem = SubElement(cell_element, processed_subchild.tag)\n                        subchildelem.text, subchildelem.tail = processed_subchild.text, processed_subchild.tail\n                        # set attributes\n                        for attr in subelem.attrib:\n                            subchildelem.set(attr, subelem.get(attr))\n                subelem.tag = 'done'\n            # add cell\n            if len(cell_element) > 0 or text", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        result = language_classifier(temp_text, temp_comments)\n        if result is not None and result != target_language:\n            LOGGER.warning('Language %s not matching %s', result, target_language)\n            docmeta.language = result\n            return True, docmeta\n    return False, docmeta", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # if element.text is None:\n    #     return True\n    # if element.text.strip() in COMMENTS_BLACKLIST:\n    #     return True\n    # if RE_FILTER.match(element.text.strip()):\n    #     return True\n    # if element.text.strip().startswith('Mehr zum Thema: '):\n    #     return True\n    # if element.text.strip().startswith('More on this.'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.strip().startswith('Dieser Beitrag wurde'):\n    #     return True\n    # if element.text.", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for elem in tree.iterfind('.//script[@type]'):\n        if elem.get('type') in {'application/ld+json', 'application/settings+json'}:\n            if elem.text is not None:\n                # normalize JSON\n                json_text = JSON_MINIFY.sub('', elem.text)\n                # try to parse JSON\n                try:\n                    json_data = json.loads(json_text)\n                except json.JSONDecodeError:\n                    # try to parse JSON from text\n                    json_data = json.loads(json_text)\n                # extract metadata from JSON\n                if isinstance(json_data, dict):\n                    if '@type' in json_data and json_data['@type'] == 'NewsArticle':\n                        if 'headline' in json_data:\n                            metadata.title = json_data['headline']\n                        if 'author' in json_data:\n                            metadata.author = json_data['author']\n                        if 'datePublished' in json_data:\n                            metadata.date = json_data['datePublished']\n                        if 'description' in json_data:\n                            metadata.description = json_data['description']\n                        if 'image' in json_data:\n                            metadata.image = json_data['image']\n                        if 'publisher' in json_data:\n                            metadata.sitename = json_data['publisher']\n                        if 'keywords' in json_data:\n                            metadata.tags = json_data['keywords']\n                    elif '@type' in json_data and json_data['@type'] == 'Article':\n                        if 'headline' in json_data:\n                            metadata.title = json_data['headline']\n                        if 'author' in json_data:\n                            metadata.author = json_data['author']\n                        if 'datePublished' in json_data:\n                            metadata.date = json_data['datePublished']\n                        if 'description' in json_data:\n                            metadata.description = json_data['description']\n                        if 'image' in json_data:\n                            metadata.image = json_data['image']\n                        if 'pub", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init\n    postbody = Element('body')\n    # determine language\n    if target_language is None:\n        target_language = 'English'\n    # get stoplist\n    stoplist = get_stoplist(target_language)\n    if stoplist is None:\n        stoplist = get_stoplist('English')\n    # proceed\n    try:\n        paragraphs = custom_justext(tree, stoplist)\n        for paragraph in paragraphs:\n            if not paragraph.is_boilerplate:\n                postbody.append(paragraph.to_html_tree())\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None\n    return postbody", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        if isinstance(key, int) and key < 0:\n            key = len(self) + key\n        value = super().get(key, default)\n\n        if isinstance(value, BaseDocument) and value._instance is None:\n            value._instance = self._instance\n        elif isinstance(value, dict) and not isinstance(value, BaseDict):\n            value = BaseDict(value, None, f\"{self._name}.{key}\")\n            super().__setitem__(key, value)\n            value._instance = self._instance\n        elif isinstance(value, list) and not isinstance(value, BaseList):\n            value = BaseList(value, None, f\"{self._name}.{key}\")\n            super().__setitem__(key, value)\n            value._instance = self._instance\n        return value\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in all_column_types:\n                all_column_types[key] = set()\n            all_column_types[key].add(type(value))\n    return types_for_column_types(all_column_types)", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = pm.list_plugin_distinfo()\n    out = []\n    for name, (plugin, dist) in plugins:\n        d = {\"name\": name}\n        hooks = pm.get_hookcallers(plugin)\n        d[\"hooks\"] = [h.name for h in hooks]\n        if dist:\n            d[\"version\"] = dist.version\n            d[\"project_name\"] = dist.project_name\n        out.append(d)\n    return out", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.messaging_opts[\"quiet\"]:\n            if arg:\n                self.stdout.write(text % arg)\n            else:\n                self.stdout.write(text)\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=True,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    diffs = migration_script.upgrade_ops.as_diffs()\n    if diffs:\n        raise util.AutogenerateDiffsDetected(\n            f\"New upgrade operations detected: {diffs}\"\n        )\n    else:\n        config.print_stdout(\"No new upgrade operations detected.\")\n\n    return migration_script", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n    elif sql:\n        raise util.CommandError(\n            \"upgrade with --sql requires <fromrev>:<torev>\"\n        )\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def show_history(rev, context):\n        for sc in script.walk_revisions(\n            rev_range or \"base\", rev\n        ):  # type: ignore[arg-type]\n            cmd_line = sc.cmd_format(verbose, include_branches=True)\n            if indicate_current and sc.is_current:\n                cmd_line += \" <-- current\"\n            config.print_stdout(cmd_line)\n        return []\n\n    with EnvironmentContext(\n        config, script, fn=show_history, starting_rev=rev_range, rev=rev\n    ):\n        script.run_env()", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_stamp(rev, context):\n        return script._stamp_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        destination_rev=revision,\n        tag=tag,\n        purge=purge,\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version(rev, context):\n        script._ensure_version_table(rev)\n        return []\n\n    with EnvironmentContext(\n        config, script, fn=ensure_version, as_sql=sql, destination_rev=None\n    ):\n        script.run_env()\n\n", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = metadata_col.server_default\n    conn_default = conn_col.server_default\n\n    if metadata_default is None and conn_default is None:\n        return None\n\n    if isinstance(metadata_default, sa_schema.DefaultClause):\n        if isinstance(metadata_default.arg, str):\n            metadata_default = metadata_default.arg\n        else:\n            metadata_default = str(\n                metadata_default.arg.compile(\n                    dialect=autogen_context.dialect,\n                    compile_kwargs={\"literal_binds\": True},\n                )\n            )\n\n    if isinstance(conn_default, sa_schema.DefaultClause):\n        if isinstance(conn_default.arg, str):\n            conn_default = conn_default.arg\n        else:\n            conn_default = str(\n                conn_default.arg.compile(\n                    dialect=autogen_context.dialect,\n                    compile_kwargs={\"literal_binds\": True},\n                )\n            )\n\n    if isinstance(metadata_default, str):\n        metadata_default = _render_server_default_for_compare(\n            metadata_default, autogen_context\n        )\n    else:\n        metadata_default = None\n\n    if isinstance(conn_default, str):\n        conn_default = _render_server_default_for_compare(\n            conn_default, autogen_context\n        )\n    else:\n        conn_default = None\n\n    if metadata_default != conn_default:\n        if isinstance(metadata_col.server_default, sa_schema.Computed):\n            _compare_computed_default(\n                autogen_context,\n                alter_column_op,\n                schema,\n                tname,\n                cname,\n                conn_col,\n                metadata_col,\n            )\n        elif isinstance(metadata_col.server_default, sa_schema.Identity):\n            diff, is_alter = _compare_identity_default(\n                autogen_context,\n                alter_column_op,\n                schema,\n                t", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    if default is None:\n        return None\n\n    if isinstance(default, FetchedValue):\n        return _render_fetched_value(default, autogen_context)\n\n    if isinstance(default, str):\n        if repr_:\n            return \"'%s'\" % default.replace(\"'\", \"\\\\'\")\n        else:\n            return default\n\n    if isinstance(default, TextClause):\n        return _render_potential_expr(default, autogen_context)\n\n    if isinstance(default, ColumnElement):\n        return _render_potential_expr(default, autogen_context)\n\n    if sqla_compat._server_default_is_computed(default):\n        return _render_computed(default, autogen_context)\n\n    if sqla_compat._server_default_is_identity(default):\n        return _render_identity(default, autogen_context)\n\n    if isinstance(default, DefaultClause):\n        if isinstance(default.arg, str):\n            return _render_potential_expr(default, autogen_context)\n\n    return None\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer is not None:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"<unknown Python object %r>\" % constraint\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return _uq_constraint(constraint, autogen_context, False)", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.table is not None:\n        for c in constraint.table.constraints:\n            if c is constraint:\n                return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sCheckConstraint(%(sqltext)r, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"sqltext\": constraint.sqltext,\n        \"args\": \", \".join(\n            [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }\n\n", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    opts = context.opts\n\n    # the set of tables to be compared, derived from the\n    # target metadata.  this is the \"right side\" of the comparison.\n    target_tables = set(metadata.tables.values())\n\n    # the set of tables to be compared, derived from the\n    # current database.  this is the \"left side\" of the comparison.\n    current_tables = set(context.inspector.get_sorted_table_and_fkc_names())\n\n    # the set of all tables to be compared.  this is the\n    # union of the left and right side.\n    all_tables = target_tables.union(current_tables)\n\n    # the set of all schemas to be compared.  this is the\n    # union of the left and right side.\n    all_schemas = set(\n        [\n            schema\n            for schema in target_tables.union(current_tables)\n            if schema.schema is not None\n        ]\n    )\n\n    # the set of all sequences to be compared.  this is the\n    # union of the left and right side.\n    all_sequences = set(\n        [\n            sequence\n            for sequence in target_tables.union(current_tables)\n            if sequence.schema is not None\n        ]\n    )\n\n    # the set of all enums to be compared.  this is the\n    # union of the left and right side.\n    all_enums = set(\n        [\n            enum\n            for enum in target_tables.union(current_tables)\n            if enum.schema is not None\n        ]\n    )\n\n    # the set of all types to be compared.  this is the\n    # union of the left and right side.\n    all_types = set(\n        [\n            type_\n            for type_ in target_tables.union(current_tables)\n            if type_.schema is not None\n        ]\n    )\n\n    # the set of all functions to be compared.  this is the\n    # union of the left and right side.\n    all_functions = set(\n        [\n            function\n            for function", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        try:\n            yield\n        finally:\n            self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connection.dialect.has_table(\n            connection, tablename, schemaname\n        )\n    else:\n        return connection.dialect.has_table(\n            connection.connection, tablename, schemaname\n        )\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is None:\n        return None\n    assert dialect is not None\n    if sqla_14:\n        name = dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n        return name\n    else:\n        name = quoted_name(constraint.name, constraint.schema)\n        return name.replace('\"', \"\")", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"%s://user:pass@host/db\" % dialect\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    config = _testing_config()\n    with open(config.config_file_name, \"w\") as f:\n        f.write(text)\n    return config", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script.generate_revision(\n        a, \"revision a\", refresh=True, head=\"base\"\n    )\n    write_script(\n        script,\n        a,\n        \"\"\"\\", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = create_mock_engine(dialect)\n    buf = io.StringIO()\n    return engine, buf", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        t = sa_schema.Table(\n            source,\n            self.metadata(),\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        u = sa_schema.UniqueConstraint(*local_cols, name=name, **kw)\n        t.append_constraint(u)\n        return u", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        if tablename is not None:\n            t = sa_schema.Table(tablename, m, schema=schema)\n        else:\n            t = sa_schema.Table(\"_\", m)\n        return sa_schema.Index(name, *columns, **kw)", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, PrimaryKeyConstraint):\n            return cls.from_constraint(constraint)\n        elif isinstance(constraint, UniqueConstraint):\n            return cls.from_constraint(constraint)\n        elif isinstance(constraint, ForeignKeyConstraint):\n            return cls.from_constraint(constraint)\n        elif isinstance(constraint, CheckConstraint):\n            return cls.from_constraint(constraint)\n        else:\n            raise ValueError(\n                \"DropConstraintOp cannot be created from constraint of type %s\"\n                % type(constraint)\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table = sqla_compat.Table(\n                self.table_name,\n                sqla_compat.MetaData(),\n                schema=self.schema,\n            )\n            return constraint\n        else:\n            raise ValueError(\"No reverse operation available\")\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            columns=index.columns,\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            table.columns.values(),\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            **table.kwargs,\n        )\n\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=dict(table.kwargs),\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        return (\n            \"modify_column\",\n            self.schema,\n            self.table_name,\n            self.column_name,\n            self.existing_type,\n            self.existing_server_default,\n            self.existing_nullable,\n            self.existing_comment,\n            self.modify_nullable,\n            self.modify_server_default,\n            self.modify_type,\n            self.modify_comment,\n        )\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"Cannot reverse a DropColumnOp that was not generated by \"\n                \"AddColumnOp.from_column_and_tablename()\"\n            )\n        return self._reverse\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema, _reverse=AddColumnOp(tname, col))\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse:\n            return self._reverse.to_column(migration_context)\n        else:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            return schema_obj.column(self.column_name, NULLTYPE)\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map\n        return self.heads\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                del self._revision_map[revision.revision]\n            else:\n                raise RevisionError(\n                    \"Revision %s is present more than once\"\n                    % revision.revision\n                )\n        self._revision_map[revision.revision] = revision\n        if revision.branch_labels:\n            assert revision._orig_branch_labels is not None\n            for branch_label in revision._orig_branch_labels:\n                if branch_label in self._revision_map:\n                    map_rev = self._revision_map[branch_label]\n                    assert map_rev is not None\n                    raise RevisionError(\n                        \"Branch name '%s' in revision %s already \"\n                        \"used by revision %s\"\n                        % (\n                            branch_label,\n                            revision.revision,\n                            map_rev.revision,\n                        )\n                    )\n                self._revision_map[branch_label] = revision\n\n        self._add_depends_on([revision], self._revision_map)\n        self._normalize_depends_on([revision], self._revision_map)\n\n        self._update_bases(revision, self._revision_map)\n        self._update_real_bases(revision, self._revision_map)\n\n        self._check_and_add_referenced_revisions(revision, self._revision_map)\n\n        self._update_heads(revision, self._revision_map)\n        self._update_real_heads(revision, self._revision_map)\n\n        self._add_branches([revision], self._revision_map)\n", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n\n        if isinstance(id_, str):\n            id_ = (id_,)\n\n        if isinstance(id_, tuple):\n            id_ = list(id_)\n\n        if isinstance(id_, list):\n            return tuple(\n                self._revision_for_ident(\n                    self._resolve_revision_number(id_)[0][0]\n                )\n                for id_ in id_\n            )\n\n        raise ValueError(f\"Invalid id_ type: {type(id_)}\")\n", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        if id_ == \"base\":\n            return self._revision_map[None]\n        elif id_ == \"head\":\n            return self._revision_map[self.get_current_head()]\n        elif id_ == \"heads\":\n            return self._revision_map[self.get_current_head()]\n        else:\n            return self._revision_map[id_]", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n\n        branch_rev = self._resolve_branch(check_against)\n        if branch_rev is None:\n            return ()\n\n        return tuple(\n            target\n            for target in targets\n            if self._shares_lineage(target, branch_rev.revision)\n        )\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if upper is None:\n            upper = \"base\"\n        if lower is None:\n            lower = \"head\"\n\n        if upper == \"base\" and lower == \"head\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"head\" and lower == \"head\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"head\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"head\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"head\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case: no need to do anything.\n            return\n\n        if upper == \"base\" and lower == \"base\":\n            # Special case:", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Initialize a dictionary to store the in-degree of each node\n        in_degree = {node: 0 for node in revisions}\n\n        # Initialize a queue to store the nodes with zero in-degree\n        queue = collections.deque()\n\n        # Initialize a list to store the topological order\n        order = []\n\n        # Compute the in-degree of each node\n        for node in revisions:\n            for child in node.nextrev:\n                in_degree[child] += 1\n\n        # Add the nodes with zero in-degree to the queue\n        for node in revisions:\n            if in_degree[node] == 0:\n                queue.append(node)\n\n        # Perform the topological sort\n        while queue:\n            node = queue.popleft()\n            order.append(node)\n            for child in node.nextrev:\n                in_degree[child] -= 1\n                if in_degree[child] == 0:\n                    queue.append(child)\n\n        # Check if the graph has a cycle\n        if len(order) != len(revisions):\n            raise DependencyCycleDetected(\n                [node.revision for node in revisions]\n            )\n\n        return order\n", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    try:\n        fn = _registry[name]\n    except KeyError:\n        raise util.CommandError(\n            f\"No formatter with name '{name}' registered\"\n        ) from None\n    else:\n        return fn(revision, options)", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        node = self._cache.get(page)\n        if node is None:\n            data = self._read_page(page)\n            node = Node.load(data, self._tree_conf)\n            self._cache[page] = node\n        return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self.last_page += 1\n        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        data = self._read_page(0)\n        root_node_page = int.from_bytes(\n            data[0:PAGE_REFERENCE_BYTES], ENDIAN\n        )\n        page_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES+OTHERS_BYTES],\n            ENDIAN\n        )\n        order = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES+OTHERS_BYTES:\n                 PAGE_REFERENCE_BYTES+2*OTHERS_BYTES],\n            ENDIAN\n        )\n        key_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES+2*OTHERS_BYTES:\n                 PAGE_REFERENCE_BYTES+3*OTHERS_BYTES],\n            ENDIAN\n        )\n        value_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES+3*OTHERS_BYTES:\n                 PAGE_REFERENCE_BYTES+4*OTHERS_BYTES],\n            ENDIAN\n        )\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        return root_node_page, tree_conf", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._write_page_in_tree(0, data)\n        self._tree_conf = tree_conf\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n            self._not_committed_pages = dict()\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        self._fd.seek(0)\n        for page, page_start in self._committed_pages.items():\n            data = read_from_file(self._fd, page_start,\n                                  page_start + self._page_size)\n            yield page, data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            fsync_file_and_dir(self._dir_fd, None)", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n\n    ", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n\n        if self.overflow_page:\n            value = b''\n            used_value_length = 0\n        else:\n            value = self.value\n            used_value_length = len(value)\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            bytes(self._tree_conf.key_size - used_key_length) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value +\n            bytes(self._tree_conf.value_size - used_value_length) +\n            (self.overflow_page or 0).to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data\n", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )\n\n", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n        used_page_length = len(data)\n        header = self._node_type_int.to_bytes(\n            NODE_TYPE_BYTES, ENDIAN\n        )\n        header += used_page_length.to_bytes(\n            USED_PAGE_LENGTH_BYTES, ENDIAN\n        )\n        if self.next_page is None:\n            next_page = 0\n        else:\n            next_page = self.next_page\n        header += next_page.to_bytes(\n            PAGE_REFERENCE_BYTES, ENDIAN\n        )\n        data[0:len(header)] = header\n        padding_length = self._tree_conf.page_size - len(data)\n        data.extend(b'\\x00' * padding_length)\n        return data", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type))\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        root_node = self._root_node\n        while True:\n            if isinstance(root_node, (LonelyRootNode, LeafNode)):\n                return root_node\n            root_node = self._mem.get_node(root_node.smallest_entry.after)\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n            if not issubclass(extension_class, Extension):\n                logger.warning(\n                    \"Skipping extension %s: not a subclass of Extension\",\n                    entry_point.name,\n                )\n                continue\n        except ImportError as exc:\n            logger.warning(\n                \"Skipping extension %s: failed to import: %s\",\n                entry_point.name,\n                exc,\n            )\n            continue\n\n        extension = extension_class()\n        config_schema = extension.get_config_schema()\n        config_defaults = extension.get_default_config()\n        command = extension.get_command()\n\n        data = ExtensionData(\n            extension=extension,\n            entry_point=entry_point,\n            config_schema=config_schema,\n            config_defaults=config_defaults,\n            command=command,\n        )\n\n        if validate_extension_data(data):\n            installed_extensions.append(data)\n\n    return installed_extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    if data.extension.ext_name != data.entry_point.name:\n        logger.error(\n            \"Extension name %s does not match entry point name %s\",\n            data.extension.ext_name,\n            data.entry_point.name,\n        )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.exception(\n            f\"Failed to validate environment for extension {data.extension.ext_name}: {e}\"\n        )\n        return False\n\n    try:\n        data.extension.get_config_schema()\n    except Exception as e:\n        logger.exception(\n            f\"Failed to get config schema for extension {data.extension.ext_name}: {e}\"\n        )\n        return False\n\n    try:\n        data.extension.get_default_config()\n    except Exception as e:\n        logger.exception(\n            f\"Failed to get default config for extension {data.extension.ext_name}: {e}\"\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name:\n        return f\"{name} {get_version()} Python/{platform.python_version()}\"\n    else:\n        return f\"Mopidy/{get_version()} Python/{platform.python_version()}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key}\")\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        return config_lib.read(os.path.join(os.path.dirname(__file__), \"ext.conf\"))", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_default_config()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Path()\n        schema[\"zeroconf\"] = config_lib.String()\n        schema[\"allowed_origins\"] = config_lib.List()\n        schema[\"csrf_protection\"] = config_lib.Boolean()\n        schema[\"default_app\"] = config_lib.String()\n        schema[\"default_app_config\"] = config_lib.Dict()\n        schema[\"websocket_host\"] = config_lib.Hostname()\n        schema[\"websocket_port\"] = config_lib.Port()\n        return schema", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM).close()\n        return True\n    except socket.error:\n        logger.debug(\"IPv6 not supported\")\n        return False", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if \":\" in hostname:\n        # IPv6 address\n        if hostname.startswith(\"::ffff:\"):\n            # IPv4-mapped IPv6 address\n            return hostname[7:]\n        else:\n            # IPv6 address\n            return f\"[{hostname}]\"\n    else:\n        # IPv4 address\n        return hostname\n\n", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    xdg_config_dir = pathlib.Path(\n        os.environ.get(\"XDG_CONFIG_HOME\", \"~/.config\")\n    ).expanduser()\n    xdg_cache_dir = pathlib.Path(\n        os.environ.get(\"XDG_CACHE_HOME\", \"~/.cache\")\n    ).expanduser()\n    xdg_data_dir = pathlib.Path(\n        os.environ.get(\"XDG_DATA_HOME\", \"~/.local/share\")\n    ).expanduser()\n    xdg_state_dir = pathlib.Path(\n        os.environ.get(\"XDG_STATE_HOME\", \"~/.local/state\")\n    ).expanduser()\n    xdg_runtime_dir = pathlib.Path(\n        os.environ.get(\"XDG_RUNTIME_DIR\", \"~/.local/runtime\")\n    ).expanduser()\n\n    xdg_dirs = {\n        \"XDG_CONFIG_DIR\": xdg_config_dir,\n        \"XDG_CACHE_DIR\": xdg_cache_dir,\n        \"XDG_DATA_DIR\": xdg_data_dir,\n        \"XDG_STATE_DIR\": xdg_state_dir,\n        \"XDG_RUNTIME_DIR\": xdg_runtime_dir,\n    }\n\n    user_dirs = _get_user_dirs(xdg_config_dir)\n    xdg_dirs.update(user_dirs)\n\n    return xdg_dirs", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    if args_verbosity_level is not None:\n        verbosity_level = base_verbosity_level + args_verbosity_level\n    else:\n        verbosity_level = base_verbosity_level + logging_config[\"verbosity\"]\n\n    min_level = min(LOG_LEVELS.keys())\n    max_level = max(LOG_LEVELS.keys())\n    if verbosity_level < min_level:\n        verbosity_level = min_level\n    if verbosity_level > max_level:\n        verbosity_level = max_level\n\n    return verbosity_level", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not isinstance(arg, list):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n    for element in arg:\n        if not isinstance(element, cls):\n            raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif urllib.parse.urlparse(arg).scheme == \"\":\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    [check_uri(a, msg) for a in arg]", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        detect_extm3u_header: parse_extm3u,\n        detect_pls_header: parse_pls,\n        detect_xspf_header: parse_xspf,\n        detect_asx_header: parse_asx,\n    }\n\n    for detector, parser in handlers.items():\n        if detector(data):\n            return list(parser(data))\n\n    return list(parse_urilist(data))", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        errors = {}\n        result = {}\n\n        for key, value in values.items():\n            if key in self:\n                try:\n                    result[key] = self[key].deserialize(value)\n                except ValueError as e:  # deserialization failed\n                    result[key] = None\n                    errors[key] = str(e)\n            else:\n                errors[key] = \"Unknown config key: {}\".format(key)\n\n        # Check for deprecated keys\n        for key in list(result.keys()):\n            if key not in self:\n                del result[key]\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return encode(value)\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return \"********\"\n        return super().serialize(value)", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value).strip().lower()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if value in self.true_values:\n            return True\n        elif value in self.false_values:\n            return False\n        else:\n            raise ValueError(f\"{value} is not a boolean\")\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if self._separator in value:\n            first_value, second_value = value.split(self._separator, 1)\n        elif self._optional_pair:\n            first_value = second_value = value\n        else:\n            raise ValueError(\n                f\"config value must include separator: {self._separator}\"\n            )\n\n        first_value = self._subtypes[0].deserialize(first_value)\n        second_value = self._subtypes[1].deserialize(second_value)\n\n        return first_value, second_value", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_values = [\n            self._subtypes[i].serialize(value[i], display=display)\n            for i in range(2)\n        ]\n\n        if display and self._optional_pair and serialized_values[0] == serialized_values[1]:\n            return serialized_values[0]\n\n        return f\"{serialized_values[0]}{self._separator}{serialized_values[1]}\"", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n        return \"\\n\".join(subtype.serialize(v, display=display) for v in value)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return encode(value.lower())", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value in log.COLORS:\n            return encode(log.COLORS[value])\n        return \"\"", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value).strip().lower()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if value not in self.levels:\n            raise ValueError(\n                f\"invalid value for log level: {value} (valid values are: {', '.join(self.levels.keys())})\"\n            )\n\n        return self.levels[value]", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        if value in self.levels.values():\n            return next(\n                (k for k, v in self.levels.items() if v == value), \"\"\n            )\n        return \"\"", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.startswith(\"unix:\"):\n            return str(value)\n        try:\n            socket.getaddrinfo(value, None)\n            return value\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value}\")\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n    raw_config = _load(files, defaults, overrides)\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n\n    config, errors = _validate(raw_config, schemas)\n\n    if errors:\n        raise ValueError(f\"Errors in config: {errors}\")\n\n    return Proxy(config)\n\n", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    # Get default config for each extension\n    defaults = []\n    for extension_data in extensions_data:\n        defaults.append(read(extension_data.path / \"default.conf\"))\n\n    # Load config from a series of config files\n    raw_config = _load([], defaults, [])\n\n    # Validate config against schemas\n    schemas = _schemas[:]\n    for extension_data in extensions_data:\n        schemas.extend(extension_data.schema)\n    config, errors = _validate(raw_config, schemas)\n\n    # Create header with version information for each extension\n    versions = []\n    for extension_data in extensions_data:\n        versions.append(f\"{extension_data.name} {extension_data.version}\")\n    header = _INITIAL_HELP.format(versions=\", \".join(versions))\n\n    # Format config\n    return _format(config, {}, schemas, True, True)\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    parser = configparser.RawConfigParser()\n    parser.comment_prefixes = (\"#\", \";\")\n\n    for default in defaults:\n        parser.read_string(_preprocess(default))\n\n    for file_path in files:\n        file_path = pathlib.Path(file_path)\n        if file_path.is_dir():\n            for file_path in file_path.glob(\"*.conf\"):\n                _load_file(parser, file_path)\n        else:\n            _load_file(parser, file_path)\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n    for schema in schemas:\n        if schema.name not in raw_config:\n            continue\n        section_config = raw_config[schema.name]\n        try:\n            validated_config[schema.name] = schema.deserialize(section_config)\n        except ValueError as e:\n            errors[schema.name] = str(e)\n    return validated_config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    result = []\n    for (instr, descs) in six.iteritems(_known):\n        if instrument is None or instr.find(instrument) == 0:\n            for (desc, tun) in six.iteritems(descs[1]):\n                if (\n                    nr_of_strings is None\n                    or tun.count_strings() == nr_of_strings\n                ) and (\n                    nr_of_courses is None\n                    or tun.count_courses() == nr_of_courses\n                ):\n                    result.append(tun)\n    return result", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        notes = []\n        for x in self.bar:\n            notes += x[2].get_note_names()\n        return (max(notes), min(notes))\n", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        from mingus.core import chords\n        res = []\n        for x in self.bar:\n            res.append(\n                [\n                    x[0],\n                    chords.determine(\n                        x[2].get_note_names(), self.key.key, shorthand\n                    ),\n                ]\n            )\n        return res\n\n    ", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.int_to_note(notes.note_to_int(self.name) + interval)\n        else:\n            self.name = notes.int_to_note(notes.note_to_int(self.name) - interval)\n\n    ", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if not isinstance(integer, int):\n            raise NoteFormatError(\"Note.from_int() requires an int\")\n        if integer < 0:\n            raise NoteFormatError(\"Note.from_int() requires a positive int\")\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = int(integer / 12)\n        return self\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        return standard_pitch * 2 ** (int(self) / 12.0)\n\n    ", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # TODO: Review this.\n        # This is a bit of a hack, but it works.\n        # We need to find the closest note to the hertz value.\n        # We do this by calculating the difference between the hertz value and the note's hertz value.\n        # We then compare the difference to the difference between the hertz value and the next note's hertz value.\n        # If the difference is smaller than the difference between the hertz value and the next note's hertz value, we set the note to the current note.\n        # If the difference is larger, we set the note to the next note.\n        # We do this for all notes.\n        # The note with the smallest difference is the closest note.\n        # We then set the note's name and octave to the closest note's name and octave.\n        # We then return the note.\n        # This is a bit of a hack, but it works.\n        # We need to find the closest note to the hertz value.\n        # We do this by calculating the difference between the hertz value and the note's hertz value.\n        # We then compare the difference to the difference between the hertz value and the next note's hertz value.\n        # If the difference is smaller than the difference between the hertz value and the next note's hertz value, we set the note to the current note.\n        # If the difference is larger, we set the note to the next note.\n        # We do this for all notes.\n        # The note with the smallest difference is the closest note.\n        # We then set the note's name and octave to the closest note's name and octave.\n        # We then return the note.\n        # This is a bit of a hack, but it works.\n        # We need to find the closest note to the hertz value.\n        # We do this by calculating the difference between the hertz value and the note's hertz value.\n        # We then compare the difference to the difference between the hertz value and the next note's hertz value.\n        # If the difference is smaller than the difference between the hertz value and the next note's hertz value, we", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return notes.to_shorthand(self.name, self.octave)\n\n    ", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        notes = chords.from_shorthand(shorthand)\n        self.add_notes(notes)\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.empty()\n        self.add_notes(intervals.from_shorthand(startnote, shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_progression_shorthand(shorthand, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return [n.name for n in self.notes]\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"The note_int must be in the range 0-11\")\n    note = fifths[note_int]\n    if accidentals == \"#\":\n        return note\n    elif accidentals == \"b\":\n        return note + \"b\"\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % accidentals)\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if not isinstance(note, str):\n        return False\n    if len(note) == 1:\n        return note in _note_dict\n    elif len(note) == 2:\n        return note[0] in _note_dict and note[1] in [\"#\", \"b\"]\n    else:\n        return False", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    val = 0\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n    result = note[0]\n    while val > 0:\n        result = augment(result)\n        val -= 1\n    while val < 0:\n        result = diminish(result)\n        val += 1\n    return result", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 4)", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 9)", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if note1[0] == note2[0]:\n        return 0\n\n    # [name, half notes for major version of this interval]\n    fifth_steps = [\n        [\"unison\", 0],\n        [\"fifth\", 7],\n        [\"second\", 2],\n        [\"sixth\", 9],\n        [\"third\", 4],\n        [\"seventh\", 11],\n        [\"fourth\", 5],\n    ]\n\n    # Count half steps between note1 and note2\n    half_notes = 0\n    n1 = notes.fifths.index(note1[0])\n    n2 = notes.fifths.index(note2[0])\n    number_of_fifth_steps = n2 - n1\n    if n2 < n1:\n        number_of_fifth_steps = len(notes.fifths) - n1 + n2\n\n    # Get the proper list from the number of fifth steps\n    current = fifth_steps[number_of_fifth_steps]\n\n    # maj = number of major steps for this interval\n    maj = current[1]\n\n    # if maj is equal to the half steps between note1 and note2 the interval is\n    # major or perfect\n    if maj == half_notes:\n        return maj\n    elif maj + 1 <= half_notes:\n        # if maj + 1 is equal to half_notes, the interval is augmented.\n        return maj + half_notes\n    elif maj - 1 == half_notes:\n        # etc.\n        return maj - half_notes\n    elif maj - 2 >= half_notes:\n        return maj - half_notes\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # [interval name, interval function, shorthand]\n    interval_lookup = [\n        [\"unison\", major_unison, \"1\"],\n        [\"second\", major_second, \"2\"],\n        [\"third\", major_third, \"3\"],\n        [\"fourth\", major_fourth, \"4\"],\n        [\"fifth\", major_fifth, \"5\"],\n        [\"sixth\", major_sixth, \"6\"],\n        [\"seventh\", major_seventh, \"7\"],\n    ]\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Define a dictionary to map shorthand interval names to their corresponding intervals\n    interval_dict = {\n        \"1\": 0,\n        \"b2\": 1,\n        \"2\": 2,\n        \"b3\": 3,\n        \"3\": 4,\n        \"4\": 5,\n        \"b5\": 6,\n        \"5\": 7,\n        \"b6\": 8,\n        \"6\": 9,\n        \"b7\": 10,\n        \"7\": 11,\n    }\n\n    # Check if the input is valid\n    if note not in notes.fifths or interval not in interval_dict:\n        return False\n\n    # Get the index of the starting note in the list of notes\n    start_index = notes.fifths.index(note)\n\n    # Get the number of half steps in the interval\n    half_steps = interval_dict[interval]\n\n    # Calculate the index of the resulting note\n    if up:\n        result_index = (start_index + half_steps) % 12\n    else:\n        result_index = (start_index - half_steps) % 12\n\n    # Return the resulting note\n    return notes.fifths[result_index]", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    dhalf = measure(note1, note2)\n    return dhalf in [0, 2, 4, 5, 7, 9, 11] or include_fourths and dhalf == 5", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    return measure(note1, note2) in [0, 3, 4, 5, 7]", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals > 7:\n        accidentals = accidentals % 7\n    elif accidentals < -7:\n        accidentals = accidentals % -7\n\n    if accidentals >= 0:\n        return major_keys[accidentals], minor_keys[accidentals]\n    else:\n        return minor_keys[abs(accidentals)], major_keys[abs(accidentals)]", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key not in major_keys:\n        raise NoteFormatError(\"'%s' is not a valid key.\" % key)\n    return major_keys.index(key) - 7", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    accidentals = get_key_signature(key)\n    if accidentals == 0:\n        return []\n    elif accidentals > 0:\n        return [\"sharp \" + base_scale[i] for i in range(accidentals)]\n    else:\n        return [\"flat \" + base_scale[i] for i in range(abs(accidentals))]", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in _key_cache:\n        return _key_cache[key]\n\n    notes = base_scale[:]\n    for i in range(get_key_signature(key)):\n        if i < 0:\n            notes = notes[1:] + [notes[0] + \"b\"]\n        else:\n            notes = notes[:-1] + [notes[-1] + \"#\"]\n\n    _key_cache[key] = notes\n    return notes\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if key[0].islower():\n        return key.upper()\n    else:\n        return key[0].lower()", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), notes.augment(intervals.perfect_fifth(note))]", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Check if the chord is a polychord\n    if len(chord) > 4:\n        if no_polychords:\n            return []\n        else:\n            return determine_polychords(chord, shorthand)\n\n    # Check if the chord is a seventh chord\n    if len(chord) == 4:\n        return determine_seventh(chord, shorthand, no_inversions)\n\n    # Check if the chord is a triad\n    if len(chord) == 3:\n        return determine_triad(chord, shorthand, no_inversions)\n\n    # Check if the chord is an extended chord\n    if len(chord) == 5:\n        return determine_extended_chord5(chord, shorthand, no_inversions)\n\n    # Check if the chord is an extended chord\n    if len(chord) == 6:\n        return determine_extended_chord6(chord, shorthand, no_inversions)\n\n    # Check if the chord is an extended chord\n    if len(chord) == 7:\n        return determine_extended_chord7(chord, shorthand, no_inversions)\n\n    # If the chord is not recognized, return an empty list\n    return []", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    # Determine the base value\n    for i in range(len(base_values)):\n        if base_values[i] == value:\n            base_value = base_values[i]\n            break\n        elif base_values[i] > value:\n            base_value = base_values[i - 1]\n            break\n\n    # Determine the number of dots\n    dots = 0\n    while value - base_value > 0:\n        value -= dots(base_value)\n        dots += 1\n\n    # Determine the ratio\n    if value == base_value:\n        ratio = 1\n    elif value == base_triplets[base_values.index(base_value)]:\n        ratio = 3\n    elif value == base_quintuplets[base_values.index(base_value)]:\n        ratio = 5\n    elif value == base_septuplets[base_values.index(base_value)]:\n        ratio = 7\n    else:\n        ratio = 0\n\n    return base_value, dots, ratio\n\n", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    if depth == 0:\n        return substitute_harmonic(progression, substitute_index)\n    else:\n        res = []\n        for sub in substitute_harmonic(progression, substitute_index):\n            res.extend(substitute(progression, substitute_index, depth - 1))\n        return res", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    return numerals[(numerals.index(roman_numeral) + skip_count) % 7]\n\n", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.DEBUG\n    else:\n        level = logging.WARNING\n\n    # Setup the logger.\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # Setup the stderr handler.\n    stderr_handler = logging.StreamHandler(stream=sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n    logger.addHandler(stderr_handler)\n\n    # Setup the stdout handler.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(stream=sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n        logger.addHandler(stdout_handler)", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory for the bundle.\n    root_directory = tempfile.mkdtemp(prefix='exodus-bundle-')\n\n    # Populate the filename template.\n    output_filename = render_template(output,\n        executables=('-'.join(os.path.basename(executable) for executable in executables)),\n        extension=('tgz' if tarball else 'sh'),\n    )\n\n    # Store a gzipped tarball of the bundle in memory.\n    tar_stream = io.BytesIO()\n    with tarfile.open(fileobj=tar_stream, mode='w:gz') as tar:\n        tar.add(root_directory, arcname='exodus')\n\n    # Configure the appropriate output mechanism.\n    if output_filename == '-':\n        base64_encoded_tarball = base64.b64encode(tar_stream.getvalue()).decode('utf-8')\n        script_content = render_template_file('install-bundle-noninteractive.sh',\n            base64_encoded_tarball=base64_encoded_tarball)\n        output_file.write(script_content.encode('utf-8'))\n    else:\n        output_file.write(render_template_file('install-bundle.sh').encode('utf-8'))\n        output_file.write(tar_stream.getvalue())\n\n    # Write out the success message.\n    logger.info('Successfully created \"%s\".' % output_filename)\n    return True\n\n    # Create a bundle object.\n    bundle = Bundle(working_directory=True, chroot=chroot)\n\n    # Add the executables.\n    for index, executable in enumerate(executables):\n        entry_point = rename[index] if index < len(rename) else None\n        bundle.add_file(executable, entry_point=entry_point)\n\n    # Add the additional files.\n    for path in add:\n        bundle.add_file(path)\n\n    # Create the bundle.\n    bundle.create_bundle(shell_launc", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        first_four_bytes = f.read(4)\n        return first_four_bytes == b'\\x7fELF'", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if not os.path.exists(binary):\n        for directory in os.environ['PATH'].split(':'):\n            path = os.path.join(directory, binary)\n            if os.path.exists(path):\n                return path\n        raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n    return binary", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    absolute_path = os.path.normpath(os.path.abspath(path))\n    if not os.path.exists(absolute_path):\n        if search_environment_path:\n            absolute_path = resolve_binary(path)\n        else:\n            raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(absolute_path):\n        raise UnexpectedDirectoryError('The \"%s\" file is a directory.' % path)\n    return absolute_path", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a binary ELF file.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    combined_output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n    return combined_output", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = self.find_direct_dependencies(linker_file)\n        new_dependencies = dependencies\n        while new_dependencies:\n            new_dependencies = set()\n            for dependency in dependencies:\n                new_dependencies |= dependency.elf.find_direct_dependencies(linker_file)\n            new_dependencies -= dependencies\n            dependencies |= new_dependencies\n        return dependencies\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = self.file_factory(path, entry_point=entry_point, chroot=self.chroot)\n        self.files.add(file)\n        if file.elf and file.elf.linker_file:\n            self.linker_files.add(file.elf.linker_file)\n        return file", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.normpath(os.path.abspath(os.path.join(self.working_directory, 'bundles', self.hash)))\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        hashes = [file.hash for file in self.files]\n        hashes.sort()\n        combined = ''.join(hashes).encode('utf-8')\n        return hashlib.sha256(combined).hexdigest()\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = '1' if full_linker else '0'\n    code = render_template_file('launcher.sh', linker_basename=linker_basename,\n                                linker_dirname=linker_dirname, library_path=library_path,\n                                executable=executable, full_linker=full_linker)\n    return compile(code)", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    prefixes = [\n        'open(\"',\n        'open64(\"',\n        'openat(\"',\n        'openat64(\"',\n    ]\n    for prefix in prefixes:\n        if line.startswith(prefix):\n            parts = line[len(prefix):].split('\", ')\n            if len(parts) == 2:\n                return parts[0]\n    return None", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content.splitlines():\n        path = extract_exec_path(line)\n        if path:\n            paths.append(path)\n            continue\n        path = extract_open_path(line)\n        if path:\n            paths.append(path)\n            continue\n        path = extract_stat_path(line)\n        if path:\n            paths.append(path)\n            continue\n    if existing_only:\n        paths = [path for path in paths if os.path.exists(path) and not os.path.isdir(path)]\n    return paths", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, tz=timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return \".\"\n\n    if _requires_normalization(path) is not None:\n        raise ValueError(\"path contains illegal back-references\")\n\n    initial_slashes = path.startswith(\"/\")\n    is_abs = initial_slashes\n    split_path = path.split(\"/\")\n    new_path = []\n    for p in split_path:\n        if p == \"..\":\n            try:\n                if new_path[-1] == \"..\":\n                    new_path.append(p)\n                elif new_path[-1] == \".\":\n                    new_path[-1] = p\n                else:\n                    if new_path[-1] == \"\":\n                        if is_abs:\n                            continue\n                    else:\n                        new_path.pop()\n            except IndexError:\n                if not is_abs:\n                    new_path.append(p)\n        elif p == \".\" or p == \"\":\n            continue\n        else:\n            new_path.append(p)\n\n    new_path = \"/\".join(new_path)\n    if initial_slashes:\n        new_path = \"/\" + new_path\n\n    if new_path == \"\":\n        new_path = \".\"\n\n    return new_path", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    if path == \"/\":\n        return [\"/\"]\n    path = path.lstrip(\"/\")\n    path = path.rstrip(\"/\")\n    return path.split(\"/\")", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    if reverse:\n        path = relpath(normpath(path))\n        path_list = path.split(\"/\")\n        path_list.reverse()\n        return path_list\n    else:\n        path = abspath(normpath(path))\n        path_list = path.split(\"/\")\n        path_list.pop(0)\n        return path_list", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    if not paths:\n        return \"\"\n    absolute = isabs(paths[0])\n    paths = [p.strip(\"/\") for p in paths if p not in \"/\"]\n    path = \"/\".join(paths)\n    if absolute:\n        path = \"/\" + path\n    return path", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    if not path:\n        return []\n    _path = path.strip(\"/\")\n    if \"//\" in _path:\n        _path = normpath(_path)\n    if not _path:\n        return []\n    return _path.split(\"/\")", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    head, tail = os.path.split(path)\n    return head, tail", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return abspath(path1) == abspath(path2)", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1):]", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if base == path:\n        return \"\"\n    if base == \"/\":\n        return path\n    if base.endswith(\"/\"):\n        base = base[:-1]\n    if path.startswith(base):\n        return path[len(base) :]\n    return path\n\n", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=True) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        if _has_drive_letter(path_snippet):\n            drive_letter, path = path_snippet.split(\":\", 1)\n            return \"{drive_letter}:{path}\".format(\n                drive_letter=drive_letter, path=six.moves.urllib.request.pathname2url(path)\n            )\n        else:\n            return six.moves.urllib.request.pathname2url(path_snippet)\n    else:\n        return six.moves.urllib.request.pathname2url(path_snippet)", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    return [parse_line(line) for line in lines if line.strip()]", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for format in formats:\n        try:\n            return time.mktime(time.strptime(t, format))\n        except ValueError:\n            pass\n    return None", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if not isinstance(ls, six.string_types):\n            raise ValueError(\"permissions must be a string\")\n        if len(ls) != 10:\n            raise ValueError(\"permissions string must be 10 characters long\")\n        if ls[0] not in (\"d\", \"-\"):\n            raise ValueError(\"first character must be '-' or 'd'\")\n        if ls[1] not in (\"r\", \"-\"):\n            raise ValueError(\"second character must be '-' or 'r'\")\n        if ls[2] not in (\"w\", \"-\"):\n            raise ValueError(\"third character must be '-' or 'w'\")\n        if ls[3] not in (\"x\", \"-\"):\n            raise ValueError(\"fourth character must be '-' or 'x'\")\n        if ls[4] not in (\"r\", \"-\"):\n            raise ValueError(\"fifth character must be '-' or 'r'\")\n        if ls[5] not in (\"w\", \"-\"):\n            raise ValueError(\"sixth character must be '-' or 'w'\")\n        if ls[6] not in (\"x\", \"-\"):\n            raise ValueError(\"seventh character must be '-' or 'x'\")\n        if ls[7] not in (\"r\", \"-\"):\n            raise ValueError(\"eighth character must be '-' or 'r'\")\n        if ls[8] not in (\"w\", \"-\"):\n            raise ValueError(\"ninth character must be '-' or 'w'\")\n        if ls[9] not in (\"x\", \"-\"):\n            raise ValueError(\"tenth character must be '-' or 'x'\")\n\n        perms = []\n        if ls[1] == \"r\":\n            perms.append(\"u_r\")\n        if ls[2] == \"w\":\n            perms.append(\"u_w\")\n        if ls[3] == \"x\":\n            perms.append(\"u_x\")\n        if ls[4] == \"r\":\n            perms.append(\"g_r\")\n        if ls[5] == \"w\":\n            perms.append(\"g_", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(init)\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\"):\n            return \"\"\n        return \".\" + name.split(\".\")[-1]\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return name.split(\".\")[1:]\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return name\n        basename, dot, ext = name.rpartition(\".\")\n        return basename\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return cast(ResourceType, self.get(\"details\", \"type\"))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        mech_config = get_mech_config(limit)\n\n        hosts = []\n        current_host = None\n\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                if current_host:\n                    hosts.append(current_host)\n\n                current_host = {\n                    \"Host\": line.split(\" \")[1],\n                }\n\n            elif current_host is not None and \"=\" in line:\n                key, value = line.split(\"=\")\n                current_host[key] = value\n\n        if current_host:\n            hosts.append(current_host)\n\n        names_data = []\n        for host in hosts:\n            name, data, groups = _make_name_data(host)\n            names_data.append((name, data, groups))\n\n        return names_data\n", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided.\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError((\"Could not find Ansible inventory file: {0}\").format(inventory_filename))\n\n        show_warning()\n\n        return parse_inventory(inventory_filename)\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    if not isinstance(flags, list):\n        flags = [flags]\n\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the local directory exists\n    if not os.path.isdir(dest) and create_local_dir:\n        os.makedirs(dest)\n\n    # Ensure the local file exists\n    local_filename = os.path.join(dest, os.path.basename(src))\n    if not os.path.isfile(local_filename) or force:\n        yield FileDownloadCommand(src, local_filename)\n    else:\n        host.noop(\"file {0} is already downloaded\".format(local_filename))", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if not hasattr(src, \"read\") and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not os.path.exists(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    # Ensure the source file is a file\n    if not os.path.isfile(src):\n        raise IOError(\"Not a file: {0}\".format(src))\n\n    # Ensure the source file is readable\n    if not os.access(src, os.R_OK):\n        raise IOError(\"Cannot read: {0}\".format(src))\n\n    # Get the file info\n    file_info = host.get_fact(File, path=dest)\n\n    # If we're uploading a directory ensure it's a directory\n    if os.path.isdir(src):\n        if file_info is not False:\n            if not file_info or not file_info[\"is_directory\"]:\n                raise IOError(\"Destination is not a directory: {0}\".format(dest))\n\n    # If we're uploading a file ensure it's a file\n    if os.path.isfile(src):\n        if file_info is not False:\n            if not file_info or file_info[\"is_directory\"]:\n                raise IOError(\"Destination is not a file: {0}\".format(dest))\n\n    # If we're uploading a file ensure the destination is a file\n    if os.path.isfile(src) and not file_info:\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, dest, user, group)\n\n        yield FileUploadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # If we're uploading a directory ensure the destination is a directory\n    if os.path.is", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    if present and not touch:\n        raise OperationError(\"If present is True touch must be True\")\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n    else:  # edit\n        changed = False\n\n        # Check mode\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    kwargs.pop(\"state\", None)\n    kwargs.pop(\"host\", None)\n    yield FunctionCommand(function, args, kwargs)\n\n", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the meta kwargs (globals that apply to all hosts)\n    global_arguments, global_argument_keys = pop_global_arguments(kwargs)\n\n    # If this op is being called inside another, just return here\n    # (any unwanted/op-related kwargs removed above).\n    if host.in_op and not host.in_callback_op:\n        if global_argument_keys:\n            _error_msg = \"Nested operation called with global arguments: {0} ({1})\".format(\n                global_argument_keys,\n                get_call_location(),\n            )\n            raise PyinfraError(_error_msg)\n        return func(*args, **kwargs) or []\n\n    names, add_args = _generate_operation_name(func, host, kwargs, global_arguments)\n    op_order, op_hash = _solve_operation_consistency(names, state, host)\n\n    # Ensure shared (between servers) operation meta, mutates state\n    op_meta = _ensure_shared_op_meta(state, op_hash, op_order, global_arguments, names)\n\n    # Attach normal args, if we're auto-naming this operation\n    if add_args:\n        op_meta = _attach_args(op_meta, args, kwargs)\n\n    # Check if we're actually running the operation on this host\n    # Run once and we've already added meta for this op? Stop here.\n    if op_meta.global_arguments[\"_run_once\"]:\n        has_run = False\n        for ops in state.ops.values():\n            if op_hash in ops:\n                has_run = True\n                break\n\n        if has_run:\n            return OperationMeta(op_hash)\n\n    # \"Run\" operation - here we make a generator that will yield out actual commands to execute\n    # and, if we're diff-ing, we then iterate the generator now to determine if any changes\n    # *would* be made based on the *current* remote state.\n\n    def command_generator() ->", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    facts = {}\n    greenlets = []\n    for host in state.inventory:\n        greenlets.append(\n            gevent.spawn(\n                get_facts,\n                state,\n                host,\n                *args,\n                **kwargs,\n            ),\n        )\n    gevent.joinall(greenlets)\n    for greenlet in greenlets:\n        facts.update(greenlet.value)\n    return facts\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    greenlets = []\n\n    for host in state.inventory:\n        greenlets.append(gevent.spawn(host.connect, state.config.SUDO, state.config.SUDO_PASSWORD))\n\n    gevent.joinall(greenlets, raise_error=True)\n\n    for host in state.inventory:\n        if host.is_connected:\n            state.activate_host(host)", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    # Pop the keys from the kwargs\n    popped_kwargs: dict[str, Any] = {}\n    found_keys: list[str] = []\n    for key in keys_to_check:\n        if key in kwargs:\n            popped_kwargs[key] = kwargs.pop(key)\n            found_keys.append(key)\n\n    # Pop the keys from the host data\n    if host is not None:\n        for key in keys_to_check:\n            if key in host.data:\n                popped_kwargs[key] = host.data.pop(key)\n                found_keys.append(key)\n\n    # Pop the keys from the deploy context\n    if state is not None:\n        for key in keys_to_check:\n            if key in state.deploy_kwargs:\n                popped_kwargs[key] = state.deploy_kwargs.pop(key)\n                found_keys.append(key)\n\n    # Pop the keys from the config\n    for key in keys_to_check:\n        if key in state.config.GLOBAL_KWARGS:\n            popped_kwargs[key] = state.config.GLOBAL_KWARGS[key]\n            found_keys.append(key)\n\n    # Translate the keys from non-prefixed to prefixed\n    for key in keys_to_check:\n        if key.startswith(\"_\"):\n            continue\n\n        prefixed_key = f\"_{key}\"\n        if prefixed_key in keys_to_check and prefixed_key not in popped_kwargs:\n            if key in popped_kwargs:\n                popped_kwargs[prefixed_key] = popped_kwargs.pop(key)\n                found_keys.append(prefixed_key)\n\n    # Translate the keys from prefixed to non-prefixed\n    for key in keys_to_check:\n        if not key.startswith(\"_\"):\n            continue\n\n        non_", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation_name = commands[0]\n    operation_func = try_import_module_attribute(operation_name, prefix=\"pyinfra.operations\")\n\n    args = []\n    kwargs = {}\n\n    for command in commands[1:]:\n        if \"=\" in command:\n            key, value = command.split(\"=\", 1)\n            kwargs[key] = json.loads(value)\n        else:\n            args.append(command)\n\n    return operation_func, args, kwargs", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.overload_print()\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"include_files and exclude_files cannot be specified at the same time\")\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                self.generate_report(sys.stdout, \"html\", file_info)\n            elif output_file.endswith(\".json\"):\n                self.generate_report(sys.stdout, \"json\", file_info)\n            elif output_file.endswith(\".gz\"):\n                with gzip.open(output_file, \"wb\") as f:\n                    self.generate_report(f, \"json\", file_info)\n            else:\n                raise ValueError(\"Unknown output file format\")\n        else:\n            self.generate_report(output_file, \"json\", file_info)\n        self.final_messages.append((\"view_command\", {\"output_file\": output_file}))\n        self.print_messages()", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, ast.Tuple) or isinstance(node, ast.List):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n\n        new_source = []\n        for line in source.split(\"\\n\"):\n            for pattern, transform in self.re_patterns:\n                re_match = pattern.match(line)\n                if re_match:\n                    new_source.append(transform(self, re_match))\n                    break\n            else:\n                new_source.append(line)\n        return \"\\n\".join(new_source)", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        logline = []\n        if msg:\n            logline.append('MSG: {0}'.format(msg))\n        if detail:\n            logline.append('DETAIL: {0}'.format(detail))\n        if hint:\n            logline.append('HINT: {0}'.format(hint))\n        if structured:\n            logline.append('STRUCTURED: {0}'.format(\n                WalELogger._fmt_structured(structured)))\n\n        return '\\n'.join(logline)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            os.remove(key.path)\n        remove_empty_dirs(common_dir_path([k.path for k in keys]))", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise Exception('Pool closed')\n\n        if self.member_burden + len(tpart) > self.max_members:\n            raise Exception('Too much work outstanding')\n\n        if self.concurrency_burden >= self.max_concurrency:\n            raise Exception('Too much concurrency')\n\n        self._start(tpart)\n\n        # Check for errors in previously submitted greenlets.\n        while True:\n            gevent.sleep(0)\n            if self.wait_change.balance < 0:\n                self._wait()\n            else:\n                break", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Iterate through the files in the archive_status directory\n        for f in os.listdir(path.join(xlog_dir, 'archive_status')):\n            # Filter out non-segment files\n            if not re.match(storage.SEGMENT_REGEXP, f):\n                continue\n\n            # Create a WalSegment instance for each segment file\n            yield WalSegment(path.join(xlog_dir, f))\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        self.closed = True\n        self.wait_change.put(None)\n\n        # Wait for all greenlets to exit.\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # Force greenlets to exit.\n        for g in self.greenlets:\n            g.kill()\n\n        # Wait for all greenlets to exit.\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # Raise any exceptions that occurred.\n        for g in self.greenlets:\n            if g.exception:\n                raise g.exception", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        self.expect += 1\n        g = gevent.Greenlet(self.transferer, segment)\n        self.greenlets.add(g)\n        g.link(self._complete_execution)\n        g.start()", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        return s.decode('utf-8', 'ignore')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        # Create a dictionary of redefined methods\n        redefined_methods = {\n            'mapper': self.mapper,\n            'combiner': self.combiner,\n            'reducer': self.reducer,\n            'mapper_init': self.mapper_init,\n            'mapper_final': self.mapper_final,\n            'mapper_cmd': self.mapper_cmd,\n            'mapper_pre_filter': self.mapper_pre_filter,\n            'mapper_raw': self.mapper_raw,\n            'reducer_init': self.reducer_init,\n            'reducer_final': self.reducer_final,\n            'reducer_cmd': self.reducer_cmd,\n            'reducer_pre_filter': self.reducer_pre_filter,\n            'combiner_init': self.combiner_init,\n            'combiner_final': self.combiner_final,\n            'combiner_cmd': self.combiner_cmd,\n            'combiner_pre_filter': self.combiner_pre_filter,\n            'spark': self.spark,\n            'spark_args': self.spark_args\n        }\n\n        # Remove any methods that are not redefined\n        redefined_methods = {k: v for k, v in redefined_methods.items() if v != getattr(MRJob, k)}\n\n        # Create a list of MRStep objects\n        steps = []\n        for i, (step_type, step_method) in enumerate(redefined_methods.items()):\n            if step_type == 'spark':\n                steps.append(SparkStep(\n                    name='Spark Step',\n                    spark_method=step_method,\n                    spark_args=step_method()\n                ))\n            else:\n                kwargs = {step_type: step_method}\n                if step_type == 'spark_args':\n                    kwargs['spark_args'] = step_method()\n                steps.append(MRStep(\n                    name", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        self.stderr.write('reporter:status:%s\\n' % msg)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose)\n\n        # make a runner\n        runner = self.make_runner()\n\n        # start the job\n        runner.run()\n\n        # print the output\n        if self._should_cat_output():\n            for line in runner.cat_output():\n                self.stdout.write(line)\n                self.stdout.flush()\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        if quiet:\n            log_level = logging.WARNING\n        elif verbose:\n            log_level = logging.DEBUG\n        else:\n            log_level = logging.INFO\n\n        # Set up the logger for the main module\n        logger = logging.getLogger(__name__)\n        logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob module\n        mrjob_logger = logging.getLogger('mrjob')\n        mrjob_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.runner module\n        mrjob_runner_logger = logging.getLogger('mrjob.runner')\n        mrjob_runner_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.emr module\n        mrjob_emr_logger = logging.getLogger('mrjob.emr')\n        mrjob_emr_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.fs module\n        mrjob_fs_logger = logging.getLogger('mrjob.fs')\n        mrjob_fs_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.parse module\n        mrjob_parse_logger = logging.getLogger('mrjob.parse')\n        mrjob_parse_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.spark module\n        mrjob_spark_logger = logging.getLogger('mrjob.spark')\n        mrjob_spark_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.step module\n        mrjob_step_logger = logging.getLogger('mrjob.step')\n        mrjob_step_logger.setLevel(log_level)\n\n        # Set up the logger for the mrjob.util module\n        mrjob_util_logger = logging.getLogger('mrjob.util')\n        mrjob_util_logger.setLevel(log_level)", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.run_mapper:\n            self.run_mapper()\n        elif self.options.run_combiner:\n            self.run_combiner()\n        elif self.options.run_reducer:\n            self.run_reducer()\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        kwargs = self._non_option_kwargs()\n        kwargs.update(self._kwargs_from_switches(\n            ('bootstrap_mrjob', 'bootstrap_python', 'bootstrap_cmds',\n             'cleanup', 'cmdenv', 'cmdenv_whitelist', 'hadoop_bin',\n             'hadoop_extra_args', 'hadoop_streaming_jar_on_emr',\n             'hadoop_version', 'label', 'owner', 'python_bin',\n             'setup_cmds', 'setup_mrjob', 'setup_python', 'upload_files',\n             'upload_dirs', 'upload_archives', 'verbose', 'quiet',\n             'no_output', 'no_output_dir', 'no_cleanup', 'no_conf',\n             'no_pass_through_opts', 'no_strict_protocols', 'no_steps',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_opts', 'no_strict_protocols',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_opts', 'no_strict_protocols',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_opts', 'no_strict_protocols',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_opts', 'no_strict_protocols',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_opts', 'no_strict_protocols',\n             'no_cleanup', 'no_output', 'no_output_dir', 'no_steps',\n             'no_conf', 'no_pass_through_", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        step = self._get_step(step_num, MRStep)\n\n        combiner = step['combiner']\n        combiner_init = step['combiner_init']\n        combiner_final = step['combiner_final']\n\n        if combiner_init:\n            for k, v in combiner_init() or ():\n                yield k, v\n\n        for k, v in self.combine_pairs(self._read_input(), step_num=step_num):\n            for k, v in combiner(k, v) or ():\n                yield k, v\n\n        if combiner_final:\n            for k, v in combiner_final() or ():\n                yield k, v", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'passthru options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"passthru options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.runner == 'inline'\n", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        read = self.output_protocol().read\n\n        for chunk in chunks:\n            for line in to_lines(chunk):\n                yield read(line)\n", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem(\n                [\n                    self.fs.hadoop,\n                    self.fs.local,\n                ]\n            )\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for path in self._hadoop_streaming_jar_dirs():\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n            for root, dirs, files in os.walk(path):\n                for filename in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(filename):\n                        return os.path.join(root, filename)\n\n        return None\n", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # check if there are any Hadoop streaming steps\n        if any(step['type'] == 'streaming' for step in self._get_steps()):\n            self.get_hadoop_streaming_jar()\n\n        # check if there are any Spark steps\n        if any(_is_spark_step_type(step['type']) for step in self._get_steps()):\n            self.get_spark_submit_bin()\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n\n        if not hadoop_streaming_jar:\n            raise ValueError('no Hadoop streaming jar')\n\n        args = []\n\n        args.extend(self.get_hadoop_bin())\n\n        args.extend(['jar', hadoop_streaming_jar])\n\n        args.extend(self._interpolate_streaming_step_args(step_num))\n\n        return args\n", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            path = self.fs.join(log_dir, 'history')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for history log in %s...' % path)\n                yield [path]\n\n    ", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                if application_id:\n                    task_log_dir = posixpath.join(\n                        log_dir, 'userlogs', application_id)\n                else:\n                    task_log_dir = posixpath.join(log_dir, 'userlogs')\n\n                log.info('Looking for task logs in %s...' % task_log_dir)\n                yield [task_log_dir]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return self.uri(path)\n\n        name = name_uniquely(path, self._names_taken, unhide=True)\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n\n        return self.uri(path)", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            raise ValueError('%r is not a known local file' % (path,))\n\n        return posixpath.join(self.prefix, self._path_to_name[path])", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return {path: self.uri(path) for path in self._path_to_name}\n", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        name_to_path = {}\n        for name, (path_type, path) in self._name_to_typed_path.items():\n            if type is None or path_type == type:\n                name_to_path[name] = path\n        return name_to_path", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for typed_path in self._name_to_typed_path.values():\n            if type is None or typed_path[0] == type:\n                paths.add(typed_path[1])\n\n        return paths", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(variable, {}).values():\n        if alternative in os.environ:\n            return os.environ[alternative]\n\n    return default", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if not isinstance(variable, string_types):\n        raise TypeError('%r is not a string' % variable)\n\n    if not isinstance(version, string_types):\n        raise TypeError('%r is not a string' % version)\n\n    if version_gte(version, '2'):\n        return variable\n\n    if version_gte(version, '1'):\n        return _JOBCONF_MAP.get(variable, {}).get('1.0', variable)\n\n    return _JOBCONF_MAP.get(variable, {}).get('0.20', variable)", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].values())\n    else:\n        return [variable]", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        raise TypeError\n\n    translated_jobconf = {}\n    unrecognized_jobconf = []\n    for key, value in jobconf.items():\n        if key in _JOBCONF_MAP:\n            translated_key = map_version(hadoop_version, _JOBCONF_MAP[key])\n            if translated_key != key:\n                unrecognized_jobconf.append(key)\n            translated_jobconf[translated_key] = value\n        else:\n            translated_jobconf[key] = value\n\n    if unrecognized_jobconf:\n        log.warning(\n            \"Detected hadoop configuration property names that do not match version %s:\\n\"\n            \"The have been translated to the following names:\\n%s\",\n            hadoop_version,\n            '\\n'.join(sorted(\n                '%s:%s' % (key, map_version(hadoop_version, _JOBCONF_MAP[key]))\n                for key in unrecognized_jobconf)))\n\n    return dict(jobconf, **translated_jobconf)", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if version is None:\n        raise TypeError\n\n    return version >= '2.0'", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_cores']\n        cores_per_executor = self._opts['cores_per_executor']\n        executor_memory = self._opts['executor_memory']\n\n        if cores_per_executor is None:\n            cores_per_executor = num_executors\n\n        if executor_memory is None:\n            executor_memory = _DEFAULT_EXECUTOR_MEMORY\n\n        return 'local-cluster[{},{},{}]'.format(\n            num_executors,\n            cores_per_executor,\n            int(math.ceil(_to_num_bytes(executor_memory) / 1024 / 1024))\n        )\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(item) for item in x]\n    elif isinstance(x, dict):\n        result = {}\n        for k, v in x.items():\n            if isinstance(v, ClearedValue):\n                if isinstance(v.value, dict):\n                    result.update(_fix_clear_tags(v.value))\n                else:\n                    result[k] = v.value\n            else:\n                result[k] = _fix_clear_tags(v)\n        return result\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    confs = []\n    for conf_path in conf_paths:\n        confs.extend(load_opts_from_mrjob_conf(runner_alias, conf_path))\n\n    if not confs:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return confs", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=4, sort_keys=True)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    for seq in reversed(seqs):\n        if seq is not None:\n            if isinstance(seq, string_types):\n                return [seq]\n            elif isinstance(seq, bytes):\n                return [seq]\n            elif not isinstance(seq, list):\n                return [seq]\n            else:\n                return seq\n    else:\n        return []", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is None:\n            continue\n\n        if isinstance(cmd, string_types):\n            return shlex_split(cmd)\n        else:\n            return list(cmd)\n\n    return None", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            result[k] = v\n\n    return result", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                # delete cleared key\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # just set the value\n                else:\n                    result[k] = _to_java_str(v)\n\n    return result", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    path_seqs = [path_seqs] if isinstance(path_seqs, string_types) else path_seqs\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n    path_seqs = [path_seqs] if isinstance(path_seqs, tuple) else path_seqs\n    path_seqs = [path_seqs] if isinstance(path_seqs, set) else path_seqs\n    path_seqs = [path_seqs] if isinstance(path_seqs, dict) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs, list) else path_seqs\n\n    path_seqs = [path_seqs] if isinstance(path_seqs,", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect all keys from opts_list that are not wrapped in ClearedValue\n    keys = set(k for d in opts_list for k, v in d.items() if not isinstance(v, ClearedValue))\n\n    # combine values for each key\n    combined_opts = {}\n    for key in keys:\n        # get the sub-combiner for this key, or use combine_values\n        sub_combiner = combiners.get(key, combine_values)\n        # get the values for this key from each dictionary\n        values = [d.get(key) for d in opts_list]\n        # combine the values using the sub-combiner\n        combined_opts[key] = sub_combiner(*values)\n\n    return combined_opts", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._python_bin()", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '%s failed: %s' % (self.step_desc, self.reason)\n        else:\n            return '%s failed' % self.step_desc", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (k, v) for k, v in self._asdict().items()))\n", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if (step_num == 0 or self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = dict(\n            (k, getattr(self, k))\n            for k in self._STEP_ATTRS\n            if k not in self._HIDDEN_ATTRS and getattr(self, k) is not None)\n\n        desc['type'] = self._STEP_TYPE\n\n        return desc", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        key_encoded, value_encoded = key_value\n\n        if key_encoded != self._last_key_encoded:\n            self._last_key_decoded = self._loads(key_encoded)\n            self._last_key_encoded = key_encoded\n\n        return (self._last_key_decoded, self._loads(value_encoded))", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    if globals is None:\n        globals = {}\n\n    if locals is None:\n        locals = {}\n\n    safe_globals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n        'open': NameError('name \\'open\\' is not defined')\n    }\n\n    safe_globals.update(globals)\n\n    safe_locals = {\n        '__builtins__': None\n    }\n\n    safe_locals.update(locals)\n\n    return eval(expr, safe_globals, safe_locals)", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n    else:\n        return _to_lines(chunks)", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme != 's3':\n        raise ValueError('Not an S3 URI: %s' % uri)\n    if not parsed_uri.netloc:\n        raise ValueError('Not an S3 URI: %s' % uri)\n    if not parsed_uri.path:\n        raise ValueError('Not an S3 URI: %s' % uri)\n    return parsed_uri.netloc, parsed_uri.path.lstrip('/')", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return pathname2url(abspath(path_or_uri))\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if counters is None:\n        counters = {}\n\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if isinstance(stderr, BytesIO):\n        stderr = stderr.readlines()\n\n    statuses = []\n    other = []\n\n    for line in stderr:\n        line = line.strip()\n\n        m = _COUNTER_RE.match(line)\n        if m:\n            group, counter, amount = m.groups()\n            if group not in counters:\n                counters[group] = {}\n            counters[group][counter] = int(amount)\n        else:\n            m = _STATUS_RE.match(line)\n            if m:\n                statuses.append(m.group(1))\n            else:\n                other.append(line)\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # this is for Hadoop 1.x\n    for line in html_bytes.splitlines():\n        m = _JOB_TRACKER_HTML_RE.search(line)\n        if m:\n            return float(m.group(1))\n\n    return None", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None, None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _JOB_TRACKER_HTML_RE.findall(html_bytes)\n    if len(matches) >= 2:\n        return float(matches[0]), float(matches[1])\n    else:\n        return None, None", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    if isinstance(path, bytes):\n        path = path.decode('utf-8')\n\n    # check if the path is a pre-YARN task log path\n    m = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if job_id and m.group('job_id') != job_id:\n            return None\n\n        return dict(\n            attempt_id=m.group('attempt_id'),\n            log_type=m.group('log_type'),\n            path=path,\n            step_num=int(m.group('step_num')),\n            task_num=int(m.group('task_num')),\n            task_type=m.group('task_type'),\n            timestamp=int(m.group('timestamp')),\n        )\n\n    # check if the path is a YARN task log path\n    m = _YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if application_id and m.group('application_id') != application_id:\n            return None\n\n        return dict(\n            container_id=m.group('container_id'),\n            log_type=m.group('log_type'),\n            path=path,\n        )\n\n    return None", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    records = parse.parse_hadoop_log4j_records(lines)\n    return _parse_task_syslog_records(records)", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_step_sort_key, reverse=True)", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # make sure *lines* is a generator\n    lines = iter(lines)\n\n    result = {}\n\n    for line in lines:\n        m = _SUBMITTED_APPLICATION_RE.match(line)\n        if m:\n            # need this on YARN or we won't be able to find container logs\n            result['application_id'] = m.group('application_id')\n            continue\n\n        if line.endswith(_TRACEBACK_ENDS_WITH):\n            # start of a traceback\n            traceback = [line]\n            for line in lines:\n                traceback.append(line)\n                if line.startswith(_CAUSED_BY):\n                    # end of a traceback\n                    break\n            result['traceback'] = ''.join(traceback)\n            continue\n\n        if record_callback:\n            record_callback(line)\n\n    return result\n\n", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return\n\n        if not self._read_logs():\n            return\n\n        log.info('Scanning logs for probable cause of failure...')\n\n        if 'history' not in log_interpretation:\n            self._interpret_history_log(log_interpretation)\n\n        if 'step' not in log_interpretation:\n            self._interpret_step_logs(log_interpretation, step_type)\n\n        if 'task' not in log_interpretation:\n            self._interpret_task_logs(log_interpretation, step_type)\n\n        if 'task' in log_interpretation:\n            log_interpretation['task']['probable_cause'] = self._pick_probable_cause(\n                log_interpretation['task'])\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if not m:\n        return None\n\n    if job_id and m.group('job_id') != job_id:\n        return None\n\n    return dict(\n        job_id=m.group('job_id'),\n        path=path,\n        yarn='.jhist' in m.group('suffix'))", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for record in _parse_pre_yarn_history_records(lines):\n        record_type = record.get('type')\n\n        if record_type == 'Task':\n            if 'COUNTERS' in record['fields'] and 'TASKID' in record['fields']:\n                task_to_counters[record['fields']['TASKID']] = _parse_pre_yarn_counters(record['fields']['COUNTERS'])\n\n        elif record_type == 'Failed':\n            if 'ERROR' in record['fields'] and record['fields']['ERROR'] != '':\n                error = dict(\n                    hadoop_error=dict(\n                        message=record['fields']['ERROR'],\n                        start_line=record['start_line'],\n                        num_lines=record['num_lines']))\n\n                if 'TASKID' in record['fields']:\n                    error['task_id'] = record['fields']['TASKID']\n\n                if 'TASKATTEMPTID' in record['fields']:\n                    error['attempt_id'] = record['fields']['TASKATTEMPTID']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n        elif record_type == 'Job':\n            if 'COUNTERS' in record['fields']:\n                result['counters'] = _parse_pre_yarn_counters(record['fields']['COUNTERS'])\n\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    for line_num, line in enumerate(lines):\n        m = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not m:\n            continue\n\n        record = dict(\n            type=m.group('type'),\n            fields={},\n            start_line=line_num,\n            num_lines=1)\n\n        for key_pair_match in _PRE_YARN_HISTORY_KEY_PAIR.finditer(\n                m.group('key_pairs')):\n\n            key = key_pair_match.group('key')\n            value = _pre_yarn_history_unescape(\n                key_pair_match.group('escaped_value'))\n\n            record['fields'][key] = value\n\n        yield record", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n    for line in lines:\n        # counters\n        m = _INDENTED_COUNTERS_MESSAGE_RE.match(line)\n        if m:\n            result['counters'] = _parse_indented_counters(lines)\n            continue\n\n        # output_dir\n        m = _OUTPUT_DIRECTORY_RE.match(line)\n        if m:\n            result['output_dir'] = m.group('output_dir')\n            continue\n\n        # application_id\n        m = _SUBMITTED_APPLICATION_RE.match(line)\n        if m:\n            result['application_id'] = m.group('application_id')\n            continue\n\n        # job_id\n        m = _RUNNING_JOB_RE.match(line)\n        if m:\n            result['job_id'] = m.group('job_id')\n            continue\n\n        # progress\n        m = _JOB_PROGRESS_RE.match(line)\n        if m:\n            result['progress'] = dict(\n                map=int(m.group('map')),\n                reduce=int(m.group('reduce')),\n                message=line,\n            )\n\n        # invalid jar\n        m = _NOT_A_VALID_JAR_RE.match(line)\n        if m:\n            error = dict(\n                hadoop_error=dict(\n                    message=line,\n                ),\n            )\n            result.setdefault('errors', [])\n            result['errors'].append(error)\n\n        # task failure\n        m = _TASK_ATTEMPT_FAILED_RE.match(line)\n        if m:\n            error_str = '\\n'.join(line.splitlines()[1:])\n            if not error_str:  # if no exception, print something\n                error_str = line\n\n            error = dict(\n                attempt_id=m.group('attempt_id'),\n                hadoop_error=dict(\n                    message=error_str,\n                )\n            )\n\n           ", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n\n    for error in errors:\n        if error.get('container_id'):\n            key = error['container_id']\n        else:\n            key = error['time']\n\n        if key not in merged_errors:\n            merged_errors[key] = error\n\n    if attempt_to_container_id:\n        for attempt_id, container_id in attempt_to_container_id.items():\n            if container_id in merged_errors:\n                merged_errors[container_id]['attempt_id'] = attempt_id\n\n    def sort_key(key):\n        error = merged_errors[key]\n        return (\n            _is_probably_task_error(error),\n            error.get('time'),\n        )\n\n    return [merged_errors[key] for key in sorted(merged_errors, key=sort_key)]\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        m = _SSH_URI_RE.match(path_glob)\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['find', fs_path, '-type', 'f'])\n\n        for line in p.stdout:\n            yield line.strip()\n\n        self._ssh_finish_run(p)", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield line\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n\n        return self._hadoop_bin", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # use du -s on Hadoop 2 (see #1152)\n        if uses_yarn(self.get_hadoop_version()):\n            args = ['fs', '-du', '-s', path_glob]\n        else:\n            args = ['fs', '-dus', path_glob]\n\n        try:\n            stdout = self.invoke_hadoop(args, return_stdout=True)\n        except CalledProcessError:\n            return 0\n\n        # parse output\n        for line in BytesIO(stdout):\n            line = line.rstrip(b'\\r\\n')\n\n            # ignore total item count\n            if line.startswith(b'Found '):\n                continue\n\n            fields = line.split(b' ')\n\n            # Throw out directories\n            if fields[0].startswith(b'd'):\n                continue\n\n            # Try to figure out which part of the line is the path\n            # Expected lines:\n            #\n            # HDFS:\n            # 3276 /foo/bar\n            #\n            # S3:\n            # 3276 /foo/bar\n            path_index = None\n            for index, field in enumerate(fields):\n                # look for time field, and pick one after that\n                # (can't use field[2] because that's an int in Python 3)\n                if len(field) == 5 and field[2:3] == b':':\n                    path_index = (index + 1)\n            if not path_index:\n                raise IOError(\"Could not locate path in string %r\" % line)\n\n            path = to_unicode(line.split(b' ', path_index)[-1])\n            # handle fully qualified URIs from newer versions of Hadoop ls\n            # (see Pull Request #577)\n            if is_uri(path):\n                return int(fields[0])\n            else:\n                return int(fields[0])\n\n        raise IOError('Unexpected output from Hadoop fs -du: %r' % stdout)", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            returncode = self.invoke_hadoop(['fs', '-ls', path_glob],\n                                            ok_returncodes=[-1, 0, 255],\n                                            ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)\n\n        return returncode == 0", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        version = self.get_hadoop_version()\n\n        # use -R on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        self.invoke_hadoop(['fs', '-touchz', path])", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = _from_file_uri(path_glob)\n        total_size = 0\n        for path in glob.glob(path_glob):\n            if os.path.isfile(path):\n                total_size += os.path.getsize(path)\n        return total_size", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for path in files:\n                    yield 'file:///' + os.path.join(root, path)\n        else:\n            yield 'file:///' + path_glob\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(self.read_chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        for path in glob.glob(path_glob):\n            if os.path.exists(path):\n                return True\n        return False", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.isdir(path):\n            log.debug('Creating directory %s' % path)\n            os.makedirs(path)", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) != 0:\n                raise OSError('File already exists and is not empty: %s' % path)\n        else:\n            open(path, 'w').close()", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if is not None:\n            self._disable_if[name] = disable_if", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if path.startswith('s3://'):\n            # s3://bucket/path/to/file\n            # s3a://bucket/path/to/file\n            # s3n://bucket/path/to/file\n            # s3://bucket/path/to/dir/\n            # s3a://bucket/path/to/dir/\n            # s3n://bucket/path/to/dir/\n            # s3://bucket/path/to/dir/file\n            # s3a://bucket/path/to/dir/file\n            # s3n://bucket/path/to/dir/file\n            # s3://bucket/path/to/dir/file.gz\n            # s3a://bucket/path/to/dir/file.gz\n            # s3n://bucket/path/to/dir/file.gz\n            # s3://bucket/path/to/dir/file.bz2\n            # s3a://bucket/path/to/dir/file.bz2\n            # s3n://bucket/path/to/dir/file.bz2\n            # s3://bucket/path/to/dir/file.zip\n            # s3a://bucket/path/to/dir/file.zip\n            # s3n://bucket/path/to/dir/file.zip\n            # s3://bucket/path/to/dir/file.tar\n            # s3a://bucket/path/to/dir/file.tar\n            # s3n://bucket/path/to/dir/file.tar\n            # s3://bucket/path/to/dir/file.tar.gz\n            # s3a://bucket/path/to/dir/file.tar.gz\n            # s3n://bucket/path/to/dir/file.tar.gz\n            # s3://bucket/path/to/dir/file.tar.bz2\n            # s3a://bucket/path/to/dir/file.tar.bz2\n            # s3n://b", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # extract the id and categories from the filename\n    filename = posixpath.basename(input_uri)\n    id, rest = filename.split('-', 1)\n    cats = {}\n    for cat in rest.split('-'):\n        if cat.startswith('not_'):\n            cats[cat[4:]] = False\n        else:\n            cats[cat] = True\n\n    # return a dictionary containing the parsed information\n    return dict(id=id, cats=cats)", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        value, timestamp = _unpack_two_doubles(self._m, pos)\n        return value, timestamp", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate_metrics(metrics, accumulate)\n\n    ", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files)", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header is not None and 'application/openmetrics-text' in accept_header:\n        return generate_latest, 'application/openmetrics-text; version=1.0.0; charset=utf-8'\n    return generate_latest, CONTENT_TYPE_LATEST", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    try:\n        parse_command_line(argv)\n    except IOError:\n        pass\n\n    if options.conf:\n        parse_config_file(options.conf)\n\n    parse_command_line(argv)\n\n    if options.conf:\n        parse_config_file(options.conf)\n\n    if options.debug:\n        options.logging = 'debug'\n\n    if options.logging:\n        try:\n            enable_pretty_logging(options=options)\n        except IOError:\n            options.logging = None\n\n    if options.auth:\n        if not options.auth.startswith(('http://', 'https://')):\n            options.auth = 'http://' + options.auth\n\n    if options.url_prefix:\n        if not options.url_prefix.startswith('/'):\n            options.url_prefix = '/' + options.url_prefix\n        if not options.url_prefix.endswith('/'):\n            options.url_prefix += '/'\n\n    if options.basic_auth or options.auth:\n        if not options.auth:\n            options.auth = ','.join(\n                ['{0}:{1}'.format(*creds) for creds in options.basic_auth])\n        options.auth = list(map(str.strip, options.auth.split(',')))\n\n    if options.oauth2_key and options.oauth2_secret:\n        options.auth_provider = 'google'\n\n    if options.auth and not validate_auth_option(options.auth):\n        logger.error(\"Invalid '--auth' option: %s\", options.auth)\n        sys.exit(1)\n\n    if options.inspect_timeout:\n        options.inspect_timeout /= 1000.0\n\n    if options.max_tasks:\n        options.max_tasks = int(options.max_tasks)\n\n    if options.max_workers:\n        options.max_workers = int(options.max_workers)\n\n    if options.port:\n        options.port = int(", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {} {}\".format(self.Effect, other.Effect))\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data['Version']\n\n    return PolicyDocument(Statement=statements, Version=version)", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_known_iam_permissions = all_known_iam_permissions()\n    actions = pipe(all_known_iam_permissions,\n                   mapz(_parse_action),\n                   groupbyz(lambda action: action.prefix),\n                   )\n    return actions[prefix]", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    files.sort()\n    return files[-1]", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n\n    with open(service_definition_file_path, 'r') as service_definition_file:\n        service_definition = json.load(service_definition_file)\n\n    return service_definition['operations'][operationname]", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(self._source_to_iam_prefix(), self._event_name_to_iam_action())],\n            Resource=self.resource_arns\n        )", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return pipe(records,\n                filterz(_by_timeframe(from_date, to_date)),\n                filterz(_by_role_arns(arns_to_filter_for)))", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        records = []\n        for log_file in self._valid_log_files():\n            if log_file.contains_events_in_range(from_date, to_date):\n                records.extend(log_file.records())\n\n        return records", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    for target in targets:\n        if os.path.isfile(target):\n            if target.endswith('.py'):\n                included_files.append(target)\n        elif os.path.isdir(target):\n            if recursive:\n                for root, _, files in os.walk(target):\n                    for file in files:\n                        if file.endswith('.py'):\n                            included_files.append(os.path.join(root, file))\n            else:\n                for file in os.listdir(target):\n                    if file.endswith('.py'):\n                        included_files.append(os.path.join(target, file))\n\n    excluded_files = excluded_files.split(',')\n    included_files = [\n        file for file in included_files\n        if file not in excluded_files\n    ]\n\n    for file in included_files:\n        log.debug('Discovered file: %s', file)\n\n    return included_files", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    for filename in os.listdir(directory):\n        if _is_python_file(filename):\n            module_name = os.path.splitext(filename)[0]\n            file_path = os.path.join(directory, filename)\n            _local_modules.append((module_name, file_path))\n\n    return _local_modules", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.trigger_word in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, node))\n    return trigger_nodes", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, node)", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitisers = set()\n    for sink in sinks_in_file:\n        sanitisers.update(sink.sanitisers)\n\n    sanitiser_node_dict = defaultdict(list)\n    for sanitiser in sanitisers:\n        for node in find_sanitiser_nodes(sanitiser, sinks_in_file):\n            sanitiser_node_dict[sanitiser].append(node)\n\n    return sanitiser_node_dict", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file) as fd:\n        contents = fd.read()\n\n    data = json.loads(contents)\n\n    sources = [Source(**source) for source in data['sources']]\n    sinks = [Sink.from_json(key, data) for key, data in data['sinks'].items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n        return True\n    else:\n        return True", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is None:\n        condition_keys = CaseInsensitiveDict()\n\n    if string_to_check == string_to_check_against:\n        return True\n\n    if '${' in string_to_check_against:\n        # replace ${key} with value in condition_keys\n        for key in condition_keys.keys():\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', condition_keys[key])\n\n    if '*' in string_to_check_against or '?' in string_to_check_against:\n        # replace * and ? with regex\n        return _compose_pattern(string_to_check_against).match(string_to_check) is not None\n\n    if string_to_check_against.startswith('^'):\n        # replace ^ with regex\n        return _compose_pattern(string_to_check_against[1:]).match(string_to_check) is not None\n\n    return False", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n            dirname = os.path.dirname(credpath)\n            if not os.listdir(dirname):\n                shutil.rmtree(dirname)", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for root, dirs, files in os.walk(self.path):\n            for filename in files:\n                if filename.endswith(self.extension):\n                    with open(os.path.join(root, filename)) as f:\n                        credentials.append(yaml.load(f.read()))\n        return {\"_default\": {i: cred for i, cred in enumerate(credentials)}}", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        logger.debug(f\"Saving state for '{name}'\")\n        self.cursor.execute('INSERT OR REPLACE INTO states VALUES (?, ?)', (name, state))\n        self.conn.commit()", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            return None", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            self.run_forever()\n        else:\n            self.run_once()", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(\n            window_len=2, use_start_end_tokens=use_start_end_tokens\n        )\n        self.compute_rarest_windows(\n            window_len=3, use_start_end_tokens=use_start_end_tokens\n        )\n", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type == SessionType.cmds_only:\n            result = cmds_only.compute_rarest_window(\n                sessions=self.sessions,\n                window_len=window_len,\n                prior_probs=self.prior_probs,\n                trans_probs=self.trans_probs,\n                use_start_token=use_start_end_tokens,\n                use_end_token=use_start_end_tokens,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                use_geo_mean=use_geo_mean,\n            )\n        elif self.session_type == SessionType.cmds_params_only:\n            result = cmds_params_only.compute_rarest_window(\n                sessions=self.sessions,\n                window_len=window_len,\n                prior_probs=self.prior_probs,\n                trans_probs=self.trans_probs,\n                param_cond_cmd_probs=self.param_cond_cmd_probs,\n                use_start_token=use_start_end_tokens,\n                use_end_token=use_start_end_tokens,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                use_geo_mean=use_geo_mean,\n            )\n        else:\n            result = cmds_params_values.compute_rarest_window(\n                sessions=self.sessions,\n                window_len=window_len,\n                prior_probs=self.prior_probs,\n                trans_probs=self.trans_probs,\n                param_cond_cmd_probs=self.param_cond_cmd_probs,\n                value_cond_param_probs=self.value_cond_param_probs,\n                modellable_params=self.modellable_", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    from .model import Model\n\n    model = Model()\n    model.fit(data=data, session_column=session_column)\n    scored_sessions = model.score_sessions(\n        data=data, session_column=session_column, window_length=window_length\n    )\n    return scored_sessions", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    seq1_counts_sm = copy.deepcopy(seq1_counts)\n    seq2_counts_sm = copy.deepcopy(seq2_counts)\n    param_counts_sm = copy.deepcopy(param_counts)\n    cmd_param_counts_sm = copy.deepcopy(cmd_param_counts)\n\n    # Laplace smoothing\n    for cmd in seq1_counts_sm:\n        seq1_counts_sm[cmd] += 1\n    for prev in seq2_counts_sm:\n        for cur in seq2_counts_sm[prev]:\n            seq2_counts_sm[prev][cur] += 1\n    for par in param_counts_sm:\n        param_counts_sm[par] += 1\n    for cmd in cmd_param_counts_sm:\n        for par in cmd_param_counts_sm[cmd]:\n            cmd_param_counts_sm[cmd][par] += 1\n\n    # handle unseen commands, sequences of commands, and parameters\n    for cmd in seq1_counts_sm:\n        seq2_counts_sm[cmd][unk_token] = 1\n        seq2_counts_sm[unk_token][cmd] = 1\n    seq2_counts_sm[start_token][unk_token] = 1\n    seq2_counts_sm[unk_token][end_token] = 1\n    seq2_counts_sm[unk_token][unk_token] = 1\n    param_counts_sm[unk_token] = 1\n    cmd_param_counts_sm[unk_token] = {unk_token: 1}\n\n    return (\n        seq1_counts_sm,\n        seq2_counts_sm,\n        param_counts_sm,\n        cmd_param_counts_sm,\n    )\n\n", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be set to None when \"\n                \"use_start_token is set to True\"\n            )\n        window = [Cmd(name=str(start_token), params={})] + window\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be set to None when \"\n                \"use_end_token is set to True\"\n            )\n        window += [Cmd(name=str(end_token), params={})]\n\n    likelihood = 1\n    for i, cmd in enumerate(window):\n        if i == 0:\n            likelihood *= prior_probs[cmd.name]\n        else:\n            likelihood *= trans_probs[prev_cmd][cmd.name]\n        likelihood *= compute_prob_setofparams_given_cmd(\n            cmd=cmd.name,\n            params=cmd.params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_geo_mean=False,\n        )\n        prev_cmd = cmd.name\n\n    return likelihood", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n\n    likelihoods: List[float] = []\n    for i in range(s_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa E203\n        if use_start_end_tokens:\n            window = [start_token] + window + [end_token]\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            lik = lik ** (1 / window_len)\n        likelihoods.append(lik)\n\n    return likelihoods\n\n", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa E203\n\n    return rarest_window, min_lik\n\n", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be set to None when use_start_token is set to True\"\n            )\n        window = [str(start_token)] + window\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be set to None when use_end_token is set to True\"\n            )\n        window += [str(end_token)]\n\n    if isinstance(prior_probs, StateMatrix):\n        prior_probs = prior_probs.states\n    if isinstance(trans_probs, StateMatrix):\n        trans_probs = trans_probs.states\n\n    lik = 1\n    for i, cmd in enumerate(window):\n        if i == 0:\n            lik *= prior_probs[cmd]\n        else:\n            prev = window[i - 1]\n            lik *= trans_probs[prev][cmd]\n    return lik", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n\n    likelihoods = []\n    for i in range(s_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa: E203\n        prob = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            prob = prob ** (1 / window_len)\n        likelihoods.append(prob)\n\n    return likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa: E203\n\n    return rarest_window, min_lik\n\n", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    params_to_model_values = set()\n    for param, count in param_counts.items():\n        if count < 10:\n            continue\n        if param in param_value_counts:\n            values = param_value_counts[param]\n            if len(values) < 10:\n                continue\n            if len(values) > 100:\n                continue\n            if len(values) / count < 0.1:\n                continue\n            if len(values) / count > 0.5:\n                continue\n            params_to_model_values.add(param)\n    return params_to_model_values\n\n", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n    if isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = param_cond_cmd_probs.get_sm()\n    if isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = value_cond_param_probs.get_sm()\n\n    if len(params_with_vals) == 0:\n        return 1\n\n    if cmd not in param_cond_cmd_probs:\n        return 1\n\n    if cmd not in value_cond_param_probs:\n        return 1\n\n    prob = 1\n    for param, val in params_with_vals.items():\n        if param not in param_cond_cmd_probs[cmd]:\n            continue\n        if param not in value_cond_param_probs:\n            continue\n        if val not in value_cond_param_probs[param]:\n            continue\n        if param in modellable_params:\n            prob *= (\n                param_cond_cmd_probs[cmd][param] * value_cond_param_probs[param][val]\n            )\n        else:\n            prob *= param_cond_cmd_probs[cmd][param]\n\n    if use_geo_mean:\n        k = len(params_with_vals)\n        if cmd in modellable_params:\n            k += len(params_with_vals)\n        prob = prob ** (1 / k)\n\n    return prob", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\n            \"start_token should not be set to None when \"\n            \"use_start_token is set to True\"\n        )\n    if use_end_token and end_token is None:\n        raise MsticpyException(\n            \"end_token should not be set to None when \"\n            \"use_end_token is set to True\"\n        )\n\n    if use_start_token:\n        window = [Cmd(name=str(start_token), params={})] + window\n    if use_end_token:\n        window += [Cmd(name=str(end_token), params={})]\n\n    lik: float = 1\n    for i in range(len(window) - 1):\n        cmd = window[i].name\n        pars = window[i].params\n        if isinstance(pars, set):\n            pars = dict.fromkeys(pars)\n        lik *= prior_probs[cmd]\n        if i == 0:\n            lik *= trans_probs[start_token][cmd]\n        else:\n            prev_cmd = window[i - 1].name\n            lik *= trans_probs[prev_cmd][cmd]\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd,\n            params_with_vals=pars,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_geo_mean=False,\n        )\n\n    return lik", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n    if window_len > s_len:\n        return []\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token, params={})] + session + [\n            Cmd(name=end_token, params={})\n        ]\n    likelihoods = []\n    for i in range(s_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa E203\n        prob = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            prob = prob ** (1 / window_len)\n        likelihoods.append(prob)\n    return likelihoods\n\n", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa E203\n    return rarest_window, min_lik\n\n", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    cmd_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    seq2_probs: DefaultDict[Tuple[str, str], float] = defaultdict(lambda: 0)\n\n    for cmd, count in seq1_counts.items():\n        cmd_probs[cmd] = count / sum(seq1_counts.values())\n\n    for seq, count in seq2_counts.items():\n        seq2_probs[seq] = count / seq1_counts[seq[0]]\n\n    cmd_probs_sm = StateMatrix(states=cmd_probs, unk_token=unk_token)\n    seq2_probs_sm = StateMatrix(states=seq2_probs, unk_token=unk_token)\n\n    return cmd_probs_sm, seq2_probs_sm\n\n", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_param = sum(sum(param_value_counts[param].values()) for param in param_value_counts)\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_param\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent_with_error(\n                scopes=self.scopes, account=chosen_account[0]\n            )\n        if not self.result:\n            self.result = self._app_auth(self.auth_type)\n\n        if \"access_token\" in self.result:\n            self.refresh_token()", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        param_name = self.parameter_name_widget.value\n        param_desc = self.description_widget.value\n        param_type = self.type_widget.value\n        param_default = self.default_widget.value\n        if param_name in self.param_container.parameters:\n            del self.param_container.parameters[param_name]\n        self.param_container.parameters[param_name] = QueryParameter(\n            description=param_desc,\n            datatype=param_type,\n            default=param_default if self.default_reqd_widget.value else None,\n        )\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        del self.param_container.parameters[self.parameter_dropdown.value]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self._blank_parameter()\n        self._changed_data = True\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = list(self.data_env_widget.value)\n        self.metadata.data_families = [\n            data_fam.strip() for data_fam in self.data_families_widget.value.split(\",\")\n        ]\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = [\n            cluster.strip() for cluster in self.clusters_widget.value.split(\"\\n\")\n        ]\n        self.metadata.cluster_groups = [\n            cluster.strip()\n            for cluster in self.cluster_groups_widget.value.split(\"\\n\")\n        ]\n        self.metadata.tags = [\n            tag.strip() for tag in self.tags_widget.value.split(\",\")\n        ]\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self._unsaved_changes() and not self.ignore_changes.value:\n            print(\n                \"Please save or check 'Ignore changes' before saving a different file.\"\n            )\n            return\n        self._reset_change_state()\n        save_queries_to_yaml(self.query_collection, self.current_file)\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    yaml_data = yaml.safe_load(Path(yaml_file).read_text(encoding=\"utf-8\"))\n    metadata = QueryMetadata(**yaml_data[\"metadata\"])\n    defaults = _create_query_defaults(yaml_data[\"defaults\"])\n    sources = {\n        name: _create_query(query) for name, query in yaml_data[\"sources\"].items()\n    }\n    return QueryCollection(\n        file_name=yaml_file, metadata=metadata, defaults=defaults, sources=sources\n    )", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    crack_times_seconds = {\n        'online_throttling_100_per_hour': guesses / (100 / 3600),\n        'online_no_throttling_10_per_second': guesses / 10,\n        'offline_slow_hashing_1e4_per_second': guesses / 1e4,\n        'offline_fast_hashing_1e10_per_second': guesses / 1e10\n    }\n\n    crack_times_display = {\n        'online_throttling_100_per_hour':\n            display_time(crack_times_seconds['online_throttling_100_per_hour']),\n        'online_no_throttling_10_per_second':\n            display_time(crack_times_seconds['online_no_throttling_10_per_second']),\n        'offline_slow_hashing_1e4_per_second':\n            display_time(crack_times_seconds['offline_slow_hashing_1e4_per_second']),\n        'offline_fast_hashing_1e10_per_second':\n            display_time(crack_times_seconds['offline_fast_hashing_1e10_per_second'])\n    }\n\n    score = guesses_to_score(guesses)\n\n    return {\n        'crack_times_seconds': crack_times_seconds,\n        'crack_times_display': crack_times_display,\n        'score': score\n    }", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if 'guesses' in match:\n        return Decimal(match['guesses'])\n\n    # estimate the number of guesses based on password length and token length\n    min_guesses = min(match['token'].length, password.length)\n\n    # initialize guesses to the minimum possible value\n    guesses = Decimal(min_guesses)\n\n    # adjust guesses for uppercase letters\n    if re.search(r'[A-Z]', match['token']):\n        # conservative uppercase guesses: (average case + 26)\n        if re.search(r'[a-z]', match['token']):\n            # extra guesses for both upper + lower case letters\n            guesses *= 2\n        else:\n            # extra guesses for capitalization\n            guesses *= 2\n\n    # adjust guesses for l33t substitutions\n    if 'l33t' in match:\n        # extra guesses for all l33t substitutions\n        guesses *= len(match['l33t'])\n\n    # adjust guesses for repeated characters\n    if 'repeat' in match:\n        base_token = match['base_token']\n        repeat_count = match['repeat_count']\n\n        # base guesses: (base_token length + 1)\n        guesses *= len(base_token) + 1\n\n        # extra guesses for repeated characters\n        if repeat_count > 1:\n            # (uniq^repeated_chars_count - 1)\n            guesses *= pow(len(set(base_token)), repeat_count) - 1\n\n    # adjust guesses for sequences\n    if 'sequence' in match:\n        base_token = match['base_token']\n        sequence_name = match['sequence_name']\n        sequence_space = len(ADJACENCY_GRAPHS[sequence_name])\n\n        # base guesses: (base_token length)\n        guesses *= len(base_token)\n\n        # extra guesses for sequences in keyboards\n        if sequence_name in ['qwerty',", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['rank']\n    uppercase_variations = uppercase_variations(match)\n    l33t_variations = l33t_variations(match)\n    reversed_variations = (2 if match['reversed'] else 1)\n\n    return base_guesses * uppercase_variations * l33t_variations * reversed_variations", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33,\n    }\n\n    match_type = match['regex_name']\n    if match_type == 'recent_year':\n        # conservative estimate of year space: num years from REFERENCE_YEAR.\n        # if year is close to REFERENCE_YEAR, estimate a year space of 20.\n        year_space = max(abs(match['year'] - REFERENCE_YEAR), 20)\n        guesses = year_space * 365\n    else:\n        # base guesses: cardinality of charset\n        if match_type in char_class_bases:\n            base_guesses = char_class_bases[match_type]\n        else:\n            base_guesses = char_class_bases['alphanumeric']\n\n        if match_type == 'repeated':\n            base_guesses = pow(base_guesses, match['repeat_count'])\n\n        if 'characters' in match:\n            # upper- or lowercase transform\n            if len(match['token']) == 1:\n                # first calculate the number of possible bad positions,\n                # ie. for 'aa', with password length 3, there are three\n                # possible bad positions:\n                #     aaa, 1 bad\n                #     aAa, 2 bad\n                #     aaA, 3 bad\n                # so there are (len(token) - 1) * num_turns bad positions\n                # (linearly increasing)\n                bad_positions = (len(match['token']) - 1) * match['turns']\n                guesses = base_guesses * len(match['token'])\n                # for every bad position, that guess can be on a\n                # non-matching character, so subtract out the bad positions\n                guess", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = float(match['year'] - REFERENCE_YEAR)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n    guesses = year_space * 365\n\n    if match['separator']:\n        guesses *= 4\n\n    return guesses", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    s = KEYBOARD_STARTING_POSITIONS\n    d = KEYBOARD_AVERAGE_DEGREE\n    if match['graph'] == 'qwerty' or match['graph'] == 'dvorak':\n        s = KEYBOARD_STARTING_POSITIONS\n        d = KEYBOARD_AVERAGE_DEGREE\n    elif match['graph'] == 'keypad':\n        s = KEYPAD_STARTING_POSITIONS\n        d = KEYPAD_AVERAGE_DEGREE\n    else:\n        raise ValueError('Unknown keyboard/keypad type')\n\n    t = len(match['token'])\n    # Estimate the number of possible patterns w/ length t or less with t\n    # turns or less.\n    possible_turns = min(match['turns'], t)\n    possible_patterns = 0\n    for i in range(possible_turns + 1):\n        possible_patterns += nCk(t, i) * s ** i\n\n    # Estimate the number of possible patterns w/ length t or less with t\n    # turns or less with possible_turns turns.\n    guaranteed_turns = max(t - possible_turns, 0)\n    guaranteed_patterns = 0\n    for i in range(guaranteed_turns + 1):\n        guaranteed_patterns += nCk(t, i) * s ** i\n\n    min_guesses = possible_patterns * d ** possible_turns\n    max_guesses = guaranteed_patterns * d ** possible_turns\n\n    # add extra guesses for shifted keys. (% instead of 5, A instead of a.)\n    # math is similar to extra guesses of l33t substitutions in dictionary\n    # matches.\n    if match['shifted_count']:\n        S = len(ADJACENCY_GRAPHS[match['graph']])\n        U = match['shifted_count']\n        if U == 0:\n            min_guesses *= 2\n            max_guesses *= 2\n        else:\n            possibilities = 0\n            for i in", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n    if ALL_LOWER.match(word) or word.lower() == word:\n        return 1\n\n    # a capitalized word is the most common capitalization scheme,\n    # so it only doubles the search space (uncapitalized + capitalized):\n    # allcaps and end-capitalized are common enough too, underestimate as 2x,\n    # caps-internal is uncommon, so triple the search space\n    if START_UPPER.match(word):\n        return 2\n    if END_UPPER.match(word) or ALL_UPPER.match(word):\n        return 2\n\n    # otherwise calculate the number of ways to capitalize U uppercase + L\n    # lowercase letters with U + L <= 6 choices for positions of capitalization.\n    # (can't change the case of the first letter due to dictionary words, and\n    # can't change the case of last letter due to digits)\n    variations = 0\n    U = sum(1 for chr in word if chr.isupper())\n    L = sum(1 for chr in word if chr.islower())\n    for i in range(1, min(U, 6) + 1):\n        variations += nCk(U + L, i)\n\n    return variations", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for i in range(len(password)):\n        for j in range(i, len(password)):\n            password_slice = password[i:j + 1]\n            for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n                if password_slice in ranked_dict:\n                    rank = ranked_dict[password_slice]\n                    match = {\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password_slice,\n                        'matched_word': password_slice,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                    }\n                    matches.append(match)\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    reversed_string = password[::-1]\n    matches = dictionary_match(reversed_string, _ranked_dictionaries)\n\n    for match in matches:\n        i, j = match['i'], match['j']\n        match['token'] = password[i:j + 1]\n        match['reversed'] = True\n        match['i'], match['j'] = len(password) - 1 - j, len(password) - 1 - i\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    for i in range(len(password) - 1):\n        j = i + 1\n        for length in range(2, 7):\n            if j + length > len(password):\n                continue\n            sub = password[j:j + length]\n            if len(sub) > len(password):\n                continue\n            if sub.lower() in _ranked_dictionaries['l33t']:\n                continue\n            for sub_map in enumerate_l33t_subs(\n                    relevant_l33t_subtable(sub, _l33t_table)):\n                sub_trans = translate(sub, sub_map)\n                if sub_trans in _ranked_dictionaries['l33t']:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j + length - 1,\n                        'token': password[i:j + length],\n                        'matched_word': sub_trans,\n                        'rank': _ranked_dictionaries['l33t'][sub_trans],\n                        'dictionary_name': 'l33t',\n                        'reversed': False,\n                        'l33t': True,\n                        'sub': sub_map,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_matches = re.finditer(r'(.+)\\1+', password)\n    lazy_matches = re.finditer(r'(.+?)\\1+', password)\n    for match in greedy_matches:\n        base_token = match.group(1)\n        base_token_guesses = dictionary_match(base_token, _ranked_dictionaries)[0]['guesses']\n        repeat_count = len(match.group(0)) / len(base_token)\n        matches.append({\n            'pattern': 'repeat',\n            'i': match.start(),\n            'j': match.end() - 1,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_guesses': base_token_guesses,\n            'base_matches': dictionary_match(base_token, _ranked_dictionaries),\n            'repeat_count': repeat_count,\n        })\n\n    for match in lazy_matches:\n        base_token = match.group(1)\n        base_token_guesses = dictionary_match(base_token, _ranked_dictionaries)[0]['guesses']\n        repeat_count = len(match.group(0)) / len(base_token)\n        matches.append({\n            'pattern': 'repeat',\n            'i': match.start(),\n            'j': match.end() - 1,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_guesses': base_token_guesses,\n            'base_matches': dictionary_match(base_token, _ranked_dictionaries),\n            'repeat_count': repeat_count,\n        })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    delta = None\n    last_delta = None\n    for i in range(len(password) - 1):\n        if password[i].isdigit():\n            d = int(password[i + 1]) - int(password[i])\n        else:\n            try:\n                d = ord(password[i + 1]) - ord(password[i])\n            except ValueError:\n                continue\n        if delta is not None and (\n                last_delta is not None and d == last_delta or abs(d) > MAX_DELTA\n        ):\n            break\n        if d != 0:\n            delta = d\n        last_delta = d\n\n    if delta is None:\n        return []\n\n    def is_sequence(chars, delta):\n        \"\"\"\n        This function checks if a given list of characters forms a sequence with a given delta.\n        Input-Output Arguments\n        :param chars: List of characters. The list of characters to check for sequence.\n        :param delta: Integer. The delta value to check for sequence.\n        :return: Boolean. True if the list of characters forms a sequence with the given delta, False otherwise.\n        \"\"\"\n        for i in range(len(chars) - 1):\n            if chars[i + 1].isdigit():\n                d = int(chars[i + 1]) - int(chars[i])\n            else:\n                try:\n                    d = ord(chars[i + 1]) - ord(chars[i])\n                except ValueError:\n                    return False\n            if d != delta:\n                return False\n        return True\n\n    def extend_sequence(i, j, delta):\n        \"\"\"\n        This function extends a sequence starting from index i and ending at index j with a given delta.\n        Input-Output Arguments\n        :param i: Integer. The starting index of the sequence.\n        :param j: Integer. The ending index of the sequence.\n        :param delta: Integer. The delta value of the sequence.\n        :return: List of dictionaries. A list of dictionaries containing information about the extended sequence. Each dictionary includes the following keys: 'pattern', '", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        for match in regex.finditer(password):\n            i, j = match.span()\n            token = match.group(0)\n            matches.append({\n                'pattern': 'regex',\n                'token': token,\n                'i': i,\n                'j': j,\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, len(buffer), entropy)", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    supported_methods = []\n    for method in _auth_methods:\n        handler = _server_auth_handlers[method]\n        if handler.supported(conn):\n            supported_methods.append(method)\n    return supported_methods", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg not in _mac_handler:\n        raise ValueError('MAC algorithm not supported')\n\n    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        for entry in self._user_entries:\n            if (entry.key and (entry.key != key or\n                               ('cert-authority' in entry.options) != ca)):\n                continue\n\n            if entry.match_options(client_host, client_addr, cert_principals):\n                return entry.options\n\n        for entry in self._ca_entries:\n            if (entry.key and (entry.key != key or\n                               ('cert-authority' in entry.options) != ca)):\n                continue\n\n            if entry.match_options(client_host, client_addr, cert_principals):\n                return entry.options\n\n        return None", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       (stringprep.in_table_c11, stringprep.in_table_c21,\n                        stringprep.in_table_c22, stringprep.in_table_c3,\n                        stringprep.in_table_c4, stringprep.in_table_c5,\n                        stringprep.in_table_c6, stringprep.in_table_c7,\n                        stringprep.in_table_c8, stringprep.in_table_c9),\n                       True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('Extra data in packet')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        sig_alg, sig_data = decode_ssh_signature(sig)\n        if sig_alg not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n        if sig_alg.startswith(b'x509v3-'):\n            sig_alg = sig_alg[7:]\n        return self.verify_ssh(data, sig_alg, sig_data)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        if self._key is None:\n            raise KeyDecodeError('Private key is not available')\n\n        # Decode the private key\n        self._key = self._key.decode_private(self._key_params)\n\n        # Set the comment and filename\n        self.set_comment(self._comment)\n        self.set_filename(self._filename)\n\n        # Return the processed data\n        return self\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        output = f\"{self.__class__.__name__}(\"\n\n        if self.epsilon != float(\"inf\"):\n            output += f\"epsilon={self.epsilon}, \"\n\n        if self.delta != 1.0:\n            output += f\"delta={self.delta}, \"\n\n        if self.slack > 0:\n            output += f\"slack={self.slack}, \"\n\n        if len(self.spent_budget) > 0:\n            if len(self.spent_budget) > n_budget_max:\n                output += f\"spent_budget={self.spent_budget[:n_budget_max]}, ..., \"\n            else:\n                output += f\"spent_budget={self.spent_budget}, \"\n\n        output = output[:-2] + \")\"\n        return output", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        check_epsilon_delta(epsilon, delta)\n\n        if epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon}, got {epsilon}.\")\n\n        if delta < 0 or delta > 1:\n            raise ValueError(f\"Delta must be between 0 and 1, got {delta}.\")\n\n        if self.epsilon < epsilon:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {epsilon} epsilon.\")\n\n        if self.delta < delta:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {delta} delta.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n        total_epsilon, total_delta = self.total(spent_budget=spent_budget, slack=self.slack)\n\n        if self.epsilon < total_epsilon:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {epsilon} epsilon.\")\n\n        if self.delta < total_delta:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {delta} delta.\")\n\n        return True", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        check_epsilon_delta(epsilon, delta)\n        if self.check(epsilon, delta):\n            self.__spent_budget.append((epsilon, delta))\n\n        return self", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"accountant must be a BudgetAccountant or None\")\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    lower, upper = bounds\n\n    if not isinstance(lower, Real) or not isinstance(upper, Real):\n        raise TypeError(f\"Each bound must be numeric, got {lower} ({type(lower)}) and {upper} ({type(upper)}).\")\n\n    if lower > upper:\n        raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n    if array.shape[1] == 1:\n        array = np.clip(array, lower, upper)\n    else:\n        array = np.clip(array, lower, upper, out=array)\n\n    return array", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        n_total = n_past + n_noisy\n\n        # Calculate the new mean\n        # The new mean is calculated by taking a weighted average of the old mean and the new data\n        # The weight for the old mean is the number of samples represented by the old mean\n        # The weight for the new data is the number of samples represented by the new data\n        # The new mean is then calculated by dividing the sum of the weighted means by the total number of samples\n        # The new mean is calculated for each dimension in the data\n        # The new mean is then returned\n        # The new mean is calculated by taking a weighted average of the old mean and the new data\n        # The weight for the old mean is the number of samples represented by the old mean\n        # The weight for the new data is the number of samples represented by the new data\n        # The new mean is then calculated by dividing the sum of the weighted means by the total number of samples\n        # The new mean is calculated for each dimension in the data\n        # The new mean is then returned\n        # The new mean is calculated by taking a weighted average of the old mean and the new data\n        # The weight for the old mean is the number of samples represented by the old mean\n        # The weight for the new data is the number of samples represented by the new data\n        # The new mean is then calculated by dividing the sum of the weighted means by the total number of samples\n        # The new mean is calculated for each dimension in the data\n        # The new mean is then returned\n        # The new mean is calculated by taking a weighted average of the old mean and the new data\n        # The weight for the old mean is the number of samples represented by the old mean\n        # The weight for the new data is the number of samples represented by the new data\n        # The new mean is then calculated by dividing the sum of the weighted means by the total number of samples\n        # The new mean is calculated for each dimension in the data\n        # The new mean is then returned\n        # The", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        # Calculate the actual class counts\n        class_counts = np.bincount(y)\n\n        # Calculate the total number of samples\n        n_samples = y.shape[0]\n\n        # Calculate the sensitivity of the class counts\n        sensitivity = 1\n\n        # Calculate the privacy budget for each class count\n        epsilon_per_class = self.epsilon / len(class_counts)\n\n        # Add noise to the class counts using the Laplace mechanism\n        noisy_class_counts = np.zeros_like(class_counts)\n        for i in range(len(class_counts)):\n            noisy_class_counts[i] = LaplaceBoundedDomain(epsilon=epsilon_per_class, delta=0, sensitivity=sensitivity,\n                                                         lower=0, upper=n_samples, random_state=random_state).randomise(\n                class_counts[i])\n\n        return noisy_class_counts\n\n", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    random_state = check_random_state(random_state)\n\n    # If there are missing values, we need to count them\n    n_missing = np.isnan(X).sum(axis=0)\n\n    # If there are missing values, we need to remove them\n    if np.any(n_missing):\n        X = X[:, ~np.isnan(X).any(axis=0)]\n\n    # If there are no missing values, we can use the faster implementation\n    if not np.any(n_missing):\n        return _incremental_mean_and_var_fast(X, epsilon, bounds, last_mean, last_variance, last_sample_count,\n                                             random_state)\n\n    # If there are missing values, we need to use the slower implementation\n    return _incremental_mean_and_var_slow(X, epsilon, bounds, last_mean, last_variance, last_sample_count,\n                                         random_state)\n\n", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, self.fit_intercept, self.epsilon, self.bounds_X, self.bounds_y, self.copy_X, True,\n            self.random_state, **unused_args)\n\n        if sample_weight is not None:\n            warnings.warn(\"sample_weight is not supported by this class.\")\n\n        if self.bounds_X is None:\n            self.bounds_X = (X.min(axis=0), X.max(axis=0))\n\n        if self.bounds_y is None:\n            self.bounds_y = (y.min(axis=0), y.max(axis=0))\n\n        obj, noisy_coefs = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y, self.epsilon, 0,\n                                                     self.random_state)\n\n        self.coef_ = np.zeros((X.shape[1], y.shape[1] if y.ndim > 1 else 1))\n        self.intercept_ = np.zeros(y.shape[1] if y.ndim > 1 else 1)\n\n        for i in range(y.shape[1] if y.ndim > 1 else 1):\n            res = minimize(obj[i], self.coef_[:, i], method=\"L-BFGS-B\", jac=True)\n            self.coef_[:, i] = res.x\n\n        if self.fit_intercept:\n            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n\n        self.accountant.spend(epsilon=self.epsilon)\n\n        return self", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Bounds will be calculated on the data provided, resulting in additional \"\n                          \"privacy leakage. To ensure differential privacy, provide bounds.\", PrivacyLeakWarning)\n\n        if self.accountant is not None:\n            self.accountant.reset()\n\n        X = self._validate_data(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32], order=\"C\",\n                                accept_large_sparse=False, reset=True)\n\n        if self.bounds is None:\n            self.bounds = (X.min(axis=0), X.max(axis=0))\n\n        self.bounds_processed = None\n\n        random_state = self._check_random_state(self.random_state)\n\n        dims = X.shape[1]\n        total_iters = self._calc_iters(dims, X.shape[0])\n\n        centers = self._init_centers(dims, random_state)\n\n        if centers is None:\n            raise ValueError(\"Could not find valid initial centers. Please provide valid bounds.\")\n\n        for _ in range(total_iters):\n            distances, labels = self._distances_labels(X, centers)\n            centers = self._update_centers(X, centers, labels, dims, total_iters, random_state)\n\n        self.cluster_centers_ = centers\n        self.labels_ = labels\n        self.inertia_ = distances.min(axis=1).sum()\n        self.n_iter_ = total_iters\n\n        if self.accountant is not None:\n            self.accountant.spend(eps=self.epsilon, delta=0)\n\n        return self", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values_\n        }", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Tree has not been built yet. Call build() first.\")\n\n        # Find the leaves for each sample\n        leaves = self.apply(X)\n\n        # Find the unique leaves\n        unique_leaves = np.unique(leaves)\n\n        # Initialize the values array\n        self.values_ = np.zeros((len(unique_leaves), len(self.classes)), dtype=int)\n\n        # Populate the values for the real leaves\n        for leaf in unique_leaves:\n            if leaf != self._TREE_LEAF:\n                self.values_[leaf] = np.bincount(y[leaves == leaf], minlength=len(self.classes))\n\n        # Populate the values for the empty leaves\n        for leaf in unique_leaves:\n            if leaf != self._TREE_LEAF:\n                if np.sum(self.values_[leaf]) == 0:\n                    self.values_[leaf] = np.bincount(self.random_state.choice(self.classes, size=1),\n                                                     minlength=len(self.classes))\n\n        # Assign the values to the tree\n        for i, leaf in enumerate(unique_leaves):\n            if leaf != self._TREE_LEAF:\n                self.nodes[leaf].left_child = self._TREE_LEAF\n                self.nodes[leaf].right_child = self._TREE_LEAF\n                self.nodes[leaf].feature = -2\n                self.nodes[leaf].threshold = -2.0\n                self.nodes[leaf].node_id = i\n\n        return self", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist =", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n\"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n\n    for i in np.arange(dp_hist.shape[0]):\n        for j in np.arange(dp_hist.shape[1]):\n            dp_hist[i, j] = dp_mech.randomise(int(hist[i, j]))\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dp_hist_sum = dp_hist.sum()\n        dp_hist /= np.diff(xedges)[:, None]\n        dp_hist /= np.diff(yedges)[None, :]\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    # Check random state\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    # Check quantile\n    quant = np.asarray(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be between 0 and 1 inclusive\")\n\n    # Check array\n    array = np.asarray(array)\n\n    if array.ndim == 0:\n        raise ValueError(\"Array must be at least 1-dimensional\")\n\n    # Check bounds\n    if bounds is not None:\n        if len(bounds) != 2:\n            raise ValueError(\"Bounds must be of length 2\")\n\n        if bounds[0] >= bounds[1]:\n            raise ValueError(\"Bounds must be in the form (min, max)\")\n\n    # Check axis\n    if axis is None:\n        axis = tuple(range(array.ndim))\n    else:\n        axis = np.core.numeric.normalize_axis_tuple(axis, array.ndim, \"axis\")\n\n    # Check keepdims\n    if not isinstance(keepdims, bool):\n        raise TypeError(\"keepdims must be a boolean\")\n\n    # Check accountant\n    if accountant is not None:\n        accountant.check(epsilon, 1)\n\n    # Check unused arguments\n    warn_unused_args(unused_args)\n\n    # Process array of quantiles\n    quant = quant.flatten()\n\n    # Ravel array to be single-dimensional\n    array = np.ravel(array)\n\n    # Deal with a single quantile ir scalar\n    if quant.size == 1:\n        quant = quant[0]\n\n        # Check bounds\n        if bounds is None:\n            bounds = (np.min(array), np.max(array))\n\n        # Check axis\n        if isinstance(axis, tuple):\n            axis = axis[0]\n\n        # Check keepdims\n        if keepdims:\n            raise ValueError(\"keepdims must be False for a single quantile", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    warn_unused_args(unused_args)\n\n    if percent < 0 or percent > 100:\n        raise ValueError(\"Percentile must be in the range [0, 100].\")\n\n    return quantile(array, percent / 100, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                    random_state=random_state, accountant=accountant)", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    rng = check_random_state(random_state)\n    if rng.random() < np.exp(-gamma):\n        return 0\n    else:\n        return 1", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    return datetime.datetime.utcfromtimestamp(((id >> 22) + DISCORD_EPOCH) / 1000)", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    return int((dt.replace(tzinfo=datetime.timezone.utc).timestamp() * 1000 - DISCORD_EPOCH) << 22) + (2**63 if high else 0)", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(code=invite.code, event=invite.guild.id if invite.guild else None)\n    elif isinstance(invite, str):\n        if invite.isdigit():\n            return ResolvedInvite(code=invite, event=None)\n        if invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(code=invite.split('/')[-1], event=None)\n        if invite.startswith('discord.gg/'):\n            return ResolvedInvite(code=invite.split('/')[-1], event=None)\n        return ResolvedInvite(code=invite, event=None)\n    raise TypeError(f'Expected Invite or str, got {invite.__class__.__name__}')", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n    if isinstance(annotation, str):\n        return ForwardRef(annotation)\n\n    if localns is None:\n        namespace = globalns\n    else:\n        namespace = {**globalns, **localns}\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, namespace, cache)", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop(func, seconds=seconds, minutes=minutes, hours=hours, time=time, count=count, reconnect=reconnect)", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        classified_gadgets = []\n\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                classified_gadgets += self._classify(\n                    gadget,\n                    classifier,\n                    gadget_type,\n                    self._emu_iters\n                )\n            except:\n                import traceback\n                print(\"Error while classifying gadget:\")\n                print(gadget)\n                traceback.print_exc()\n\n        # Sort gadgets by string representation.\n        classified_gadgets = sorted(classified_gadgets, key=lambda x: str(x))\n\n        return classified_gadgets\n\n    ", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n\n        # sort gadgets by address\n        candidates.sort(key=lambda x: x.address)\n\n        return candidates", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)[0]\n\n                    self._cache[instr] = parsed_instr\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n\n                    continue\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + s.size - 1, offset), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == bool\n    assert type(true) in (Constant, BitVec) and type(false) in (Constant, BitVec)\n    assert true.size == false.size == size\n\n    return BitVec(size, \"ite\", cond, true, false)", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    assert all(type(arg) is BitVec for arg in args)\n\n    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)\n\n", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name,\n                                                                               self.key_size,\n                                                                               self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception:\n            self._log_translation_exception(instruction)\n            raise TranslationError(\"Unknown error\")", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            f = open(binary, 'rb')\n        except IOError:\n            raise Exception(\"Error loading file.\")\n\n        signature = f.read(4)\n\n        if signature == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        elif signature == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")\n\n        f.close()\n", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        try:\n            instr = instr.lower()\n\n            if instr not in self._cache:\n                parsed_instr = instruction.parseString(instr)[0]\n                self._cache[instr] = copy.deepcopy(parsed_instr)\n\n            return copy.deepcopy(self._cache[instr])\n\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\" % instr)\n            logger.error(e)\n            return None\n", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        try:\n            instr = instr.lower()\n\n            if instr not in self._cache:\n                self._cache[instr] = instruction.parseString(instr)[0]\n\n            return copy.deepcopy(self._cache[instr])\n\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", instr)\n            logger.error(e)\n\n            return None\n", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        pattern = _re_pattern_allow_dots\n    else:\n        pattern = _re_pattern\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = re.sub(pattern, \"\", value).strip().lower()\n    return re.sub(_re_spaces, \"-\", value)", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = luhn_checksum(partial_number * 10)\n    if check_digit == 0:\n        return check_digit\n    return 10 - check_digit", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is not None:\n        assert len(a) == len(p)\n\n    if hasattr(random, \"choices\"):\n        if length == 1 and p is None:\n            return [random.choice(a)]\n        else:\n            return random.choices(a, weights=p, k=length)\n    else:\n        choices = []\n\n        if p is None:\n            p = itertools.repeat(1, len(a))  # type: ignore\n\n        cdf = list(cumsum(p))  # type: ignore\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        for i in range(length):\n            uniform_sample = random_sample(random=random)\n            idx = bisect.bisect_right(cdf2, uniform_sample)\n            item = a[idx]\n            choices.append(item)\n        return choices", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n    for provider_path in providers:\n        provider_module = import_module(provider_path)\n        if getattr(provider_module, \"localized\", False):\n            langs = list(getattr(provider_module, \"Provider\").languages)\n            available_locales.update(langs)\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if hasattr(module, \"__package__\"):\n            providers = [\n                f\"{module.__package__}.{name}\"\n                for name in list_module(module)\n                if name != \"__pycache__\"\n            ]\n            available_providers.update(providers)\n\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        remaining_length = length - len(prefix)\n        while remaining_length > 1:\n            number += str(self.random_digit())\n            remaining_length -= 1\n\n        check_digit = self._calculate_luhn(number)\n        number += str(check_digit)\n        return number\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    return \"\".join(codes.get(ord(c), c) for c in txt)", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    # Extract the filename and extension from the path\n    filename, extension = os.path.splitext(path)\n\n    # Construct the file path without the filename\n    file_path = os.path.dirname(filename)\n\n    # Replace the version with underscores\n    v_str = version_clean.sub(\"_\", str(version))\n\n    # Construct the fingerprint\n    fingerprint = f\"{file_path}.v{v_str}m{hash_value}{extension}\"\n\n    return fingerprint", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    path_parts = path.split(\"/\")\n    filename, extension = path_parts[-1].split(\".\", 1)\n    file_path = \"/\".join(path_parts[:-1] + [filename])\n\n    if cache_regex.match(extension):\n        return file_path, True\n\n    return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if not pages_folder:\n            pages_folder = os.path.join(\n                os.path.dirname(os.path.realpath(__file__)), \"pages\"\n            )\n        if not os.path.isdir(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"{name} `pages_folder` not found. {pages_folder}\"\n            )\n    return pages_folder", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        if schema is None:\n            return [el for g in grouping for el in flatten_grouping(g)]\n        else:\n            return [\n                el\n                for g, s in zip(grouping, schema)\n                for el in flatten_grouping(g, schema=s)\n            ]\n\n    if isinstance(grouping, dict):\n        if schema is None:\n            return [el for g in grouping.values() for el in flatten_grouping(g)]\n        else:\n            return [\n                el\n                for g, s in zip(grouping.values(), schema.values())\n                for el in flatten_grouping(g, schema=s)\n            ]\n\n    return [grouping]", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        return [\n            make_grouping_by_index(schema_el, flat_values)\n            for schema_el in schema\n            for _ in flatten_grouping(schema_el)\n        ]\n\n    if isinstance(schema, dict):\n        return {\n            k: make_grouping_by_index(schema_el, flat_values)\n            for k, schema_el in schema.items()\n            for _ in flatten_grouping(schema_el)\n        }\n\n    return flat_values.pop(0)", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return list(map_grouping(fn, el) for el in grouping)\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (g, s) in enumerate(zip(grouping, schema)):\n            validate_grouping(g, s, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema.keys()))\n        for k, s in schema.items():\n            validate_grouping(grouping[k], s, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    elif requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with requests_pathname_prefix are not supported.\n            You supplied: {path} and requests_pathname_prefix was {requests_pathname}\n            \"\"\"\n        )\n    else:\n        return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname != \"/\" and not path.startswith(requests_pathname.rstrip(\"/\")):\n        return path\n    if requests_pathname.endswith(\"/\") and not path.endswith(\"/\"):\n        return path\n    return path[len(requests_pathname) :]", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        return map_js_to_py_types_flow_types(type_object).get(\n            type_object[\"name\"], lambda: \"\"\n        )(indent_num)\n    return map_js_to_py_types_prop_types(type_object).get(\n        type_object[\"name\"], lambda: \"\"\n    )()", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    data = _get_metadata(metadata_path)\n    components = []\n    for component_name in data:\n        component_metadata = data[component_name]\n        component_metadata[\"name\"] = component_name\n        component_metadata[\"namespace\"] = namespace\n        component = generate_class(component_metadata)\n        components.append(component)\n    return components", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Register the component lib for index include.\n    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n    # Generate imports file\n    with open(os.path.join(namespace, \"__init__.py\"), \"w\") as f:\n        f.write(\"from .base_component import ComponentRegistry\\n\")\n        f.write(\"from .base_component import BaseComponent\\n\")\n        f.write(\"from .base_component import BaseComponentRegistry\\n\")\n        f.write(\"from .base_component import BaseComponentMeta\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistry\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMeta\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistry\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistryMeta\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistryMetaRegistry\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistryMetaRegistryMeta\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistryMetaRegistryMetaRegistry\\n\")\n        f.write(\"from .base_component import BaseComponentMetaRegistryMetaRegistryMetaRegistryMetaRegistryMeta\\n\")\n        f.write(\"from .base_component import BaseComponent", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # pylint: disable=no-member\n        props = {\n            k: v\n            for k, v in self.__dict__.items()\n            if k in self._prop_names\n            and v is not None\n            and v is not self.UNDEFINED\n            and v is not self.REQUIRED\n        }\n\n        # Add wildcard properties\n        wildcard_props = {\n            k: v\n            for k, v in self.__dict__.items()\n            if k.startswith(\"data-\") or k.startswith(\"aria-\")\n        }\n        props.update(wildcard_props)\n\n        # Add the type and namespace\n        props[\"type\"] = self._type\n        props[\"namespace\"] = self._namespace\n\n        return props\n", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        children = getattr(self, \"children\", None)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield children\n            # pylint: disable=protected-access\n            for t in children._traverse():\n                yield t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for i in children:\n                yield i\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for t in i._traverse():\n                        yield t\n\n        # children is a string or number\n        else:\n            yield children", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if component.startswith(\"_\"):\n            continue\n        if component.startswith(\"dash\"):\n            continue\n        if component.startswith(\"html\"):\n            continue\n        if component.startswith(\"dcc\"):\n            continue\n        if component.startswith(\"dash_core_components\"):\n            continue\n        if component.startswith(\"dash_html_components\"):\n            continue\n        if component.startswith(\"dash_renderer\"):\n            continue\n        if component.startswith(\"dash_table\"):\n            continue\n        if component.startswith(\"dash_daq\"):\n            continue\n        if component.startswith(\"dash_bootstrap_components\"):\n            continue\n        if component.startswith(\"dash_cytoscape\"):\n            continue\n        if component.startswith(\"dash_extensions\"):\n            continue\n        if component.startswith(\"dash_extra_components\"):\n            continue\n        if component.startswith(\"dash_leaflet\"):\n            continue\n        if component.startswith(\"dash_player\"):\n            continue\n        if component.startswith(\"dash_canvas\"):\n            continue\n        if component.startswith(\"dash_bio\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if component.startswith(\"dash_auth\"):\n            continue\n        if", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for key, value in metadata.items():\n        if is_node(value[\"name\"]):\n            nodes.append(base + key)\n        elif is_shape(value[\"name\"]):\n            nodes = collect_nodes(value[\"value\"], base + key + \".\", nodes)\n        elif value[\"name\"] == \"arrayOf\":\n            nodes = collect_array(value, base + key, nodes)\n        elif value[\"name\"] == \"union\":\n            nodes = collect_union(value[\"value\"], base + key, nodes)\n        elif value[\"name\"] == \"objectOf\":\n            nodes = collect_object(value, base + key, nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is None:\n            self._where = reduce(operator.and_, expressions)\n        else:\n            self._where = reduce(operator.and_, (self._where,) + expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables.extend(self._database.get_views())\n        return [t.name for t in tables]\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table:\n            tables = [table] + self.get_table_dependencies(table)\n        else:\n            tables = self.tables\n\n        for table in tables:\n            self._models[table] = self._introspector.generate_model(\n                table,\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)\n\n        for model in self._models.values():\n            model.bind(self._database)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    connect_kwargs.update(connect_params)\n    scheme = parsed.scheme\n    if scheme in schemes:\n        return schemes[scheme](**connect_kwargs)\n    else:\n        raise RuntimeError('Unknown database scheme: %s' % scheme)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table(True)\n\n        if drop:\n            if insert:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'INSERT'))\n            if update:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'UPDATE'))\n            if delete:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'DELETE'))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))\n", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        if default is Sentinel:\n            return self[key]\n        with self._database.atomic():\n            try:\n                value = self[key]\n            except KeyError:\n                return default\n            del self[key]\n            return value", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if not name:\n            name = receiver.__name__\n        key = (name, sender)\n        if key in self._receivers:\n            raise ValueError('receiver named %s for sender=%s already '\n                             'exists.' % (name, sender or 'any'))\n        self._receivers.add(key)\n        self._receiver_list.append((name, receiver, sender))", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is None:\n            if name is None:\n                raise ValueError('a receiver or a name must be provided')\n            key = (name, sender)\n            if key not in self._receivers:\n                raise ValueError('receiver named %s (for sender=%s) not '\n                                 'connected' % (name, sender or 'any'))\n            self._receivers.remove(key)\n            self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list\n                                   if n != name or s != sender]\n        else:\n            key = (receiver.__name__, sender)\n            if key not in self._receivers:\n                raise ValueError('receiver %s (for sender=%s) not '\n                                 'connected' % (receiver, sender or 'any'))\n            self._receivers.remove(key)\n            self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list\n                                   if r != receiver or s != sender]", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update the commissions\n        self.commission += commission\n\n        # Update the size\n        self.size += size\n\n        # Check if the position was opened\n        if not self.isopen:\n            self.isopen = True\n            self.justopened = True\n            self.baropen = self.data.current_bar\n            self.dtopen = self.data.datetime[0]\n\n        # Update the trade length\n        self.barlen = self.data.current_bar - self.baropen\n\n        # Check if the position was closed\n        if self.size == 0:\n            self.isclosed = True\n            self.barclose = self.data.current_bar\n            self.dtclose = self.data.datetime[0]\n\n        # Update the average price\n        if abs(size) > abs(self.size):\n            self.price = price\n        else:\n            self.price = (self.price * abs(self.size) + price * abs(size)) / (abs(self.size) + abs(size))\n\n        # Update the value\n        self.value = self.price * abs(self.size)\n\n        # Update the pnl\n        self.pnl = self.value - self.commission\n\n        # Update the pnlcomm\n        self.pnlcomm = self.pnl - self.commission\n\n        # Update the status\n        if self.size > 0:\n            self.status = self.Open\n        elif self.size < 0:\n            self.status = self.Closed\n        else:\n            self.status = self.Created\n\n        # Update the history\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status,\n                    self.data.datetime[0],\n                    self.barlen,\n                    self.size,\n                    self.price,\n                    self.value,\n                    self.pnl,\n                    self.pnlcomm,\n                    self.data.tz\n                )\n            )\n            self.history[-1].doupdate(order, size, price, commission)\n", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                config=self.config.vars.type_schema,\n                type_schema=self._type_schema,\n            )\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if self.content is None:\n            return \"\"\n\n        if isinstance(self.content, list):\n            rows = self.content\n        else:\n            rows = [self.content]\n\n        return self.render_template(\n            \"frequency_table.html\",\n            rows=rows,\n        )", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        template_name = \"diagram.html\"\n        template_path = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), template_name\n        )\n        with open(template_path, \"r\") as f:\n            template = jinja2.Template(f.read())\n\n        return template.render(\n            image=self.image,\n            alt=self.alt,\n            caption=self.caption,\n            **self.kwargs,\n        )", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    n_infinite = len(finite_values) - len(finite_values[np.isfinite(finite_values)])\n    if n_infinite > 0:\n        # TODO: add logging\n        pass\n\n    n_finite = len(finite_values)\n    n_missing = n_infinite + sum(pd.isnull(finite_values))\n\n    if n_finite == 0:\n        # TODO: add logging\n        return {}\n\n    # Apply the tolerance factor to detect the number of distinct values\n    n_distinct = n_unique\n\n    n_bins = config.plot.histogram.bins\n    if n_distinct < n_bins:\n        n_bins = n_distinct\n\n    if n_bins > config.plot.histogram.max_bins:\n        n_bins = config.plot.histogram.max_bins\n\n    minimum, maximum = finite_values.min(), finite_values.max()\n    _, histogram_bins = np.histogram(finite_values, bins=n_bins, range=(minimum, maximum))\n\n    histogram, histogram_bins = np.histogram(\n        finite_values, bins=histogram_bins, range=(minimum, maximum), weights=weights\n    )\n\n    return {\n        f\"{name}_counts\": histogram,\n        f\"{name}_bin_edges\": histogram_bins,\n        f\"{name}_n_infinite\": n_infinite,\n        f\"{name}_n_finite\": n_finite,\n        f\"{name}_n_missing\": n_missing,\n        f\"{name}_n_distinct\": n_distinct,\n    }", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        raise NotImplementedError()\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_dataframe = dataframe.copy()\n        for column in self._get_numerical_columns(dataframe):\n            discretized_dataframe[column] = self._discretize_column(\n                discretized_dataframe[column]\n            )\n\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n\n        return discretized_dataframe", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n    categoricals = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Categorical\" and 1 < value[\"n_distinct\"] <= threshold\n    }\n\n    if len(categoricals) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(categoricals), len(categoricals))),\n        index=categoricals,\n        columns=categoricals,\n    )\n\n    for col_1, col_2 in itertools.combinations(categoricals, 2):\n        score = _pairwise_cramers(df[col_1], df[col_2])\n        (\n            correlation_matrix.loc[col_1, col_2],\n            correlation_matrix.loc[col_2, col_1],\n        ) = (score, score)\n\n    return correlation_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n\n    # `index` and `columns` must not be a set since Pandas 1.5,\n    # so convert it to a list. The order of the list is arbitrary.\n    categoricals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n            and 1 < value[\"n_distinct\"] <= threshold\n        }\n    )\n\n    numericals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n        }\n    )\n\n    if len(categoricals) + len(numericals) <= 1:\n        return None\n\n    matrix = np.zeros((len(categoricals) + len(numericals),) * 2)\n    np.fill_diagonal(matrix, 1.0)\n    correlation_matrix = pd.DataFrame(\n        matrix,\n        index=categoricals + numericals,\n        columns=categoricals + numericals,\n    )\n\n    # Discretize the DataFrame\n    df_discretized = df[categoricals + numericals].apply(\n        pd.qcut, q=config.categorical_maximum_correlation_distinct, labels=False, duplicates=\"drop\"\n    )\n\n    # Calculate the correlation scores\n    for name1, name2 in itertools.combinations(categoricals + numericals, 2):\n        if name1 in categoricals and name2 in categoricals:\n            confusion_matrix = pd.crosstab(df_discretized[name1], df_discretized[name2])\n            if confusion_matrix.empty:\n                correlation_matrix.loc[name2, name1] = np.nan\n            else:\n                correlation_matrix.loc[name2, name1] = _cramers_corrected_stat(\n                    confusion_", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    if args.silent:\n        # Restrict visualization features\n        minimal = True\n        explorative = False\n    else:\n        minimal = args.minimal\n        explorative = args.explorative\n\n    config_file = args.config_file\n\n    if config_file is not None:\n        config_file = Path(config_file).resolve()\n\n    profile = ProfileReport(\n        args.input_file,\n        title=args.title,\n        config_file=config_file,\n        minimal=minimal,\n        explorative=explorative,\n        pool_size=args.pool_size,\n        infer_dtypes=args.infer_dtypes,\n    )\n    profile.to_file(output_file=args.output_file)\n\n    if not args.silent:\n        profile.to_notebook_if_not_none()", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    # If not exists, download and create file\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].apply(lambda x: type(x) in types).any():\n            df = df.explode(col)\n            df[[f\"{col}_{c}\" for c in df[col].columns]] = pd.DataFrame(\n                df[col].tolist(), index=df.index\n            )\n            df.drop(columns=col, inplace=True)\n    return df", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n    elif isinstance(serializer, str):\n        return import_from_string(serializer)\n    elif isinstance(serializer, type):\n        if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n            raise NotImplementedError(\n                f'Serializer must implement `dumps` and `loads` methods. Got {serializer}'\n            )\n        return serializer\n    else:\n        raise NotImplementedError(\n            f'Serializer must be either a string or a class. Got {serializer}'\n        )\n\n", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\",\n                self._inferred_intent,\n            )\n        )\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    from lux.action.filter import register_filter_action\n    from lux.action.custom import register_custom_action\n    from lux.action.export import register_export_action\n    from lux.action.chart import register_chart_action\n    from lux.action.enhance import register_enhance_action\n    from lux.action.generalize import register_generalize_action\n\n    from lux.vis.VisList import VisList\n    from lux.vis.Vis import Vis\n\n    def no_vis(vis):\n        return True\n\n    def transpose_vis(vis):\n        return (\n            (type(vis) == VisList)\n            and len(vis) == 1\n            and (type(vis[0]) == Vis)\n            and (vis[0].get_attr_by_channel(\"x\") and vis[0].get_attr_by_channel(\"y\"))\n        )\n\n    def no_filter_action(vis):\n        filter_specs = vis.get_filter_specs()\n        if filter_specs:\n            for clause in filter_specs:\n                if clause.value != \"\":\n                    return True\n        return False\n\n    def multi_vis(vis):\n        return type(vis) == VisList and len(vis) > 1\n\n    def multi_filter_vis(vis):\n        return multi_vis(vis) and no_filter_action(vis)\n\n    def multi_vis_with_dual_y(vis):\n        return multi_vis(vis) and len(vis.get_attr_by_channel(\"y\")) == 2\n\n    def multi_vis_with_dual_x(vis):\n        return multi_vis(vis) and len(vis.get_attr_by_channel(\"x\")) == 2\n\n    def multi_vis_with_dual_xy(vis):\n        return multi_vis(vis) and len(vis.get_attr_by_channel(\"x\")) == 2 and len(vis.get_attr_by_channel(\"y\")) == 2\n\n    def multi_vis_with_nd_y(vis):\n        return multi_vis(vis", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if not isinstance(locations, (list, tuple)):\n        locations = [locations]\n    bounds = [[None, None], [None, None]]\n    for location in locations:\n        for coord in iter_coords(location):\n            for i in (0, 1):\n                bounds[i][0] = none_min(bounds[i][0], coord[i])\n                bounds[i][1] = none_max(bounds[i][1], coord[i])\n    if lonlat:\n        bounds = _locations_mirror(bounds)\n    return bounds", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        schema_version = self.data.get(\"$schema\", \"\")\n        if not schema_version:\n            return 2\n\n        version_match = re.search(r\"\\d+\\.\\d+\\.\\d+\", schema_version)\n        if not version_match:\n            return 2\n\n        version_string = version_match.group(0)\n        major_version = int(version_string.split(\".\")[0])\n        return major_version\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if color not in colors:\n        return string\n    if platform.system() == \"Windows\":\n        return string\n    return colors[color] + string + \"\\033[0m\"", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        ret_songs_list = []\n        ret_errors = []\n        threads = []\n        for source in sources_list:\n            t = threading.Thread(\n                target=self.search_thread,\n                args=(source, keyword, ret_songs_list, ret_errors),\n            )\n            threads.append(t)\n            t.start()\n        for t in threads:\n            t.join()\n        if ret_errors:\n            for source, err in ret_errors:\n                self.logger.error(\"Error occurred when searching %s: %s\" % (source, err))\n        ret_songs_list = list(\n            {song[\"songname\"] + song[\"singer\"][0] + str(song[\"size\"]): song for song in ret_songs_list}.values()\n        )\n        ret_songs_list.sort(key=lambda x: x[\"songname\"])\n        return ret_songs_list", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"ascii\")\n\n    rem = len(input) % 4\n\n    if rem > 0:\n        input += b\"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input)", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Must be a positive integer\")\n\n    bytes_val = val.to_bytes((val.bit_length() + 7) // 8, \"big\")\n\n    if len(bytes_val) == 0:\n        bytes_val = b\"\\x00\"\n\n    return base64url_encode(bytes_val)", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        key_bytes = force_bytes(key)\n\n        if key_bytes.startswith(b\"-----BEGIN\"):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n            )\n        elif key_bytes.startswith(b\"ssh-rsa\"):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n            )\n\n        return key_bytes", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        if isinstance(key_obj, bytes):\n            key_obj = key_obj.decode(\"utf-8\")\n\n        obj = {\n            \"kty\": \"oct\",\n            \"k\": base64url_encode(force_bytes(key_obj)).decode(),\n        }\n\n        if as_dict:\n            return obj\n        else:\n            return json.dumps(obj)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        try:\n            if isinstance(jwk, str):\n                obj = json.loads(jwk)\n            elif isinstance(jwk, dict):\n                obj = jwk\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an octet key\")\n\n        return base64url_decode(obj.get(\"k\"))\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        if strict_parsing:\n            raise\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            r = recursive_update(d.get(k, {}), v)\n            d[k] = r\n        else:\n            d[k] = u[k]\n    return d", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            yield key, dictionary[key]\n    for key in sorted(dictionary.keys()):\n        value = dictionary[key]\n        if is_non_empty_dict(value):\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value\n    for key in sorted(dictionary.keys()):\n        value = dictionary[key]\n        if not is_non_empty_dict(value):\n            yield key, value", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in sorted(d.items()):\n        if isinstance(value, dict):\n            for k, val in iterate_flattened(value):\n                yield join_paths(key, k), val\n        else:\n            yield key, value", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(len(split_path)):\n        yield \".\".join(split_path[: i + 1])", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    if not is_prefix(base, path):\n        raise AssertionError(f\"{base} not a prefix of {path}\")\n    return path[len(base) + 1 :]", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        current_dict = nested_dict\n        parts = key.split(\".\")\n        for part in parts[:-1]:\n            current_dict = current_dict.setdefault(part, {})\n        current_dict[parts[-1]] = value\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(type(e).__name__, str(e)))\n    return \"\\n\".join(lines)", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    return parse_version(importlib.import_module(name).__version__)", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = function.__name__\n        return self.command(function)\n\n    ", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if self.current_run is not None:\n            raise RuntimeError(\n                \"Cannot run an experiment while another run is active.\"\n            )\n\n        run = self._create_run(\n            command_name, config_updates, named_configs, info, meta_info, options\n        )\n        try:\n            run.run()\n        finally:\n            self.current_run = None\n        return run\n", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        cf = self.capture(function, prefix=prefix)\n        self.commands[function.__name__] = cf\n        return cf\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.configurations.append(config_scope)\n        return config_scope", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self._add_named_config(func.__name__, ConfigScope(func))\n        return self.named_configs[func.__name__]", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for cmd_name, cmd in ingredient.commands.items():\n                cmd_name = join_paths(ingredient.path, cmd_name)\n                cmd_name = self.post_process_name(cmd_name, ingredient)\n                yield cmd_name, cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                yield name, conf", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(\"invalid filename or file not found {filename}\")\n        filename = get_py_file_if_possible(filename)\n        digest = get_digest(filename)\n        repo, commit, is_dirty = get_commit_if_possible(filename, save_git_info)\n        return Source(filename, digest, repo, commit, is_dirty)\n\n    ", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            return os.path.relpath(self.filename, base_dir), self.digest\n        else:\n            return self.filename, self.digest", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in cls.modname_to_dist:\n            return cls(cls.modname_to_dist[mod.__name__], mod.__version__)\n        else:\n            return cls(mod.__name__, mod.__version__)\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if filename.startswith(experiment_path):\n        return True\n    if modname.startswith(experiment_path):\n        return True\n    return False", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    experiment_path, main = get_main_file(globs, save_git_info)\n    if base_dir is None:\n        base_dir = experiment_path\n\n    sources = source_discovery_strategies[SETTINGS.SOURCE_DISCOVERY](\n        globs, base_dir, save_git_info\n    )\n    if main is not None:\n        sources.add(main)\n\n    dependencies = dependency_discovery_strategies[SETTINGS.DEPENDENCY_DISCOVERY](\n        globs, base_dir\n    )\n\n    if SETTINGS.DEPENDENCY_DISCOVERY != \"none\":\n        try:\n            import numpy\n\n            dependencies.add(PackageDependency.create(numpy))\n        except ImportError:\n            pass\n\n    return main, sources, dependencies\n\n", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        self.save_file(filename)\n        self.run_entry[\"resources\"].append(filename)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        # If the signature is bound to an instance, the first argument is the instance itself, so we need to skip it.\n        if bound:\n            args = args[1:]\n\n        # If there is a variable-length argument, we need to remove it from the list of free parameters.\n        if self.vararg_name:\n            args = args[:-1]\n\n        # If there is a wildcard keyword argument, we need to remove it from the list of free parameters.\n        if self.kw_wildcard_name:\n            kwargs = {}\n\n        # If there are positional-only arguments, we need to remove them from the list of free parameters.\n        if self.positional_args:\n            args = args[len(self.positional_args) :]\n\n        # If there are keyword-only arguments, we need to remove them from the list of free parameters.\n        if self.kwargs:\n            kwargs = {k: v for k, v in kwargs.items() if k not in self.kwargs}\n\n        # Return the list of free parameters.\n        return args + list(kwargs)", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        self._assert_no_unexpected_args(expected_args, args)\n        self._assert_no_unexpected_kwargs(expected_args, kwargs)\n        self._assert_no_duplicate_args(expected_args, args, kwargs)\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n        self._assert_no_missing_args(args, kwargs, bound)\n        return args, kwargs", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self.fixed[key], DogmaticDict):\n                missing_keys |= {\n                    join_paths(key, m) for m in self.fixed[key].revelation()\n                }\n        return missing_keys\n", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.split(\"\\n\")\n    indent = \"\"\n    for line in lines:\n        if not is_empty_or_comment(line):\n            indent = \"\".join(\n                [\n                    a\n                    for a, b in zip(line, lines[0])\n                    if a == b and a in [\" \", \"\\t\"]\n                ]\n            )\n            break\n    return \"\\n\".join([dedent_line(line, indent) for line in lines])", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        kwonlyargs=self.kwonlyargs,\n                                        kwonlydefaults=self.kwonlydefaults,\n                                        annotations=self.annotations if with_annotations else {})\n            return sig", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            if self.kwonlyargs:\n                sig_str = self.get_sig_str(with_annotations=False)\n                sig_str = self._KWONLY_MARKER.sub('', sig_str)\n            else:\n                sig_str = self.get_sig_str(with_annotations=False)\n            return sig_str[1:-1]", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            func = func.func\n        fb = cls(func.__name__, **cls._argspec_to_dict(func))\n        fb.doc = func.__doc__\n        fb.module = func.__module__\n        fb.is_async = _inspect_iscoroutinefunction(func)\n        fb.dict = dir_dict(func)\n        return fb\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        if self.defaults:\n            defaults_dict.update(dict(zip(reversed(self.args), reversed(self.defaults))))\n        if self.kwonlydefaults:\n            defaults_dict.update(self.kwonlydefaults)\n        return defaults_dict\n", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(self.args)\n        else:\n            return tuple(self.args + self.kwonlyargs)\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                binary_type.__name__,\n                type(s).__name__\n            ))\n        if self.buffer.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        self.seek(0)\n        if self._rolled:\n            self.seek(0, os.SEEK_END)\n            ret = self.tell()\n        else:\n            ret = len(self.buffer.getvalue())\n        self.seek(pos)\n        return ret\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            ret = self.buffer.read().decode('utf-8')\n        else:\n            ret = self.buffer.read(n).decode('utf-8')\n        self._tell = self.tell() + len(ret)\n        return ret", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self._traverse_codepoints(0, pos)\n        elif mode == os.SEEK_CUR:\n            self._traverse_codepoints(self._tell, pos)\n        elif mode == os.SEEK_END:\n            self._traverse_codepoints(self.len, pos)\n        else:\n            raise ValueError('Invalid whence ({mode}, should be 0, 1, or 2)'.format(mode=mode))\n        return self._tell", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        return self._tell", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            amt = -1\n        if amt < 0:\n            amt = float('inf')\n        ret = []\n        while amt > 0:\n            if self._index >= len(self._fileobjs):\n                break\n            f = self._fileobjs[self._index]\n            data = f.read(amt)\n            if not data:\n                self._index += 1\n                continue\n            ret.append(data)\n            amt -= len(data)\n        return self._joiner.join(ret)\n\n    ", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n        self._index = 0\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index < 0:\n            index += len(self)\n        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            raise IndexError()\n        self.lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(a) == 0:\n            return self.pop()\n        elif len(a) == 1:\n            index = a[0]\n            if len(self.lists) == 1:\n                return self.lists[0].pop(index)\n            else:\n                list_idx, rel_idx = self._translate_index(index)\n                if list_idx is None:\n                    raise IndexError()\n                return self.lists[list_idx].pop(rel_idx)\n        else:\n            raise TypeError('pop expected at most 1 arguments, got %d' % len(a))\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        self.lists = [sorted(l) for l in self.lists]\n        self._balance_list(0)\n", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        if path_text.startswith(u'/'):\n            path_text = path_text[1:]\n        self.path_parts = tuple([unquote(p) if '%' in p else p\n                                 for p in path_text.split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        dest = to_unicode(dest)\n        if dest.startswith('//'):\n            dest = self.scheme + ':' + dest\n        elif dest.startswith('/'):\n            dest = self.scheme + '://' + self.host + dest\n        elif dest.startswith('?'):\n            dest = self.scheme + '://' + self.host + self.path + dest\n        elif dest.startswith('#'):\n            dest = self.scheme + '://' + self.host + self.path + '?' + self.query_params.to_text() + dest\n        elif dest.startswith('mailto:'):\n            dest = dest\n        else:\n            dest = self.scheme + '://' + self.host + '/' + dest\n        return URL(dest).normalize()\n", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            if self.uses_netloc:\n                _add(self._netloc_sep)\n        if self.uses_netloc:\n            _add(self.get_authority(full_quote=full_quote,\n                                    with_userinfo=True))\n        if self.path:\n            _add(self.path)\n        if self._query:\n            _add('?')\n            _add(self._query)\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        parts = []\n        _add = parts.append\n        for key, values in self.iteritems(multi=True):\n            for value in values:\n                _add(quote_query_part(key, full_quote=full_quote))\n                _add('=')\n                _add(quote_query_part(value, full_quote=full_quote))\n                _add('&')\n        if parts:\n            parts.pop()\n        return u''.join(parts)\n\n", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n        if tb is None:\n            raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = 'Traceback (most recent call last):\\n'\n        ret += ''.join([f.tb_frame_str() for f in self.tb_info.frames])\n        ret += '%s: %s' % (self.exc_type, self.exc_msg)\n        return ret", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if tb:\n        traceback.print_tb(tb, limit, file)\n    lines = format_exception_only(etype, value)\n    for line in lines:\n        print(line, file=file, end='')", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        ret = []\n        for frame in self.frames:\n            ret.append('  File \"%s\", line %s, in %s' % (\n                frame['filepath'], frame['lineno'], frame['funcname']))\n            if frame['source_line']:\n                ret.append('    %s' % frame['source_line'])\n        ret.append('%s: %s' % (self.exc_type, self.exc_msg))\n        return '\\n'.join(ret)", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        if isinstance(tb_str, bytes):\n            tb_str = tb_str.decode('utf-8')\n        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n        for line in lines:\n            if line.startswith('Traceback (most recent call last):'):\n                continue\n            m = _frame_re.match(line)\n            if m:\n                frames.append(m.groupdict())\n                continue\n            if line.startswith('  File '):\n                continue\n            if line.startswith('    '):\n                frames[-1]['source_line'] = line[4:]\n                continue\n            if exc_type is None:\n                exc_type, exc_msg = line.split(':', 1)\n                exc_type = exc_type.strip()\n                exc_msg = exc_msg.strip()\n        return cls(exc_type, exc_msg, frames)\n", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._data.extend(data)\n        self._set_width()\n        self._fill()\n", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{0}(headers={1}, data={2})'.format(\n                type(self).__name__, self.headers, self._data)\n        return '{0}({1})'.format(type(self).__name__, self._data)", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        if not self._data:\n            return ''\n        if with_headers:\n            headers = self.headers\n            if not headers:\n                headers = [str(i) for i in range(self._width)]\n            else:\n                headers = [str(h) for h in headers]\n            headers = [to_text(h, maxlen=maxlen) for h in headers]\n            max_len = max([len(h) for h in headers])\n            headers = [h.center(max_len) for h in headers]\n            header_row = ' | '.join(headers)\n            header_sep = '-|-'.join(['-' * len(h) for h in headers])\n            lines = [header_row, header_sep]\n        else:\n            lines = []\n        for row in self._data:\n            row = [to_text(cell, maxlen=maxlen) for cell in row]\n            max_len = max([len(cell) for cell in row])\n            row = [cell.center(max_len) for cell in row]\n            lines.append(' | '.join(row))\n        return '\\n'.join(lines)", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds(**kw)\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(count=bins, **kw)\n        elif isinstance(bins, list):\n            bins = sorted(bins)\n        else:\n            raise ValueError('expected int or list of float for bins, not %r'\n                             % bins)\n\n        bin_counts = []\n        for i, bin_val in enumerate(bins):\n            if i == 0:\n                bin_counts.append((bin_val, len([v for v in self.data if v < bin_val])))\n            else:\n                bin_counts.append((bin_val, len([v for v in self.data if bins[i - 1] <= v < bin_val])))\n\n        return bin_counts\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item in self.item_index_map:\n            return\n        self.item_index_map[item] = len(self.item_list)\n        self.item_list.append(item)\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        index = self._get_real_index(index)\n        try:\n            ret = self.item_list[index]\n        except IndexError:\n            raise IndexError('IndexedSet index out of range')\n        del self.item_list[index]\n        del self.item_index_map[ret]\n        self._add_dead(index)\n        self._cull()\n        return ret\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val not in self.item_index_map:\n            raise ValueError('{0} is not in {1}'.format(val, type(self).__name__))\n        return self._get_apparent_index(self.item_index_map[val])\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    if type(wrapped) in (set, frozenset):\n        return _ComplementSet(included=wrapped)\n    elif type(wrapped) is _ComplementSet:\n        return wrapped.complemented()\n    else:\n        raise TypeError('argument must be a set or complement(set)')", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, unicode):\n        return ANSI_SEQUENCES.sub('', text)\n    elif isinstance(text, bytes):\n        return ANSI_SEQUENCES.sub(b'', text)\n    elif isinstance(text, bytearray):\n        return ANSI_SEQUENCES.sub(b'', text)\n    else:\n        raise ValueError('expected text or bytes, not %r' % type(text))", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    if isinstance(text, unicode):\n        text = text.encode('utf-8')\n    if isinstance(text, str):\n        text = text.decode('utf-8')\n    if ignore:\n        return text.translate(DEACCENT_MAP).encode('ascii', 'ignore')\n    else:\n        return text.translate(DEACCENT_MAP).encode('ascii', 'replace')", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    for line in iter_splitlines(text):\n        if key(line):\n            yield margin + line\n        else:\n            yield line\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    return MultiReplace(sub_map, **kwargs).sub(text)", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        anchor = self._anchor\n        cur = anchor[NEXT]\n        while cur is not anchor:\n            yield (cur[KEY], cur[VALUE])\n            cur = cur[NEXT]\n        return\n", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n\n            self.hit_count += 1\n            del self[key]\n            return link[VALUE]\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self:\n                raise KeyError('popitem(): dictionary is empty')\n            key = self._anchor[PREV][KEY]\n            ret = (key, self.pop(key))\n            return ret\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if default is _MISSING:\n                    raise\n                ret = self[key] = default\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        with self._lock:\n            if E is not None:\n                if hasattr(E, 'keys'):\n                    for k in E.keys():\n                        self[k] = E[k]\n                else:\n                    for k, v in E:\n                        self[k] = v\n            if F:\n                self.update(F)\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return '%s(max_size=%r, on_miss=%r, values=%r)' % (\n            self.__class__.__name__, self.max_size, self.on_miss,\n            super(LRI, self).__repr__())\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        if self.scoped or self.typed:\n            return ('%s(func=%r, scoped=%r, typed=%r)'\n                    % (cn, self.func, self.scoped, self.typed))\n        return '%s(func=%r)' % (cn, self.func)", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, (count, bucket) in self._count_map.items():\n            for i in xrange(count):\n                yield key\n\n    ", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            n = len(self._count_map)\n        return heapq.nlargest(n, self._count_map.items(), key=lambda x: x[1][0])\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, dict):\n            iterable = iterable.items()\n        for k, v in iterable:\n            self[k] += v\n        for k, v in kwargs.items():\n            self[k] += v\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a][0]\n        if self.free:\n            freed = heapq.heappop(self.free)\n            self.mapping[a] = freed, self.ref_map[freed]\n            return freed\n        else:\n            ref = len(self.mapping)\n            self.mapping[a] = ref, ref\n            self.ref_map[ref] = ref\n            return ref\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n    size = _validate_positive_int(size, 'chunk size')\n    if count is not None:\n        count = _validate_positive_int(count, 'count')\n    if kw:\n        raise ValueError('got unexpected keyword arguments: %r' % kw.keys())\n    if not src:\n        return\n    postprocess = lambda chk: chk\n    if isinstance(src, basestring):\n        postprocess = lambda chk, _sep=type(src)(): _sep.join(chk)\n        if _IS_PY3 and isinstance(src, bytes):\n            postprocess = lambda chk: bytes(chk)\n    src_iter = iter(src)\n    while True:\n        cur_chunk = list(itertools.islice(src_iter, size))\n        if not cur_chunk:\n            break\n        lc = len(cur_chunk)\n        if lc < size:\n            cur_chunk[lc:] = [fill_val] * (size - lc)\n        yield postprocess(cur_chunk)\n    return", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size < 0:\n        raise ValueError(\"input_size must be positive\")\n    if chunk_size < 0:\n        raise ValueError(\"chunk_size must be positive\")\n    if input_offset < 0:\n        raise ValueError(\"input_offset must be positive\")\n    if overlap_size < 0:\n        raise ValueError(\"overlap_size must be positive\")\n    if overlap_size >= chunk_size:\n        raise ValueError(\"overlap_size must be smaller than chunk_size\")\n    if input_offset >= input_size:\n        raise ValueError(\"input_offset must be smaller than input_size\")\n\n    if align:\n        chunk_size = chunk_size - overlap_size\n\n    if input_size == 0:\n        return\n\n    if input_offset + chunk_size > input_size:\n        yield (input_offset, input_size)\n        return\n\n    if align:\n        yield (input_offset, input_offset + chunk_size)\n        input_offset += chunk_size - overlap_size\n\n    while input_offset + chunk_size < input_size:\n        yield (input_offset, input_offset + chunk_size)\n        input_offset += chunk_size - overlap_size\n\n    if input_offset < input_size:\n        yield (input_offset, input_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if not callable(visit):\n        raise TypeError('expected callable visit function')\n    if not callable(enter):\n        raise TypeError('expected callable enter function')\n    if not callable(exit):\n        raise TypeError('expected callable exit function')\n    if kwargs:\n        raise TypeError('got unexpected keyword arguments: %r' % kwargs.keys())\n\n    def _remap(path, key, value):\n        new_key, new_value = visit(path, key, value)\n        if new_value is _REMAP_EXIT:\n            return _REMAP_EXIT\n        if new_value is False:\n            return False\n        if new_value is True:\n            return True\n        if new_value is None:\n            new_value = value\n        new_parent, new_items = enter(path, key, new_value)\n        if new_items is False:\n            return False\n        if new_items is True:\n            return True\n        if new_items is None:\n            new_items = []\n        new_items = [(k, _remap(path + (key,), k, v))\n                     for k, v in new_items]\n        new_items = [(k, v) for k, v in new_items if v is not False]\n        new_parent = exit(path, key, new_parent, new_parent, new_items)\n        return new_parent\n\n    return _remap((), None, root)\n\n", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not isinstance(path, tuple):\n        raise TypeError('expected tuple path, not: %r' % path)\n    if not path:\n        return root\n    try:\n        for seg in path:\n            root = root[seg]\n    except (KeyError, IndexError, TypeError) as exc:\n        raise PathAccessError(exc, seg, path)\n    return root\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n    reraise = bool(reraise)\n    path, registry, stack = (), {}, [(None, root)]\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if id_value in registry:\n            value = registry[id_value]\n        else:\n            res = enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        if query is _orig_default_visit:\n            # avoid function call overhead by inlining identity operation\n            visited_item = (key, value)\n        else:\n            try:\n                visited_item = query(path, key, value)\n            except Exception:\n                if reraise:\n                    raise\n                visited_item = True\n            if visited_item is False:\n                continue  # drop\n            elif visited_item is True:\n                visited_item = (key, value)\n            # TODO: typecheck?\n            #    raise TypeError('expected (key, value) from visit(),'\n            #                    ' not: %r' % visited_item)\n        try:\n            new_items_stack[-1][1].append(visited_item)\n        except IndexError:\n            raise TypeError('expected remappable root, not: %r' % root)\n    return value\n\n", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        with self._recv_lock:\n            if flags:\n                raise ValueError(\"non-zero flags not supported\")\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if timeout is None:\n                timeout = self.timeout\n            if timeout == 0:\n                sock = self.sock\n                sock.settimeout(0)\n                try:\n                    data = sock.recv(size)\n                except socket.error as e:\n                    if e.errno == errno.EWOULDBLOCK:\n                        raise Timeout(timeout, 'socket recv timed out')\n                    else:\n                        raise\n                else:\n                    if not data:\n                        raise ConnectionClosed('connection closed')\n                    return data\n            else:\n                if len(self.rbuf) >= size:\n                    data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n                    return data\n                data = self.recv_size(size, timeout=timeout)\n                self.rbuf = data + self.rbuf\n                return data\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if maxsize is None:\n                maxsize = _RECV_LARGE_MAXSIZE\n            if timeout is _UNSET:\n                timeout = self.timeout\n            chunks = []\n            total_bytes = 0\n            try:\n                start = time.time()\n                self.sock.settimeout(timeout)\n                nxt = self.rbuf or self.sock.recv(self._recvsize)\n                while nxt:\n                    total_bytes += len(nxt)\n                    if total_bytes >= maxsize:\n                        break\n                    chunks.append(nxt)\n                    if timeout:\n                        cur_timeout = timeout - (time.time() - start)\n                        if cur_timeout <= 0.0:\n                            raise socket.timeout()\n                        self.sock.settimeout(cur_timeout)\n                    nxt = self.sock.recv(self._recvsize)\n                else:\n                    msg = ('connection closed after reading %s of %s requested'\n                           ' bytes' % (total_bytes, maxsize))\n                    raise ConnectionClosed(msg)  # check recv buffer\n            except socket.timeout:\n                self.rbuf = b''.join(chunks)\n                msg = 'read %s of %s bytes' % (total_bytes, maxsize)\n                raise Timeout(timeout, msg)  # check recv buffer\n            except Exception:\n                # received data is still buffered in the case of errors\n                self.rbuf = b''.join(chunks)\n                raise\n            extra_bytes = total_bytes - maxsize\n            if extra_bytes:\n                last, self.rbuf = nxt[:-extra_bytes], nxt[-extra_bytes:]\n            else:\n                last, self.rbuf = nxt, b''\n            chunks.append(last)\n        return b''.join(chunks)", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.send(b'', timeout=self.timeout)\n        return", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._recv_lock:\n            with self._send_lock:\n                self.sock.close()\n                self.rbuf = b''\n                self.sbuf = []\n        return", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n        size_prefix = str(len(payload)).encode('ascii')\n        self.bsock.send(size_prefix + b':' + payload + b',')\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        return '{{0:0{0}x}}'.format((self.len + 3) // 4).format(self.val)", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n\n    ", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: memoize\n    ret = []\n    formatter = Formatter()\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit:\n            ret.append(lit)\n        if fname is not None:\n            ret.append(construct_format_field_str(fname, fspec, conv))\n    return ret", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # TODO: memoize\n    formatter = Formatter()\n    fargs, fkwargs, _dedup = [], [], set()\n\n    def _add_arg(argname, type_char='s'):\n        if argname not in _dedup:\n            _dedup.add(argname)\n            argtype = _TYPE_MAP.get(type_char, str)  # TODO: unicode\n            try:\n                fargs.append((int(argname), argtype))\n            except ValueError:\n                fkwargs.append((argname, argtype))\n\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is not None:\n            type_char = fspec[-1:]\n            fname_list = re.split('[.[]', fname)\n            if len(fname_list) > 1:\n                raise ValueError('encountered compound format arg: %r' % fname)\n            try:\n                base_fname = fname_list[0]\n                assert base_fname\n            except (IndexError, AssertionError):\n                raise ValueError('encountered anonymous positional argument')\n            _add_arg(fname, type_char)\n            for sublit, subfname, _, _ in formatter.parse(fspec):\n                # TODO: positional and anon args not allowed here.\n                if subfname is not None:\n                    _add_arg(subfname)\n    return fargs, fkwargs", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    formatter = Formatter()\n    ret = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        field = BaseFormatField(fname, fspec, conv)\n        ret.append(field)\n    return ret", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            val = self[key]\n            dict.__delitem__(self, key)\n            dict.__delitem__(self.inv, val)\n            return val\n        if default is _MISSING:\n            raise KeyError(key)\n        return default", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        dict.__delitem__(self.inv, val)\n        return key, val\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if type(iterable) is ManyToMany:\n            self.data.update(iterable.data)\n            self.inv.data.update(iterable.inv.data)\n        elif hasattr(iterable, 'items'):\n            for key, vals in iterable.items():\n                self[key] = vals\n        else:\n            for key, val in iterable:\n                self.add(key, val)\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key in self.data:\n            self.data[newkey] = self.data.pop(key)\n            for val in self.data[newkey]:\n                self.inv.data[val].remove(key)\n                self.inv.data[val].add(newkey)\n        return\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key, vals in self.data.items():\n            yield key, frozenset(vals)\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        for k, v in sorted(self.settings.items()):\n            if callable(v.value):\n                v = \"<%s()>\" % v.value.__qualname__\n            lines.append(\"%s = %s\" % (k.ljust(self.settings[k].order), v))\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gthread\"\n\n        worker_class = util.load_class(\n            uri,\n            default=\"gunicorn.workers.sync.SyncWorker\",\n            section=\"gunicorn.workers\")\n\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        raw_addresses = self.settings['bind'].get()\n        addresses = []\n        for address in raw_addresses:\n            if address.startswith('unix:'):\n                address = address[5:]\n                if not os.path.exists(address):\n                    raise ValueError(\"unix socket path %r does not exist\" % address)\n            elif address.startswith('fd:'):\n                address = address[3:]\n                if not address.isdigit():\n                    raise ValueError(\"invalid file descriptor: %r\" % address)\n            else:\n                address = util.parse_address(address, default_port='8000')\n            addresses.append(address)\n        return addresses\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            return \"gunicorn.glogging.Logger\"\n\n        if uri == \"gunicorn.instrument.statsd.Statsd\" and self.statsd_host:\n            uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(uri)\n        if hasattr(logger_class, \"setup\"):\n            logger_class.setup()\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    sockets = []\n    if fds is None:\n        for addr in conf.addresses:\n            sock_type = _sock_type(addr)\n            sock = sock_type(addr, conf, log)\n            sockets.append(sock)\n    else:\n        for fd in fds:\n            sock = _sock_type(conf.addresses[0])(conf.addresses[0], conf, log, fd=fd)\n            sockets.append(sock)\n\n    if conf.is_ssl and not conf.certfile:\n        raise RuntimeError(\"You must specify a certificate file when using SSL\")\n\n    if conf.is_ssl and not conf.keyfile:\n        raise RuntimeError(\"You must specify a key file when using SSL\")\n\n    return sockets", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                break\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        data = self.buf.getvalue()\n        if size <= len(data):\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n\n        while size > len(data):\n            data += self.reader.read(1024)\n\n        ret, rest = data[:size], data[size:]\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, type(None))):\n            raise TypeError(\"size parameter must be an int or long\")\n        if size is None:\n            size = -1\n        elif size < 0:\n            size = None\n        self.buf.seek(0, os.SEEK_END)\n        if size == 0:\n            return b\"\"\n        if self.buf.tell():\n            return self.read_from_buffer(size)\n        return self.chunk()\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.write(data)\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        if self.iter is None:\n            return b\"\"\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)\n\n    ", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        if not self.sock:\n            return\n\n        # calculate duration in milliseconds\n        duration = int(request_time.total_seconds() * 1000)\n\n        # log duration as histogram\n        self.histogram(\"gunicorn.request.duration\", duration)\n\n        # increment total request count\n        self.increment(\"gunicorn.request.count\", 1)\n\n        # increment count for different status codes\n        status_code = resp.status.split(None, 1)[0]\n        if isinstance(status_code, str):\n            status_code = int(status_code.split(None, 1)[0])\n        self.increment(\"gunicorn.request.status.{0}\".format(status_code), 1)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        error_str = self.error_type\n        if self.message:\n            error_str += f\": {self.message}\"\n        if self.field:\n            error_str += f\" on field {self.field}\"\n        return error_str\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return (\n            f\"{self.__class__.__name__}(error_type={self.error_type},\"\n            f\" message={self.message}, field={self.field})\"\n        )", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._set[item] = None\n        self._set.move_to_end(item)\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._base / 16, self._base / 16)\n        value = self._base + jitter\n        if value <= self._max / 2:\n            self._base *= 2\n        else:\n            self._base = self._max\n        return value", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"data\" in listing:\n                return listing[\"data\"]\n            elif \"children\" in listing:\n                return listing[\"children\"]\n            elif \"treatments\" in listing:\n                return listing[\"treatments\"]\n            elif \"items\" in listing:\n                return listing[\"items\"]\n            elif \"subreddits\" in listing:\n                return listing[\"subreddits\"]\n            elif \"sr_listings\" in listing:\n                return listing[\"sr_listings\"]\n            elif \"user_listings\" in listing:\n                return listing[\"user_listings\"]\n            elif \"user_reports\" in listing:\n                return listing[\"user_reports\"]\n            elif \"rules\" in listing:\n                return listing[\"rules\"]\n            elif \"site_rules\" in listing:\n                return listing[\"site_rules\"]\n            elif \"reports\" in listing:\n                return listing[\"reports\"]\n            elif \"other_reports\" in listing:\n                return listing[\"other_reports\"]\n            elif \"result\" in listing:\n                return listing[\"result\"]\n            elif \"log\" in listing:\n                return listing[\"log\"]\n            elif \"wikis\" in listing:\n                return listing[\"wikis\"]\n            elif \"modqueue\" in listing:\n                return listing[\"modqueue\"]\n            elif \"articles\" in listing:\n                return listing[\"articles\"]\n            elif \"images\" in listing:\n                return listing[\"images\"]\n            elif \"users\" in listing:\n                return listing[\"users\"]\n            elif \"subreddits\" in listing:\n                return listing[\"subreddits\"]\n            elif \"subreddit_names\" in listing:\n                return listing[\"subreddit_names\"]\n            elif \"subreddit_names\" in listing:\n                return listing[\"subreddit_names\"]\n            elif \"subreddit_names\" in listing:\n                return listing[\"subreddit_names\"]\n            elif \"subreddit_names\" in listing:\n                return listing[\"", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as fp:\n            print(authorizer.refresh_token, file=fp)\n\n    ", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()\n", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(self.key)\n        return result[0]", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        return result is not None", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        authorizer.refresh_token = self._get()", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        try:\n            self._get()\n        except KeyError:\n            self._set(refresh_token)\n            return True\n        return False", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        import platform\n        import sys\n        import jc\n        import ruamel.yaml\n        import jmespath\n        import jq\n        import jello\n        import jqp\n        import jtbl\n        import jc.parsers.proc\n        import jc.parsers.proc_pid_cmdline\n        import jc.parsers.proc_pid_environ\n        import jc.parsers.proc_pid_fdinfo\n        import jc.parsers.proc_pid_io\n        import jc.parsers.proc_pid_maps\n        import jc.parsers.proc_pid_mountinfo\n        import jc.parsers.proc_pid_numa_maps\n        import jc.parsers.proc_pid_sched\n        import jc.parsers.proc_pid_stat\n        import jc.parsers.proc_pid_statm\n        import jc.parsers.proc_pid_status\n        import jc.parsers.proc_pid_task_children\n        import jc.parsers.proc_pid_task_children_rss\n        import jc.parsers.proc_pid_task_children_threads\n        import jc.parsers.proc_pid_task_children_threads_stack\n        import jc.parsers.proc_pid_task_children_threads_stack_memory\n        import jc.parsers.proc_pid_task_children_threads_stack_memory_maps\n        import jc.parsers.proc_pid_task_children_threads_stack_memory_maps_address\n        import jc.parsers.proc_pid_task_children_threads_stack_memory_maps_permissions\n        import jc.parsers.proc_pid_task_children_threads_stack_memory_maps_pathname\n        import jc.parsers.proc_pid_task_children_threads_stack_memory_maps_rss\n        import jc.parsers.proc_pid_task_children_threads_stack_memory", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            from ruamel.yaml import YAML\n            from ruamel.yaml.compat import StringIO\n\n            yaml = YAML()\n            yaml.indent(mapping=2, sequence=4, offset=2)\n            yaml.preserve_quotes = True\n            yaml.width = 120\n            yaml.allow_unicode = True\n            yaml.compact(seq_seq=False, seq_map=False)\n            yaml.explicit_start = True\n            yaml.explicit_end = True\n            yaml.indent(sequence=4, offset=2)\n            yaml.indent(mapping=2, offset=2)\n            yaml.preserve_quotes = True\n            yaml.width = 120\n            yaml.allow_unicode = True\n            yaml.compact(seq_seq=False, seq_map=False)\n            yaml.explicit_start = True\n            yaml.explicit_end = True\n            yaml.indent(sequence=4, offset=2)\n            yaml.indent(mapping=2, offset=2)\n            yaml.preserve_quotes = True\n            yaml.width = 120\n            yaml.allow_unicode = True\n            yaml.compact(seq_seq=False, seq_map=False)\n            yaml.explicit_start = True\n            yaml.explicit_end = True\n            yaml.indent(sequence=4, offset=2)\n            yaml.indent(mapping=2, offset=2)\n            yaml.preserve_quotes = True\n            yaml.width = 120\n            yaml.allow_unicode = True\n            yaml.compact(seq_seq=False, seq_map=False)\n            yaml.explicit_start = True\n            yaml.explicit_end = True\n            yaml.indent(sequence=4, offset=2)\n            yaml.indent(mapping=2, offset=2)\n            yaml.preserve_quotes = True\n            yaml.width = 120", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    jc.utils.input_type_check(data)\n\n    raw_output: JSONDictType = {}\n\n    if jc.utils.has_data(data):\n\n        for line in filter(None, data.splitlines()):\n            key, val = line.split('=', maxsplit=1)\n            key = key.strip()\n            val = val.strip().strip('\"')\n            raw_output[key] = val\n\n    return raw_output if raw else _process(raw_output)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    next_line = next_lines.pop()\n    result = re.match(_screen_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    matches = result.groupdict()\n    screen: Screen = {\n        \"devices\": [],\n        \"screen_number\": int(matches[\"screen_number\"]),\n        \"minimum_width\": int(matches[\"minimum_width\"]),\n        \"minimum_height\": int(matches[\"minimum_height\"]),\n        \"current_width\": int(matches[\"current_width\"]),\n        \"current_height\": int(matches[\"current_height\"]),\n        \"maximum_width\": int(matches[\"maximum_width\"]),\n        \"maximum_height\": int(matches[\"maximum_height\"]),\n    }\n\n    while next_lines:\n        next_device = _parse_device(next_lines)\n        if next_device:\n            screen[\"devices\"].append(next_device)\n        else:\n            break\n    return screen\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    edid_lines: List[str] = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        edid_lines.append(result.groupdict()[\"edid_line\"])\n\n    edid_lines.reverse()\n    edid_hex = \"\".join(edid_lines)\n    edid_bytes = _hex_to_bytes(edid_hex)\n    if not edid_bytes:\n        return None\n\n    try:\n        from pyedidlib.parser import EDIDParser\n\n        edid_parser = EDIDParser(edid_bytes)\n        model: Model = {\n            \"name\": edid_parser.get_name(),\n            \"product_id\": edid_parser.get_product_id(),\n            \"serial_number\": edid_parser.get_serial_number(),\n        }\n        return model\n    except ImportError:\n        if not quiet:\n            jc.utils.warning_message(\n                [\n                    \"pyedidlib is not installed. Please install it to parse EDID information.\"\n                ]\n            )\n        return None\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None\n        and len(matches[\"is_high_resolution\"]) > 0,\n        \"frequencies\": [],\n    }\n\n    frequencies_result = re.match(_frequencies_pattern, matches[\"rest\"])\n    if not frequencies_result:\n        return mode\n\n    frequencies_matches = frequencies_result.groupdict()\n    frequencies = frequencies_matches[\"frequency\"].split(\" \")\n    stars = frequencies_matches[\"star\"].split(\" \")\n    pluses = frequencies_matches[\"plus\"].split(\" \")\n\n    for i, frequency in enumerate(frequencies):\n        mode[\"frequencies\"].append(\n            {\n                \"frequency\": float(frequency),\n                \"is_current\": stars[i] == \"*\",\n                \"is_preferred\": pluses[i] == \"+\",\n            }\n        )\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, 'arch-' + self.command_prefix),\n        ]\n", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return '{}-linux-android{}'.format(self.command_prefix, self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipes:\n            return cls.recipes[name]\n\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if recipes_dir and exists(recipes_dir):\n                for recipe_name in listdir(recipes_dir):\n                    if recipe_name == name:\n                        recipe_dir = join(recipes_dir, recipe_name)\n                        if isdir(recipe_dir):\n                            module_name = 'pythonforandroid.recipes.{}'.format(name)\n                            try:\n                                module = __import__(module_name, fromlist=['Recipe'])\n                            except ImportError:\n                                raise ValueError('Failed to import recipe {}'.format(name))\n                            recipe = module.Recipe(ctx)\n                            cls.recipes[name] = recipe\n                            return recipe\n\n        raise ValueError('Recipe {} not found'.format(name))\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"python-for-android can automatically install Homebrew on macOS, but it's not supported yet.\"\n        )\n        info(\n            \"Please follow the instructions at https://brew.sh/ to install Homebrew manually.\"\n        )\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            is not None\n        )", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    if platform == \"darwin\":\n        return [\n            prerequisite()\n            for prerequisite in prerequisites\n            if prerequisite.mandatory[platform]\n        ]\n    elif platform == \"linux\":\n        return [\n            prerequisite()\n            for prerequisite in prerequisites\n            if prerequisite.mandatory[platform]\n        ]\n    else:\n        raise Exception(\"Unsupported platform\")", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    if dep.startswith(\"file://\"):\n        # This is a file:// URL. Resolve it:\n        dep = urlunquote(urlparse(dep).path)\n    if os.path.exists(dep):\n        return dep\n    return None", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache:\n        if dependency in package_name_cache:\n            if package_name_cache[dependency][\"expiry\"] > time.time():\n                return package_name_cache[dependency][\"name\"]\n\n    name = _extract_info_from_package(dependency, extract_type=\"name\")\n    package_name_cache[dependency] = {\n        \"name\": name,\n        \"expiry\": time.time() + 60 * 60 * 24,  # 24 hours\n    }\n    return name", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    source_properties_path = join(ndk_dir, 'source.properties')\n    try:\n        with open(source_properties_path) as fh:\n            source_properties = fh.read()\n    except IOError:\n        warning(UNKNOWN_NDK_MESSAGE)\n        warning(ENSURE_RIGHT_NDK_MESSAGE.format(\n            min_supported=MIN_NDK_VERSION,\n            rec_version=RECOMMENDED_NDK_VERSION,\n            ndk_url=NDK_DOWNLOAD_URL,\n        ))\n        return\n\n    try:\n        version = LooseVersion(\n            [\n                line.split('=')[1]\n                for line in source_properties.splitlines()\n                if line.startswith('Pkg.Revision')\n            ][0]\n        )\n    except IndexError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(ENSURE_RIGHT_NDK_MESSAGE.format(\n            min_supported=MIN_NDK_VERSION,\n            rec_version=RECOMMENDED_NDK_VERSION,\n            ndk_url=NDK_DOWNLOAD_URL,\n        ))\n        return\n\n    return version", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            ),\n            instructions=(\n                'Please, set the target API to a value lower than {max_ndk_api}.'\n                ' If you get build errors, download the recommended NDK {rec_version} from {ndk_url}.'\n            ).format(\n                max_ndk_api=ARMEABI_MAX_TARGET_API,\n                rec_version=RECOMMENDED_NDK_VERSION,\n                ndk_url=NDK_DOWNLOAD_URL,\n            ),\n        )", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='You probably want to build with --ndk-api={}'.format(\n                android_api))", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(\n            self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag\n        )\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = storage_dir\n        self.build_dir = join(storage_dir, 'builds')\n        self.dist_dir = join(storage_dir, 'dists')\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if recipe.depends is None:\n        return []\n    if blacklist is None:\n        blacklist = set()\n    deps = fix_deplist(recipe.depends)\n    deps = [dep for dep in deps if dep[0] not in blacklist]\n    return deps", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    blacklist = {bitem.lower() for bitem in blacklist}\n\n    # Add all recipes to the graph, so that we can check for conflicts\n    # against them\n    graph = {}\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            if name not in blacklist:\n                graph[name] = set()\n\n    # Add all dependencies to the graph, so that we can check for conflicts\n    # against them\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            if name not in blacklist:\n                try:\n                    recipe = Recipe.get_recipe(name, ctx)\n                    dependencies = get_dependency_tuple_list_for_recipe(\n                        recipe, blacklist=blacklist\n                    )\n                except ValueError:\n                    # The recipe does not exist, so we assume it can be\n                    # installed via pip with no extra dependencies\n                    dependencies = []\n                for dependency_set in dependencies:\n                    for dependency in dependency_set:\n                        if dependency not in blacklist:\n                            graph[dependency].add(name)\n\n    # Check for conflicts\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            if name not in blacklist:\n                try:\n                    recipe = Recipe.get_recipe(name, ctx)\n                    conflicts = [dep.lower() for dep in recipe.conflicts]\n                except ValueError:\n                    # The recipe does not exist, so we assume it can be\n                    # installed via pip with no extra dependencies\n                    conflicts = []\n                for conflict in conflicts:\n                    if conflict in graph:\n                        if name in graph[conflict]:\n                            raise BuildInterruptingException(\n                                'Conflict found: {} conflicts with {}'.format(\n                                    name, conflict\n                                )\n                            )\n                        if conflict in graph:\n                            if name in graph[conflict]:\n                                raise BuildInterruptingException(\n                                    'Conflict found: {} conflicts with {}'.format(\n                                        name, conflict", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    names = [name.lower() for name in names]\n    names = set(names) - blacklist\n    if bs is not None:\n        names.update(bs.get_bootstrap_dependencies())\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        'python2', ctx, names, orders=[RecipeOrder(ctx)],\n        blacklist=blacklist\n    )\n\n    # Convert each order graph into a linear list:\n    orders = [\n        [name for name in find_order(order)]\n        for order in orders\n    ]\n\n    # Sort the orders based on preference:\n    orders.sort(key=lambda order: [\n        order.index(name)\n        for name in names\n        if name in order\n    ])\n\n    # Get the chosen order:\n    order = orders[0]\n\n    # Get the corresponding recipes:\n    recipes = [Recipe.get_recipe(name, ctx) for name in order]\n\n    # Get the corresponding python modules:\n    python_modules = [recipe.python_dependencies for recipe in recipes]\n\n    # Return the chosen order, recipes, python modules, and bootstrap instance:\n    return order, recipes, python_modules, bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Creating directory {}\".format(dn))\n        makedirs(dn)", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Move {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # Check if we depend on a webview recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"webview\", \"pywebview\"]):\n            return cls.get_bootstrap(\"webview\", ctx)\n\n        # Check if we depend on an sdl2 recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"sdl2\", \"pygame\"]):\n            return cls.get_bootstrap(\"sdl2\", ctx)\n\n        # Check if we depend on a service_only recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"service_only\"]):\n            return cls.get_bootstrap(\"service_only\", ctx)\n\n        # Check if we depend on a kivy recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"kivy\"]):\n            return cls.get_bootstrap(\"kivy\", ctx)\n\n        # Check if we depend on a pygame recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"pygame\"]):\n            return cls.get_bootstrap(\"pygame\", ctx)\n\n        # Check if we depend on a pygame recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"pyjnius\"]):\n            return cls.get_bootstrap(\"pyjnius\", ctx)\n\n        # Check if we depend on a pygame recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"pyqt5\"]):\n            return cls.get_bootstrap(\"pyqt5\", ctx)\n\n        # Check if we depend on a pygame recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"pyqt6\"]):\n            return cls.get_bootstrap(\"pyqt6\", ctx)\n\n        # Check if we depend on a pygame recipe:\n        if any(recipe in ctx.recipe_build_order for recipe in [\"pysdl2\"]):\n            return cls.", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        if not os.path.exists(bootstrap_dir):\n            raise ValueError('No bootstrap named {}'.format(name))\n        module_name = 'pythonforandroid.bootstraps.{name}'.format(name=name)\n        module = importlib.import_module(module_name)\n        bootstrap_class = getattr(module, 'Bootstrap' + name.title())\n        bootstrap = bootstrap_class(ctx)\n        bootstrap.bootstrap_dir = bootstrap_dir\n        return bootstrap\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.\n    # TODO: This function is not documented.\n    # TODO: This function is not used anywhere.\n    # TODO: This function is not tested.", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.local_recipes:\n            recipe_dir = join(self.local_recipes, self.name)\n            if exists(recipe_dir):\n                return recipe_dir\n        return join(self.ctx.recipe_dir, self.name)", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    # Remove the ACLs\n    remove_acl(filepath)\n\n    # Remove the immutable attribute\n    remove_immutable_attribute(filepath)\n\n    # Delete the file or folder\n    if os.path.isfile(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(filepath))", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths\n    if not os.path.isabs(src) or not os.path.isabs(dst):\n        raise ValueError(\"Both source and destination paths must be absolute paths.\")\n\n    # Create the necessary directories in the destination path if they do not exist\n    os.makedirs(os.path.dirname(dst), exist_ok=True)\n\n    # Copy the file or folder to the destination\n    if os.path.isfile(src):\n        shutil.copy(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"The source path must be a file or a folder.\")\n\n    # Set the appropriate file permissions for the copied file or folder\n    if os.path.isfile(dst):\n        os.chmod(dst, stat.S_IRUSR | stat.S_IWUSR)\n    elif os.path.isdir(dst):\n        os.chmod(dst, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)\n        for root, dirs, files in os.walk(dst):\n            for cur_dir in dirs:\n                os.chmod(os.path.join(root, cur_dir), stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)\n            for cur_file in files:\n                os.chmod(os.path.join(root, cur_file), stat.S_IRUSR | stat.S_IWUSR)", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    dropbox_home = None\n    host_db_path = \"~/.dropbox/host.db\"\n    host_db = os.path.expanduser(host_db_path)\n    if os.path.isfile(host_db):\n        con = sqlite3.connect(host_db)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT value \" \"FROM config \" \"WHERE key = 'user';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            dropbox_home = str(data[0])\n            dropbox_home = base64.b64decode(dropbox_home).decode(\"utf-8\")\n            con.close()\n\n    if not dropbox_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\"))\n\n    return dropbox_home", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/CopyAgent/config.db\"\n    yosemite_copy_db_path = (\n        \"Library/Application Support/CopyAgent/user_default/config.db\"\n    )\n    yosemite_copy_db = os.path.join(os.environ[\"HOME\"], yosemite_copy_db_path)\n    if os.path.isfile(yosemite_copy_db):\n        copy_db_path = yosemite_copy_db\n\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = (\n                \"SELECT value \"\n                \"FROM options \"\n                \"WHERE option = 'csmRootPath';\"\n            )\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # Check if the path is a file or a folder\n    if os.path.isfile(path):\n        # Check if the file is a symbolic link\n        if os.path.islink(path):\n            return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            chunk = f.read(1024)\n            if b\"\\0\" in chunk:\n                return False\n\n        # Check if the file is a text file\n        with open(path, \"r\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary file\n        with open(path, \"rb\") as f:\n            try:\n                f.read()\n            except UnicodeDecodeError:\n                return False\n\n        # Check if the file is a binary", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.to_er7()\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n\n        return self.send(SB + message + EB + CR)", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        return self.socket.recv(RECV_BUFFER)", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        if self.minutes < 0:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        return \"{0}{1:02d}{2:02d}\".format(\n            sign, abs(self.minutes) // 60, abs(self.minutes) % 60\n        )", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM format\")\n\n    groups = match.groups()\n    if groups[1] and groups[2]:\n        tz = _UTCOffset(int(groups[1]) * 60 + int(groups[2]))\n    else:\n        tz = None\n\n    if len(groups[0]) == 4:\n        return datetime.datetime(int(groups[0]), 1, 1, tzinfo=tz)\n    elif len(groups[0]) == 6:\n        return datetime.datetime(\n            int(groups[0][:4]), int(groups[0][4:]), 1, tzinfo=tz\n        )\n    elif len(groups[0]) == 8:\n        return datetime.datetime(\n            int(groups[0][:4]),\n            int(groups[0][4:6]),\n            int(groups[0][6:]),\n            tzinfo=tz,\n        )\n    elif len(groups[0]) == 10:\n        return datetime.datetime(\n            int(groups[0][:4]),\n            int(groups[0][4:6]),\n            int(groups[0][6:8]),\n            int(groups[0][8:]),\n            tzinfo=tz,\n        )\n    elif len(groups[0]) == 12:\n        return datetime.datetime(\n            int(groups[0][:4]),\n            int(groups[0][4:6]),\n            int(groups[0][6:8]),\n            int(groups[0][8:10]),\n            int(groups[0][10:]),\n            tzinfo=tz,\n        )\n    elif len(groups[0]) == 14:\n        return datetime.datetime(\n            int(groups[0][:4]),\n            int(groups[0][4:6]),\n            int(groups[0][6:8]),\n            int(groups[0][", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](\n            sequence=data, esc=self.esc, separators=self.separators, factory=self.factory\n        )", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) < 4 or VERSION[3] == \"final\":\n        return \".\".join(map(str, VERSION[:3]))\n    else:\n        return \".\".join(map(str, VERSION[:3])) + VERSION[3]", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            return None\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        conf = cls(file, cfg)\n\n        if not conf.check_config_sanity():\n            return None\n\n        return conf\n\n    ", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        cfgfile = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(cfgfile)\n\n    ", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        if not os.path.exists(os.path.dirname(cfgfile)):\n            os.makedirs(os.path.dirname(cfgfile))\n\n        config_parser = configparser.ConfigParser()\n        config_parser.add_section(\"twtxt\")\n        config_parser.set(\"twtxt\", \"nick\", nick)\n        config_parser.set(\"twtxt\", \"twtfile\", twtfile)\n        config_parser.set(\"twtxt\", \"twturl\", twturl)\n        config_parser.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n\n        if add_news:\n            config_parser.add_section(\"following\")\n            config_parser.set(\"following\", \"twtxt.net\", \"https://twtxt.net/twtxt.txt\")\n\n        with open(cfgfile, \"w\") as config_file:\n            config_parser.write(config_file)\n\n        return cls.from_file(cfgfile)\n\n    ", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if self.cfg.has_section(\"following\"):\n            for (nick, url) in self.cfg.items(\"following\"):\n                following.append(Source(nick, url))\n        else:\n            logger.debug(\"No sources in config.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError:\n            return {}\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        delta = now - self.created_at\n        tense = \"ago\"\n        if delta.days < 0:\n            tense = \"from now\"\n        return humanize.naturaltime(delta, future=True)\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        name = match.group(1)\n        url = match.group(2)\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    from twtxt.models import Tweet\n    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweets.append(parse_tweet(raw_tweet, source, now))\n        except (ValueError, OverflowError) as e:\n            logger.warning(e)\n    return tweets", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            wiki=self,\n            title=title,\n            ns=namespace2int(ns),\n            language=self.language,\n        )", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        res = f\"{self.title} (level: {self.level})\"\n        if self.text:\n            res += f\"\\n{self.text}\"\n        if self.sections:\n            res += f\"\\n{len(self.sections)} sections\"\n        for sec in self.sections:\n            res += f\"\\n{sec}\"\n        return res", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return None\n        return sections[-1]\n\n    ", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections\n        return []\n\n    ", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return \"{title} (id: {pageid}, ns: {ns})\".format(\n                title=self.title,\n                pageid=self.pageid,\n                ns=self.namespace,\n            )\n        return \"{title} (id: ??, ns: {ns})\".format(\n            title=self.title,\n            ns=self.namespace,\n        )", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"STARTTLS command illegal in state %s\" % self._imap.state\n            )\n\n        if self._starttls_done:\n            raise exceptions.AlreadyEnabledError(\"STARTTLS already enabled\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._imap.starttls(ssl_context)\n        self._starttls_done = True\n        self._cached_capabilities = None\n        return self._do_capabilites()\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if not self._imap.has_capability(\"ENABLE\"):\n            raise exceptions.CapabilityError(\"Server does not support ENABLE\")\n\n        if not capabilities:\n            raise ValueError(\"At least one capability must be specified\")\n\n        # Convert capabilities to bytes\n        capabilities = [to_bytes(capability) for capability in capabilities]\n\n        # Send the ENABLE command\n        typ, data = self._imap._simple_command(\"ENABLE\", *capabilities)\n        self._checkok(\"enable\", typ, data)\n\n        # Parse the response and return the enabled capabilities\n        enabled_capabilities = []\n        for capability in data:\n            if capability.startswith(b\"ENABLED\"):\n                enabled_capabilities.append(capability.split()[1])\n\n        return enabled_capabilities\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [f for f in folder_data if f is not None and f != b\"\"]\n        if not folder_data:\n            return []\n\n        # The first item is the response code, the second is the\n        # folder name, the third is the delimiter, and the rest are\n        # flags.\n        #\n        # The folder name may be an integer, in which case it is\n        # converted back to a string.\n        #\n        # If folder encoding is enabled, the folder name is decoded\n        # using UTF-7.\n        #\n        # The flags are converted to bytes.\n        #\n        # The delimiter is converted to bytes.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples, each tuple containing\n        # the flags, delimiter, and name of a folder.\n        #\n        # The return value is a list of tuples", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if readonly:\n            typ, data = self._imap._simple_command(\"EXAMINE\", folder)\n        else:\n            typ, data = self._imap._simple_command(\"SELECT\", folder)\n        self._checkok(\"select\", typ, data)\n        return self._process_select_response(data)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        return self._command_and_check(\"unselect\", unpack=True)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"noop\", unpack=True)\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._idle_tag = self._imap._new_tag()\n        logger.debug(\"< IDLE\")\n        self._imap.send(b\"IDLE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"Not in IDLE mode\")\n\n        # Wait for the socket to be ready to read\n        if self._poll_socket(self.socket(), timeout):\n            # Consume the IDLE response\n            return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n        # If the socket is not ready, return an empty list\n        return None\n", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"idle() must be called first\")\n\n        self._imap._command_complete(\"IDLE\", self._idle_tag)\n        self._idle_tag = None\n        return self._consume_until_tagged_response(b\"DONE\", \"IDLE\")\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = [\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\"]\n        folder = self._normalise_folder(folder)\n        typ, data = self._imap._simple_command(\"STATUS\", folder, seq_to_parenstr(what))\n        self._checkok(\"status\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"STATUS\")\n        return parse_response(data)\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        if not isinstance(sort_criteria, list):\n            sort_criteria = [sort_criteria]\n        args = [to_bytes(c) for c in sort_criteria] + _normalise_search_criteria(criteria, charset)\n        data = self._raw_command_untagged(b\"SORT\", args)\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        if not isinstance(algorithm, str):\n            algorithm = \" \".join(algorithm)\n        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return parse_thread_response(ids)\n", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not messages:\n            return {}\n\n        args = [\"FETCH\", join_message_ids(messages), \"(FLAGS)\"]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n        if not isinstance(msg, str):\n            raise TypeError(\"msg must be a string\")\n        if not isinstance(flags, (list, tuple)):\n            raise TypeError(\"flags must be a list or tuple\")\n        if not isinstance(msg_time, (datetime, type(None))):\n            raise TypeError(\"msg_time must be a datetime or None\")\n\n        if msg_time is None:\n            msg_time = datetime.now()\n\n        if self.use_uid:\n            raise ValueError(\"cannot APPEND by ID when not using uids\")\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_times:\n            msg_time = msg_time.astimezone(timezone.utc)\n\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        if self.normalise_", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return {}\n\n        if not isinstance(msgs, list):\n            msgs = list(msgs)\n\n        if not all(isinstance(msg, dict) for msg in msgs):\n            raise ValueError(\"All messages must be dictionaries\")\n\n        if not all(\n            all(key in msg for key in (\"msg\", \"flags\", \"date\")) for msg in msgs\n        ):\n            raise ValueError(\"All messages must contain 'msg', 'flags', and 'date' keys\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"flags\"], list) for msg in msgs):\n            raise ValueError(\"All flags must be lists\")\n\n        if not all(isinstance(msg[\"date\"], datetime) for msg in msgs):\n            raise ValueError(\"All dates must be datetime objects\")\n\n        if not all(isinstance(flag, str) for msg in msgs for flag in msg[\"flags\"]):\n            raise ValueError(\"All flags must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages must be strings\")\n\n        if not all(isinstance(msg[\"msg\"], str) for msg in msgs):\n            raise ValueError(\"All messages", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            return self._command_and_check(\"expunge\", unpack=True)\n        else:\n            return self._command_and_check(\n                \"store\",\n                join_message_ids(messages),\n                \"+FLAGS\",\n                \"(\\\\Deleted)\",\n                uid=True,\n                unpack=True,\n            )\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder))\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        if not isinstance(who, str):\n            raise TypeError(\"who must be a string\")\n        if not isinstance(what, str):\n            raise TypeError(\"what must be a string\")\n        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n\n        if what:\n            what = \"+\" + what\n\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        data = self._command_and_check(\"getquota\", _quote(mailbox))\n        parts = list(response_lexer.TokenSource(data))\n        quota_root = parts[0]\n        quotas = _parse_quota(parts[1])\n        return MailboxQuotaRoots(quota_root), quotas\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        if not isinstance(quotas, list):\n            raise ValueError(\"quotas must be a list of Quota objects\")\n        if not all(isinstance(quota, Quota) for quota in quotas):\n            raise ValueError(\"quotas must be a list of Quota objects\")\n        args = []\n        for quota in quotas:\n            args.append(quota.resource)\n            args.append(quota.usage)\n            args.append(quota.limit)\n        return self._command_and_check(\"setquota\", *args, unpack=True)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == \"OK\":\n                return data, untagged\n            elif typ == \"NO\":\n                raise exceptions.IMAPClientError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == \"BAD\":\n                raise exceptions.IMAPClientError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == \"BYE\":\n                raise exceptions.IMAPClientAbortError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == \"PREAUTH\":\n                raise exceptions.IMAPClientAbortError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == \"CAPABILITY\":\n                self._cached_capabilities = self._normalise_capabilites(data[0])\n            elif typ == \"RESP-CODE\":\n                # TODO: handle this\n                pass\n            elif typ == \"BYTES\":\n                # TODO: handle this\n                pass\n            elif typ == \"UID\":\n                # TODO: handle this\n                pass\n            elif typ == \"UNSEEN\":\n                # TODO: handle this\n                pass\n            elif typ == \"PERMANENTFLAGS\":\n                # TODO: handle this\n                pass\n            elif typ == \"FLAGS\":\n                # TODO: handle this\n                pass\n            elif typ == \"OK\":\n                # TODO: handle this\n                pass\n            elif typ == \"LIST\":\n                # TODO: handle this\n                pass\n            elif typ == \"LSUB\":\n                # TODO: handle this\n                pass\n            elif typ == \"SEARCH\":\n                # TODO: handle this\n                pass\n            elif typ == \"STATUS\":\n                # TODO: handle this\n                pass\n            elif typ == \"EXISTS\":\n                # TODO: handle this\n                pass\n            elif typ == \"RECENT\":\n                #", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    elif isinstance(criteria, int):\n        criteria = [str(criteria)]\n    elif isinstance(criteria, (list, tuple)):\n        criteria = list(criteria)\n    elif isinstance(criteria, datetime.datetime):\n        criteria = [datetime_to_INTERNALDATE(criteria)]\n    elif isinstance(criteria, datetime.date):\n        criteria = [datetime_to_INTERNALDATE(datetime.datetime.combine(criteria, datetime.time()))]\n    elif criteria is None:\n        raise ValueError(\"No criteria specified\")\n    if charset is None:\n        charset = \"us-ascii\"\n    return [_quote_criteria(c, charset) for c in criteria]\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.src is not None:\n            return self.lex.current_source.literal\n        return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = []\n    b64_buffer = bytearray()\n\n    def consume_b64_buffer(buf: bytearray) -> None:\n        if buf:\n            res.append(base64_utf7_decode(buf))\n            del buf[:]\n\n    for c in s:\n        if isinstance(c, str):\n            c = ord(c)\n        if c == AMPERSAND_ORD:\n            consume_b64_buffer(b64_buffer)\n        elif c == DASH_ORD:\n            consume_b64_buffer(b64_buffer)\n            res.append(\"&\")\n        elif 0x20 <= c <= 0x7E:\n            consume_b64_buffer(b64_buffer)\n            res.append(chr(c))\n        else:\n            b64_buffer.append(c)\n\n    consume_b64_buffer(b64_buffer)\n\n    return \"\".join(res)", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        offset = time.timezone\n        if time.localtime().tm_isdst and time.daylight:\n            offset = time.altzone\n        return cls(offset / 60)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    if timestamp is None:\n        return None\n    if isinstance(timestamp, datetime):\n        return timestamp\n    if isinstance(timestamp, bytes):\n        timestamp = _munge(timestamp)\n    dt = datetime(*parsedate_tz(timestamp)[:6])\n    if normalise:\n        dt = datetime_to_native(dt)\n    return dt", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if isinstance(dt, datetime):\n        return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")\n    else:\n        return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise AssertionError(\n            \"Server replied with a response that violates the IMAP protocol\"\n            if message is None\n            else f\"Server replied with a response that violates the IMAP protocol: {message}\"\n        )", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id:\n        config_path = get_base_path() / 'profiles' / profile / module_id / 'config.' + ext\n    else:\n        config_path = get_base_path() / 'profiles' / profile / 'config.' + ext\n    if not config_path.parent.exists():\n        config_path.parent.mkdir(parents=True)\n    return config_path", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / 'modules'\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members)", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        if isinstance(entity, types.MessageEntityBold):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityItalic):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityCode):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityPre):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityTextUrl):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityMention):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityMentionName):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityPhone):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityCashtag):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityUnderline):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityStrike):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityBlockquote):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityBankCard):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntitySpoiler):\n            result.append(text[entity.offset:entity.", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    return parser.text, parser.entities", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return text\n    if not entities:\n        return escape(text)\n\n    text = add_surrogate(text)\n    entities = sorted(entities, key=lambda e: e.offset)\n    result = StringIO()\n    for index, entity in enumerate(entities):\n        start = entity.offset\n        end = start + entity.length\n        before = takewhile(start, entities, index)\n        after = takewhile(end, entities, index)\n        if before:\n            result.write(escape(text[index:before]))\n\n        formatter = ENTITY_TO_FORMATTER.get(type(entity), None)\n        if formatter:\n            if isinstance(formatter, tuple):\n                start_tag, end_tag = formatter\n                result.write(start_tag)\n                result.write(escape(text[start:end]))\n                result.write(end_tag)\n            else:\n                result.write(formatter(entity, text[start:end]))\n        else:\n            result.write(escape(text[start:end]))\n\n        index = after\n    result.write(escape(text[index:]))\n    return del_surrogate(result.getvalue())\n\n", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n\n    key, old = _server_keys[fingerprint]\n    if old:\n        return rsa.core.encrypt_int(data, key.e, key.n)\n\n    # https://core.telegram.org/mtproto/auth_key#dh-exchange-initiation\n    # 4.1. Encryption\n    # ...\n    # 4.1.1. Old version\n    # ...\n    # 4.1.2. New version\n    # ...\n    # 4.1.3. Data Padding\n    # ...\n    # 4.1.4. Encryption\n    # ...\n    # 4.1.5. Encryption using OAEP\n    # ...\n    # 4.1.6. Encryption using RSA\n    # ...\n    # 4.1.7. Encryption using AES-256-CBC\n    # ...\n    # 4.1.8. Encryption using AES-256-IGE\n    # ...\n    # 4.1.9. Encryption using AES-256-IGE with padding\n    # ...\n    # 4.1.10. Encryption using AES-256-IGE with padding and random key\n    # ...\n    # 4.1.11. Encryption using AES-256-IGE with padding and random key and IV\n    # ...\n    # 4.1.12. Encryption using AES-256-IGE with padding and random key and IV and random salt\n    # ...\n    # 4.1.13. Encryption using AES-256-IGE with padding and random key and IV and random salt and random padding\n    # ...\n    # 4.1.14. Encryption using AES-256-IGE with padding and random key and IV and random salt and random padding and random padding\n    # ...\n    # 4.1.15. Encryption using AES-256-IG", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    str_bytes = string.encode('utf-8')\n    str_length = len(str_bytes)\n    return int_to_bytes(str_length, 2) + str_bytes", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        elif ns is False or self.__ns is None:\n            ns = None\n        if ns:\n            ##log.debug('Adding child %s with ns %s', name, ns)\n            element = self.__document.createElementNS(ns, name)\n        else:\n            ##log.debug('Adding child %s', name)\n            element = self.__document.createElement(name)\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(text)\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if filename:\n            self.__document.writexml(open(filename, 'w'), addindent=\"  \", newl=\"\\n\", encoding=\"utf-8\")\n        else:\n            if pretty:\n                return self.__document.toprettyxml(indent=\"  \", newl=\"\\n\", encoding=\"utf-8\")\n            else:\n                return self.__document.toxml(encoding=\"utf-8\")", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except (TypeError, ValueError):\n        return s", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n            .replace(microsecond=0)\n        )\n    except (TypeError, ValueError):\n        return s", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    if m is None:\n        return None\n    return {prefix + k: v for k, v in m.items()}", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message, voice, loop, language, **kwargs))\n", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(\n            Message(\n                body=body,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n\n    ", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML) and not isinstance(verb, str):\n            raise TwiMLException(\"Only appending of TwiML and strings are allowed\")\n\n        self.verbs.append(verb)\n        return self", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise ValueError(\"No signing key configured for JWT.\")\n\n        headers = self.headers.copy()\n        payload = self.payload.copy()\n\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(\n            payload,\n            self.secret_key,\n            algorithm=self.algorithm,\n            headers=headers,\n        )", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope_uri = ScopeURI(\"client\", \"outgoing\", {\"appSid\": application_sid})\n        if kwargs:\n            scope_uri.add_param(\"appParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"outgoing\"] = scope_uri", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", {\"path\": \"/2010-04-01/Events\"})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        scope_uris = [capability.to_payload() for capability in self.capabilities.values()]\n        return {\"scope\": \" \".join(scope_uris)}", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items())\n            encoded_params = urlencode(sorted_params)\n            param_string = \"?{}\".format(encoded_params)\n        else:\n            param_string = \"\"\n\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_string)\n\n    ", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True)\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # Check if the color string is in the format '#xxxxxx'\n    if match(r\"^#[0-9a-f]{6}$\", color, flags=ASCII):\n        # Convert the color string to lowercase\n        color = color.lower()\n\n    # Check if the color string is in the format '#xxx'\n    elif match(r\"^#[0-9a-f]{3}$\", color, flags=ASCII):\n        # Convert the color string to lowercase\n        color = color.lower()\n\n        # Convert the color string to the format '#xxx'\n        color = \"#\" + color[1] + color[1] + color[2] + color[2] + color[3] + color[3]\n\n    # Return the color string in the format '#xxx'\n    return color\n\n", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACKTICKS\n    max_fence_length = max(\n        [len(match) for match in findall(REGEX_CONTINUOUS_BACKTICKS, content)]\n    ) + 1\n    return \"`\" * max_fence_length\n\n", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    with suppress_output():\n        process = subprocess.run(command)\n    if process.returncode != 0:\n        controller.report_error(\n            [\n                \"Failed to open media file. Please check if the tool is installed and in PATH.\"\n            ]\n        )\n\n", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    encoded_stream_name = hash_util_encode(stream_name.replace(\" \", \"-\"))\n    return str(stream_id) + \"-\" + encoded_stream_name", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        recipients = [\n            recipient.strip()\n            for recipient in write_box.edit_text.split(\",\")\n            if recipient.strip()  # This condition avoids whitespace recipients (\",  ,\")\n        ]\n\n        recipient_user_ids = list()\n        for recipient in recipients:\n            cleaned_recipient_list = re.findall(REGEX_CLEANED_RECIPIENT, recipient)\n            recipient_name, recipient_email, invalid_text = cleaned_recipient_list[0]\n            # Discard invalid_text as part of tidying up the recipient.\n\n            if recipient_email and self.model.is_valid_private_recipient(\n                recipient_email, recipient_name\n            ):\n                recipient_user_ids.append(\n                    self.model.user_id_from_email(recipient_email)\n                )\n\n        self.recipient_user_ids = recipient_user_ids\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add an edit mode button to the header write box\n        self.edit_mode_button = EditModeButton(self.model)\n        self.header_write_box.contents.insert(\n            self.FOCUS_HEADER_PREFIX_STREAM,\n            (self.edit_mode_button, self.header_write_box.options()),\n        )\n\n        # Set the style of the stream write box using a callback\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Set the focus to the title write box\n        self.focus_position = self.FOCUS_CONTAINER_TITLE\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        if self.model.is_private_stream(stream_id):\n            self.stream_write_box.set_attr_map(\n                {\n                    None: \"stream_private\",\n                }\n            )\n            self.stream_write_box.set_text(\n                (\n                    \"stream_private\",\n                    f\"{caption} {STREAM_PRIVATE_MARKER}\",\n                )\n            )\n        else:\n            self.stream_write_box.set_attr_map(\n                {\n                    None: \"stream_public\",\n                }\n            )\n            self.stream_write_box.set_text(\n                (\n                    \"stream_public\",\n                    f\"{caption} {STREAM_PUBLIC_MARKER}\",\n                )\n            )\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users_list = self.view.users\n        matching_users = [\n            user for user in users_list if match_user(user, text[len(prefix_string) :])\n        ]\n        matching_ids = set([user[\"user_id\"] for user in matching_users])\n        matching_recipient_ids = set(self.recipient_user_ids) & set(matching_ids)\n        # Display subscribed users/recipients first.\n        sorted_matching_users = sorted(\n            matching_users,\n            key=lambda user: user[\"user_id\"] in matching_recipient_ids,\n            reverse=True,\n        )\n\n        user_names = [user[\"full_name\"] for user in sorted_matching_users]\n\n        # Counter holds a count of each name in the list of users' names in a\n        # dict-like manner, which is a more efficient approach when compared to\n        # slicing the original list on each name.\n        # FIXME: Use a persistent counter rather than generate one on each autocomplete.\n        user_names_counter = Counter(user_names)\n\n        # Append user_id's to users with the same names.\n        user_names_with_distinct_duplicates = [\n            f\"{user['full_name']}|{user['user_id']}\"\n            if user_names_counter[user[\"full_name\"]] > 1\n            else user[\"full_name\"]\n            for user in sorted_matching_users\n        ]\n\n        extra_prefix = \"{}{}\".format(\n            \"*\" if prefix_string[-1] != \"*\" else \"\",\n            \"*\" if prefix_string[-2:] != \"**\" else \"\",\n        )\n        user_typeahead = format_string(\n            user_names_with_distinct_duplicates, prefix_string + extra_prefix + \"{}**\"\n        )\n\n        return user_typeahead\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topic_names = self.model.topics_in_stream(self.stream_id)\n        topic_suggestions = match_topics(topic_names, text[len(prefix_string) :])\n\n        topic_typeaheads = format_string(topic_suggestions, prefix_string + \"{}**\")\n\n        return topic_typeaheads, topic_suggestions", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.streams\n        matching_streams = [\n            stream for stream in stream_names if match_stream_name(stream, text)\n        ]\n        stream_typeahead = format_string(matching_streams, \"#**{}**\")\n        return self._process_typeaheads(stream_typeahead, state, matching_streams)", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        prefix_string = text[:-1]\n        prefix_length = len(prefix_string)\n        prefix = text[prefix_length]\n\n        if prefix == \"@\":\n            return self.autocomplete_mentions(text, prefix_string)[0]\n        elif prefix == \"#\":\n            return self.autocomplete_streams(text, prefix_string)[0]\n        elif prefix == \":\":\n            return self.autocomplete_emojis(text, prefix_string)[0]\n        elif prefix == \"*\":\n            return self.autocomplete_stream_and_topic(text, prefix_string)[0]\n        else:\n            return None", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.get_edit_text() == \"\":\n            return ch.isprintable() and not ch.isspace()\n        else:\n            return super().valid_char(ch)\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.is_topic_narrow:\n        return False\n    if model.is_user_muted_topic(msg[\"stream_id\"], msg[\"subject\"]):\n        return True\n    if model.is_user_muted_stream(msg[\"stream_id\"]):\n        return True\n    return False", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n        self.update_widget(self.get_count_text(count), text_color)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if isinstance(self.prefix_character, tuple):\n            prefix_character, prefix_character_style = self.prefix_character\n            self.button_prefix.set_text((prefix_character_style, prefix_character))\n        else:\n            self.button_prefix.set_text(self.prefix_character)\n\n        self._label.set_text(self._caption)\n        self.button_suffix.set_text(count_text)\n        self._w.set_attr_map({None: text_color})\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        else:\n            return super().keypress(size, key)\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        parsed_link: ParsedNarrowLink = {}\n        parsed_url = urlparse(link)\n        fragment = parsed_url.fragment\n        if fragment.startswith(\"narrow/stream/\"):\n            # Remove the 'narrow/stream/' prefix.\n            fragment = fragment[14:]\n            # Split the fragment into parts.\n            parts = fragment.split(\"/\")\n            # Decode the stream data.\n            parsed_link[\"stream\"] = cls._decode_stream_data(parts[0])\n            # Decode the message ID if present.\n            if len(parts) > 2 and parts[1] == \"near\":\n                parsed_link[\"message_id\"] = cls._decode_message_id(parts[2])\n            # Decode the topic name if present.\n            if len(parts) > 3 and parts[2] == \"topic\":\n                parsed_link[\"topic_name\"] = hash_util_decode(parts[3])\n                # Decode the message ID if present.\n                if len(parts) > 4 and parts[4] == \"near\":\n                    parsed_link[\"message_id\"] = cls._decode_message_id(parts[5])\n            # Set the narrow type.\n            if \"topic_name\" in parsed_link:\n                if \"message_id\" in parsed_link:\n                    parsed_link[\"narrow\"] = \"stream:topic:near\"\n                else:\n                    parsed_link[\"narrow\"] = \"stream:topic\"\n            elif \"message_id\" in parsed_link:\n                parsed_link[\"narrow\"] = \"stream:near\"\n            else:\n                parsed_link[\"narrow\"] = \"stream\"\n        return parsed_link\n\n    ", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n\n        # Check if stream ID is valid.\n        if stream_id is not None:\n            if stream_id not in self.model.stream_dict:\n                return \"Invalid stream ID\"\n\n        # Check if stream name is valid.\n        if stream_name is not None:\n            if stream_name not in self.model.stream_dict_by_name:\n                return \"Invalid stream name\"\n\n        # Check if stream is subscribed to by the user.\n        if stream_id is not None:\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"You are not subscribed to this stream\"\n\n        # Check if stream name is valid.\n        if stream_name is not None:\n            if not self.model.is_user_subscribed_to_stream(stream_name):\n                return \"You are not subscribed to this stream\"\n\n        # Patch stream ID or name if necessary.\n        if stream_id is None:\n            stream_id = self.model.stream_dict_by_name[stream_name][\"id\"]\n            parsed_link[\"stream\"][\"stream_id\"] = stream_id\n        elif stream_name is None:\n            stream_name = self.model.stream_dict[stream_id][\"name\"]\n            parsed_link[\"stream\"][\"stream_name\"] = stream_name\n\n        return \"\"", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        model = self.model\n        if \"stream\" == parsed_link[\"narrow\"]:\n            if not model.is_user_subscribed_to_stream(parsed_link[\"stream\"][\"stream_id\"]):\n                return \"The stream seems to be unsubscribed\"\n        elif \"stream:near\" == parsed_link[\"narrow\"]:\n            if not model.is_user_subscribed_to_stream(parsed_link[\"stream\"][\"stream_id\"]):\n                return \"The stream seems to be unsubscribed\"\n            if not model.is_valid_message(parsed_link[\"message_id\"]):\n                return \"The message id seems to be invalid\"\n        elif \"stream:topic\" == parsed_link[\"narrow\"]:\n            if not model.is_user_subscribed_to_stream(parsed_link[\"stream\"][\"stream_id\"]):\n                return \"The stream seems to be unsubscribed\"\n            if not model.is_valid_topic(\n                parsed_link[\"stream\"][\"stream_id\"], parsed_link[\"topic_name\"]\n            ):\n                return \"The topic seems to be invalid\"\n        elif \"stream:topic:near\" == parsed_link[\"narrow\"]:\n            if not model.is_user_subscribed_to_stream(parsed_link[\"stream\"][\"stream_id\"]):\n                return \"The stream seems to be unsubscribed\"\n            if not model.is_valid_topic(\n                parsed_link[\"stream\"][\"stream_id\"], parsed_link[\"topic_name\"]\n            ):\n                return \"The topic seems to be invalid\"\n            if not model.is_valid_message(parsed_link[\"message_id\"]):\n                return \"The message id seems to be invalid\"\n        return \"\"", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        if parsed_link[\"narrow\"] == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            self.controller.narrow_to_topic(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            self.controller.narrow_to_near(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            self.controller.narrow_to_topic_near(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n    for theme_name, theme in THEMES.items():\n        try:\n            theme_styles = theme.STYLES\n            theme_meta = theme.META\n            validate_theme(theme_name, theme_styles, theme_meta)\n            complete_themes.append(theme_name)\n        except (AttributeError, KeyError):\n            incomplete_themes.append(theme_name)\n    return sorted(complete_themes), sorted(incomplete_themes)\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    if color_depth != 16:\n        return\n\n    invalid_colors = []\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = fg.value.split()\n        bg_code16, bg_code256, bg_code24, *bg_props = bg.value.split()\n\n        if fg_code16 not in valid_16_color_codes:\n            invalid_colors.append(fg_code16)\n        if bg_code16 not in valid_16_color_codes:\n            invalid_colors.append(bg_code16)\n\n    if invalid_colors:\n        raise InvalidThemeColorCode(\n            f\"Invalid color codes in theme '{theme_name}': {', '.join(invalid_colors)}\"\n        )", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style, (fg, bg) in theme_styles.items():\n        if color_depth == 1:\n            fg = \"default\"\n            bg = \"default\"\n        elif color_depth == 16:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 256:\n            fg = fg.split()[1]\n            bg = bg.split()[1]\n        elif color_depth == 2 ** 24:\n            fg = fg.split()[2]\n            bg = bg.split()[2]\n        else:\n            raise ValueError(f\"Invalid color depth: {color_depth}\")\n\n        if style is None:\n            style = \"\"\n        else:\n            style = f\"{style}:\"\n\n        urwid_theme.append((f\"{style}fg\", fg))\n        urwid_theme.append((f\"{style}bg\", bg))\n\n    return urwid_theme", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    background = theme_meta[\"pygments\"][\"background\"]\n    overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    # Add background color to Pygments styles\n    pygments_styles[\"background_color\"] = background.value\n\n    # Add overrides to Pygments styles\n    pygments_styles.update(overrides)\n\n    # Add Pygments styles to Urwid theme\n    for style_name, style_value in pygments_styles.items():\n        if style_name in STANDARD_TYPES:\n            urwid_theme.append((style_name, style_value, style_value))", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        binding\n        for binding in KEY_BINDINGS.values()\n        if not binding.get(\"excluded_from_random_tips\", False)\n    ]", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        if data is None:\n            return self.xform_data\n        else:\n            return self.reduce['model'].transform(data, **self.reduce['params'])\n", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        if data is None:\n            data = self.xform_data\n        else:\n            data = self.transform(data)\n        return plot(data, **kwargs)", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpaper.load_yaml(path)\n    return topic2papers", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.utils import BibAbbreviations\n\n    return BibAbbreviations(get_bib_abbrv_file())", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n    return gettext.translation(domain, localedir, languages=languages)", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # remove all comments from the string\n    text = sqlparse.format(sql, strip_comments=True).strip()\n    # check if the string contains an open comment\n    if '/*' in text:\n        return False\n    # check if the string ends with a semicolon\n    return text.endswith(';') or text.endswith('\\\\g') or text.endswith('\\\\G')", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    return upload_payload(payload, service_endpoint_uri, separate_process)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(\n            target=self._listen_for_request, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(\n            target=self._listen_for_response, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n\n        logger.info('Started Json rpc client.')", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None:\n            raise ValueError(u'Method cannot be None.')\n\n        if params is None:\n            raise ValueError(u'Params cannot be None.')\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        # Check if the response is in the response map.\n        if request_id in self.response_map:\n            if not self.response_map[request_id].empty():\n                return self.response_map[request_id].get()\n\n        # Check if the response is in the event map.\n        if 0 in self.response_map:\n            if not self.response_map[0].empty():\n                return self.response_map[0].get()\n\n        # Check if the response is in the exception map.\n        if not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        return None", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug(u'Json Rpc client shutdown.')", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {u'jsonrpc': u'2.0', u'method': method, u'params': params}\n        if request_id:\n            content[u'id'] = request_id\n\n        content_json = json.dumps(content)\n        content_bytes = content_json.encode(self.encoding)\n        content_length = len(content_bytes)\n        header = self.HEADER.format(content_length)\n        header_bytes = header.encode(self.encoding)\n\n        try:\n            self.stream.write(header_bytes)\n            self.stream.write(content_bytes)\n        except ValueError as ex:\n            # Stream was closed.\n            logger.debug(u'JSON RPC Writer on send_request() encountered exception: %s', ex)\n            raise", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        content = [None]\n        while self.needs_more_data:\n            self.needs_more_data = False\n            if self.read_state == ReadState.Header:\n                self.read_next_chunk()\n                self.try_read_headers()\n            elif self.read_state == ReadState.Content:\n                self.read_next_chunk()\n                self.try_read_content(content)\n\n        # Trim the buffer.\n        self.trim_buffer_and_resize(self.read_offset)\n\n        # Reset state.\n        self.read_state = ReadState.Header\n        self.headers = {}\n        self.expected_content_length = 0\n\n        # Parse the content.\n        try:\n            return json.loads(content[0])\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_response encountered exception: %s', ex)\n            raise\n\n    ", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # Check if we need to resize the buffer.\n        if self.buffer_end_offset >= len(self.buffer):\n            self.buffer.extend(bytearray(self.DEFAULT_BUFFER_SIZE))\n\n        # Read data from the stream into the buffer.\n        bytes_read = self.stream.readinto(self.buffer[self.buffer_end_offset:])\n\n        # Update the buffer end offset.\n        self.buffer_end_offset += bytes_read\n\n        # If the stream is empty or closed externally, raise an exception.\n        if bytes_read == 0:\n            raise ValueError(u'Stream is empty or closed externally.')\n\n        return True", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Scan for the end of the header.\n        header_end_index = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset)\n        if header_end_index == -1:\n            # We did not find the end of the header.\n            return False\n\n        # We found the end of the header.\n        header_end_index += 4\n        # Get the header string.\n        header_string = self.buffer[self.read_offset:header_end_index].decode(\n            self.encoding)\n        # Split the header string by new line.\n        header_lines = header_string.split('\\r\\n')\n        # Reset the headers dictionary.\n        self.headers = {}\n        # Iterate over the header lines.\n        for header_line in header_lines:\n            # Split the header line by colon.\n            header_key_value = header_line.split(':')\n            # If the header line is not a key-value pair, skip it.\n            if len(header_key_value) != 2:\n                continue\n            # Strip the key and value.\n            header_key = header_key_value[0].strip()\n            header_value = header_key_value[1].strip()\n            # Add the key-value pair to the headers dictionary.\n            self.headers[header_key] = header_value\n\n        # Check if the 'content-length' header is present.\n        if 'content-length' in self.headers:\n            # Store the value in the expected content length.\n            self.expected_content_length = int(self.headers['content-length'])\n\n        # Update the read offset.\n        self.read_offset = header_end_index\n        # Set the read state to content.\n        self.read_state = ReadState.Content\n\n        return True", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        self.update_keywords(text)\n        self.update_names(text)\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return (Path(),)\n\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.parsed is None:\n        return (Keyword(),)\n\n    if stmt.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(text_before_cursor)\n\n    return suggest_based_on_last_token(stmt.last_token, stmt)", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)[0]\n    tokens = parsed.tokens\n\n    # Find the first meaningful token\n    idx, tok = parsed.token_next(0, skip_ws=True, skip_cm=True)\n    if tok and tok.ttype is Keyword and tok.value.upper() == 'WITH':\n        # This is a CTE query\n        ctes = []\n        while True:\n            # Find the next CTE\n            idx, tok = parsed.token_next_by(idx, (CTE,))\n            if not tok:\n                break\n\n            cte = get_cte_from_token(tok, token_start_pos(tokens, idx))\n            if cte:\n                ctes.append(cte)\n\n        return ctes, sql[ctes[-1].stop:]\n\n    return [], sql", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    stream = extract_from_part(parsed)\n    return tuple(extract_table_identifiers(stream))", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        return body", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key, value in resp.items():\n            if key in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[key], value)", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = headers.get(X_GOOG_MESSAGE_NUMBER)\n    if message_number is None:\n        raise InvalidNotificationError(\"Missing message number\")\n    try:\n        message_number = int(message_number)\n    except ValueError:\n        raise InvalidNotificationError(\"Invalid message number\")\n\n    state = headers.get(X_GOOG_RESOURCE_STATE)\n    if state is None:\n        raise InvalidNotificationError(\"Missing resource state\")\n\n    resource_uri = headers.get(X_GOOG_RESOURCE_URI)\n    if resource_uri is None:\n        raise InvalidNotificationError(\"Missing resource URI\")\n\n    resource_id = headers.get(X_GOOG_RESOURCE_ID)\n    if resource_id is None:\n        raise InvalidNotificationError(\"Missing resource ID\")\n\n    if headers.get(X_GOOG_CHANNEL_ID) != channel.id:\n        raise InvalidNotificationError(\"Invalid channel ID\")\n\n    return Notification(message_number, state, resource_uri, resource_id)", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if token is None:\n        token = uuid.uuid4().hex\n    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=1)\n    expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\n        \"web_hook\",\n        uuid.uuid4().hex,\n        token,\n        url,\n        expiration=expiration,\n        params=params,\n    )\n\n", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[\"alt\"] = self.alt_param\n        query_tuples = []\n        for key, value in params.items():\n            if isinstance(value, list):\n                for element in value:\n                    query_tuples.append((key, element.encode(\"utf-8\")))\n            elif isinstance(value, str) and callable(value):\n                query_tuples.append((key, value().encode(\"utf-8\")))\n            else:\n                query_tuples.append((key, value))\n        return urllib.parse.urlencode(query_tuples)", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status >= 300:\n            raise googleapiclient.errors.HttpError(resp, content)\n        if resp.status == 204:\n            return self.no_content_response\n        return self.deserialize(content)", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key not in original:\n            patch[key] = modified[key]\n        elif original[key] != modified[key]:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    if not uri:\n        return uri\n\n    uri_parts = list(urllib.parse.urlparse(uri))\n    query_params = urllib.parse.parse_qs(uri_parts[4])\n\n    for key, value in params.items():\n        if key in query_params:\n            if len(query_params[key]) > 1:\n                raise ValueError(\n                    \"URL contains repeated value: %s -> %s\"\n                    % (key, \", \".join(query_params[key]))\n                )\n            query_params[key] = [value]\n        else:\n            query_params[key] = [value]\n\n    uri_parts[4] = urllib.parse.urlencode(query_params, doseq=True)\n    return urllib.parse.urlunparse(uri_parts)", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    query_params[name] = value\n    new_query = urllib.parse.urlencode(query_params, doseq=True)\n    new_url = urllib.parse.urlunparse(\n        (\n            parsed_url.scheme,\n            parsed_url.netloc,\n            parsed_url.path,\n            parsed_url.params,\n            new_query,\n            parsed_url.fragment,\n        )\n    )\n    return new_url", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for _ in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write('\\n' * 100)\n                stdout.write(txt_frame)\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        pass", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        if not nsMap:\n            nsMap = {}\n        self.nsMap = nsMap\n        self.isFault = False\n        self.parser = ParserCreate(namespace_separator=NS_SEP)\n        self.parser.StartElementHandler = self.StartElementHandler\n        self.parser.EndElementHandler = self.EndElementHandler\n        self.parser.CharacterDataHandler = self.CharacterDataHandler\n        self.parser.StartNamespaceDeclHandler = self.StartNamespaceDeclHandler\n        self.parser.EndNamespaceDeclHandler = self.EndNamespaceDeclHandler\n        self.resultType = resultType\n        self.msg = None\n        if isinstance(response, six.binary_type):\n            self.parser.Parse(response)\n        else:\n            self.parser.ParseFile(response)\n        if self.isFault:\n            raise GetVmodlType(\"vmodl.fault.SystemError\")(msg=self.msg)\n        return self.deser.GetResult()", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    global _threadLocalContext\n    return _threadLocalContext.__dict__.setdefault('reqCtx', StringDict())\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    return int(-1.0 * (element_count * math.log(false_positive_probability)) / (LOG_2 * LOG_2) / 8.0 + 0.5)", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.script_hash())\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n    r1 = 15\n    r2 = 13\n    m = 5\n    n = 0xe6546b64\n\n    hash_value = seed\n\n    num_blocks = len(data) // 4\n\n    for block_start in range(0, num_blocks * 4, 4):\n        # Get a 4-byte string from the input starting at block_start.\n        # Convert this string to an integer, call it k.\n        k = struct.unpack('>I', data[block_start:block_start + 4])[0]\n\n        k = (k * c1) & 0xffffffff\n        k = (k << r1) | (k >> (32 - r1))\n        k = (k * c2) & 0xffffffff\n\n        hash_value = hash_value ^ k\n        hash_value = (hash_value << r2) | (hash_value >> (32 - r2))\n        hash_value = (hash_value * m) & 0xffffffff\n\n    # Handle the last few bytes of the input array.\n    remaining_bytes = len(data) & 3\n    remaining_value = 0\n\n    if remaining_bytes >= 3:\n        remaining_value = (remaining_value << 8) + data[-3]\n    if remaining_bytes >= 2:\n        remaining_value = (remaining_value << 8) + data[-2]\n    if remaining_bytes >= 1:\n        remaining_value = (remaining_value << 8) + data[-1]\n\n    remaining_value = (remaining_value << (8 * remaining_bytes))\n\n    remaining_value = (remaining_value * c1) & 0xffffffff\n    remaining_value = (remaining_value << r1) | (remaining_value >> (32 - r1))\n    remaining_value = (remaining_value * c2) & 0xffffffff\n\n    hash_value = hash_value ^", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            network = importlib.import_module(prefix + \".\" + symbol)\n            if network.symbol == symbol:\n                network.symbol = symbol\n                return network\n        except ImportError:\n            pass\n    raise ValueError(\"can't find network for netcode %s\" % symbol)", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        s = s[::-1]\n        first_byte = s[0]\n        value = first_byte & 0x7f\n        if require_minimal and value == 0 and len(s) > 1:\n            raise ScriptError(\"Minimal encoding error\")\n        is_negative = (first_byte & 0x80) != 0\n        for b in s[1:]:\n            value <<= 8\n            value |= b\n        if is_negative:\n            value = -value\n        return value", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    stack.append(hashlib.new('ripemd160', stack.pop()).digest())", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()))", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(hashlib.sha256(stack.pop()).digest()).digest())", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"can't parse provider for %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if hasattr(THREAD_LOCALS, \"providers\") and netcode in THREAD_LOCALS.providers:\n        return THREAD_LOCALS.providers[netcode]\n    return providers_for_netcode_from_env(netcode)", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index += len(self._locked_chain)\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        longest_chain = self._longest_local_block_chain()\n        if index < len(longest_chain):\n            the_hash = longest_chain[index]\n        else:\n            the_hash = self._longest_chain_cache[index - len(longest_chain)]\n        parent_hash = self.parent_hash if index <= 0 else longest_chain[index-1]\n        weight = self.weight_lookup.get(the_hash)\n        return (the_hash, parent_hash, weight)", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # Check if the path between h1 and h2 is already in the cache\n        if (h1, h2) in path_cache:\n            return path_cache[(h1, h2)]\n\n        # Find the maximum path from h1\n        path1 = self.maximum_path(h1, path_cache)\n\n        # Find the maximum path from h2\n        path2 = self.maximum_path(h2, path_cache)\n\n        # Find the common ancestor\n        common_ancestor = None\n        for i in range(min(len(path1), len(path2))):\n            if path1[i] == path2[i]:\n                common_ancestor = path1[i]\n            else:\n                break\n\n        # If no common ancestor is found, return None\n        if common_ancestor is None:\n            return None\n\n        # Find the ancestral path from h1 to the common ancestor\n        ancestral_path1 = path1[:path1.index(common_ancestor)]\n\n        # Find the ancestral path from h2 to the common ancestor\n        ancestral_path2 = path2[:path2.index(common_ancestor)]\n\n        # Cache the ancestral paths\n        path_cache[(h1, h2)] = (ancestral_path1, ancestral_path2)\n        path_cache[(h2, h1)] = (ancestral_path2, ancestral_path1)\n\n        # Return the ancestral paths\n        return ancestral_path1, ancestral_path2\n", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    spec = Encoding.BECH32 if spec == Encoding.BECH32 else Encoding.BECH32M\n    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    if data[0] == 0 and spec != Encoding.BECH32:\n        return (None, None)\n    if data[0] != 0 and spec != Encoding.BECH32M:\n        return (None, None)\n    return (data[0], decoded)", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child_index in path:\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child_index)\n        bip32_pub_node = bip32_pub_node.subkey_for_path(str(child_index))\n    return bip32_pub_node.root().subkey_for_path(path)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(c) for c in ip_bin)", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # check if field is blacklisted\n    if contents.lower().split()[0] in FIELD_BLACKLIST_CMDS:\n        return True\n\n    # check if field has too many arguments\n    field_args = FIELD_WORD_REGEX.findall(contents)\n    for field in FIELD_BLACKLIST:\n        if field[0].lower() == contents.lower().split()[0]:\n            if len(field_args) > field[1] + field[2]:\n                return True\n\n    # check if field has too many switches\n    field_switches = [arg for arg in field_args if FIELD_SWITCH_REGEX.match(arg)]\n    for field in FIELD_BLACKLIST:\n        if field[0].lower() == contents.lower().split()[0]:\n            if len(field_switches) > len(field[3]) + len(field[4]):\n                return True\n\n    # check if field has unacceptable format switches\n    field_format_switches = [arg for arg in field_args if arg.lower() in field[5]]\n    for field in FIELD_BLACKLIST:\n        if field[0].lower() == contents.lower().split()[0]:\n            if len(field_format_switches) > len(field[5]):\n                return True\n\n    return False", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt_file = PptFile(filename)\n        ppt_file.close()\n        return True\n    except Exception as e:\n        logging.debug(e)\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if isinstance(arg, str):\n        if treat_str_as_data:\n            data = arg\n        else:\n            data = open(arg, 'rb').read()\n    elif isinstance(arg, bytes):\n        data = arg\n    else:\n        data = arg.read()\n    return data.startswith(RTF_MAGIC)", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # first try: filename from src_path\n    if src_path:\n        sane_fname = sanitize_filename(os.path.basename(src_path),\n                                      max_len=max_len)\n        if sane_fname:\n            yield sane_fname\n\n    # second try: filename from tmp_path\n    if tmp_path:\n        sane_fname = sanitize_filename(os.path.basename(tmp_path),\n                                      max_len=max_len)\n        if sane_fname:\n            yield sane_fname\n\n    # third try: filename from filename\n    if filename:\n        sane_fname = sanitize_filename(os.path.basename(filename),\n                                      max_len=max_len)\n        if sane_fname:\n            yield sane_fname\n\n    # fourth try: random name\n    for _ in range(MAX_FILENAME_ATTEMPTS):\n        sane_fname = sanitize_filename(\n            ''.join(random.choice('0123456789abcdef')\n                    for _ in range(16)),\n            max_len=max_len)\n        if sane_fname:\n            yield sane_fname\n\n    # fifth try: random name without suffix\n    for _ in range(MAX_FILENAME_ATTEMPTS):\n        sane_fname = sanitize_filename(\n            ''.join(random.choice('0123456789abcdef')\n                    for _ in range(16)),\n            max_len=max_len,\n            replacement='')\n        if sane_fname:\n            yield sane_fname\n\n    # last try: random name with noname_index\n    sane_fname = sanitize_filename(\n        ''.join(random.choice('0123456789abcdef')\n                for _ in range(16)) + '_' + str(noname_index),\n        max_len=max_len)\n    if s", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    parser = XmlParser(filename)\n    if parser.is_single_xml():\n        # check for prog id in xml prolog\n        with uopen(filename, 'r') as handle:\n            match = re.search(OFFICE_XML_PROGID_REGEX, handle.read(1024))\n        if match:\n            if match.group(1) == WORD_XML_PROG_ID:\n                return DOCTYPE_WORD_XML\n            elif match.group(1) == EXCEL_XML_PROG_ID:\n                return DOCTYPE_EXCEL_XML\n            else:\n                return DOCTYPE_NONE\n        else:\n            return DOCTYPE_NONE\n\n    # regular ooxml file\n    content_types, _ = parser.get_content_types()\n    if not content_types:\n        return DOCTYPE_NONE\n\n    # check for content types\n    doctype = DOCTYPE_NONE\n    for subfile, content_type in content_types.items():\n        if content_type.startswith(CONTENT_TYPES_EXCEL):\n            doctype = DOCTYPE_EXCEL\n        elif content_type.startswith(CONTENT_TYPES_WORD):\n            doctype = DOCTYPE_WORD\n        elif content_type.startswith(CONTENT_TYPES_PPT):\n            doctype = DOCTYPE_POWERPOINT\n        elif content_type in CONTENT_TYPES_NEUTRAL:\n            pass\n        else:\n            logger.warning('Unknown content type {0} for {1}'\n                           .format(content_type, subfile))\n\n    # check for relationships\n    for subfile, elem, _ in parser.iter_xml(FILE_RELATIONSHIPS):\n        if elem.tag.endswith('Relationship'):\n            if elem.attrib['Type'] == 'http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument':\n                doctype = DOCTYPE_WORD\n            elif elem.attrib['Type'] ==", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise IOError('file is closed')\n        if self.pos >= self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset {0}'.format(offset))\n        if new_pos < 0:\n            raise ValueError('negative position {0}'.format(new_pos))\n        if new_pos > self.size:\n            raise ValueError('position {0} beyond end of file ({1})'\n                             .format(new_pos, self.size))\n        if new_pos == self.pos:\n            return\n        if new_pos > self.pos:\n            self._seek_skip(new_pos - self.pos)\n        else:\n            self.reset()\n            self._seek_skip(new_pos)\n        self.pos = new_pos\n        # print('ZipSubFile: seek to {0}, pos now {1}'\n        #       .format(new_pos, self.pos))", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.is_single_xml():\n            if subfiles:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            # do not use uopen, xml parser determines encoding on its own\n            with open(self.filename, 'rb') as handle:\n                yield None, handle, 0\n            self.did_iter_all = True\n        else:\n            zipper = None\n            try:\n                zipper = ZipFile(self.filename)\n                if not subfiles:\n                    subfiles = zipper.namelist()\n                elif isstr(subfiles):\n                    subfiles = [subfiles, ]\n                else:\n                    # make a copy in case original args are modified\n                    # Not sure whether this really is needed...\n                    subfiles = tuple(subfile for subfile in subfiles)\n\n                for subfile in subfiles:\n                    with zipper.open(subfile, 'r') as handle:\n                        try:\n                            for elem, depth in self._iter_xml(handle, need_children, tags):\n                                yield subfile, elem, depth\n                        except BadOOXML as oo_err:\n                            if oo_err.more_info.startswith('invalid subfile') and \\\n                                    subfile in oo_err.more_info:\n                                # no subfile in zip, so probably no ms office xml.\n                                # Maybe OpenDocument format? In any case, try to analyze.\n                                pass\n                            else:\n                                raise\n                if not subfiles:\n                    self.did_iter_all = True\n            except KeyError as orig_err:\n                # Note: do not change text of this message without adjusting\n                #       conditions in except handlers\n                raise BadOOXML(self.filename,\n                               'invalid subfile: ' + str(orig_err))\n            except BadZipfile:\n                raise BadOOXML(self.filename, 'not in zip format')\n            finally:\n                if zipper:\n                    zipper.close()\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        #TODO: add support for OpenXML\n        #TODO: add support for RTF\n        #TODO: add support for OLE package\n        #TODO: add support for OLE1\n        #TODO: add support for OLE2\n        #TODO: add support for OLE3\n        #TODO: add support for OLE4\n        #TODO: add support for OLE5\n        #TODO: add support for OLE6\n        #TODO: add support for OLE7\n        #TODO: add support for OLE8\n        #TODO: add support for OLE9\n        #TODO: add support for OLE10\n        #TODO: add support for OLE11\n        #TODO: add support for OLE12\n        #TODO: add support for OLE13\n        #TODO: add support for OLE14\n        #TODO: add support for OLE15\n        #TODO: add support for OLE16\n        #TODO: add support for OLE17\n        #TODO: add support for OLE18\n        #TODO: add support for OLE19\n        #TODO: add support for OLE20\n        #TODO: add support for OLE21\n        #TODO: add support for OLE22\n        #TODO: add support for OLE23\n        #TODO: add support for OLE24\n        #TODO: add support for OLE25\n        #TODO: add support for OLE26\n        #TODO: add support for OLE27\n        #TODO: add support for OLE28\n        #TODO: add support for OLE29\n        #TODO: add support for OLE30\n        #TODO: add support for OLE31\n        #TODO: add support for OLE32\n        #TODO: add support for OLE33\n        #TODO: add support for OLE34\n        #TODO: add support for OLE35\n        #TODO: add support for OLE36\n        #TODO: add", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    return nacaddr.IP(arg)\n  except nacaddr.AddressValueError:\n    raise argparse.ArgumentTypeError('%s is not a valid IP address' % arg)", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  ip1 = options.gmp[0]\n  ip2 = options.gmp[1]\n  # get the network objects for each IP\n  ip1_nets = get_ip_parents(ip1, db)\n  ip2_nets = get_ip_parents(ip2, db)\n  # get the common, diff1, and diff2\n  common = []\n  diff1 = []\n  diff2 = []\n  for net1 in ip1_nets:\n    for net2 in ip2_nets:\n      if net1[0] == net2[0]:\n        common.append(net1[0])\n        break\n    else:\n      diff1.append(net1[0])\n  for net2 in ip2_nets:\n    for net1 in ip1_nets:\n      if net2[0] == net1[0]:\n        break\n    else:\n      diff2.append(net2[0])\n  return common, diff1, diff2", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, sec_obj = options.cmp\n  first_name, first_nets = get_nets([first_obj], db)[0]\n  sec_name, sec_nets = get_nets([sec_obj], db)[0]\n  meta = (first_name, sec_name, sorted(list(set(first_nets) | set(sec_nets))))\n  results = sorted(list(set(first_nets) - set(sec_nets)))\n  return meta, results", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):  # pylint disable=protected-access\n    ip_obj = ip\n  else:\n    ip_obj = ipaddress.ip_network(ip, strict)\n  if ip_obj.version == 4:\n    return IPv4(ip_obj, comment, token)\n  elif ip_obj.version == 6:\n    return IPv6(ip_obj, comment, token)\n  else:\n    raise ValueError('Unknown IP version')", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        with warnings.catch_warnings():\n            if getattr(self.args, 'no_header_row', None):\n                warnings.simplefilter('ignore', agate.exceptions.ColumnWarning)\n\n            try:\n                self.main()\n            except (KeyboardInterrupt, BrokenPipeError):\n                pass\n\n        if 'f' not in self.override_flags:\n            self.input_file.close()", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    if isinstance(schema, str):\n        schema = StringIO(schema)\n\n    reader = FixedWidthReader(f, schema, **kwargs)\n\n    if output is None:\n        output = StringIO()\n\n    writer = agate.csv.writer(output)\n\n    for i, row in enumerate(reader):\n        if i < skip_lines:\n            continue\n\n        writer.writerow(row)\n\n    if output is None:\n        return output.getvalue()", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    backend_search = _re_backend.search(line)\n    if backend_search is not None:\n        return backend_search.groups()[0]\n    else:\n        return None", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.startswith(\"_\"):\n        return DUMMY_CONSTANT.format(name, backend_name)\n    elif name.startswith(\"_\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n\n    ", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        return max(candidates, key=self.probability)\n\n    ", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not text:\n            return text, []\n        if not include_symbol:\n            text = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]', '', text)\n        details = []\n        for block, idx in self.split_text_by_all(text, include_symbol):\n            if len(block) > 1 and re.search('[A-Za-z]', block):\n                if block in self.custom_confusion:\n                    correct_word = self.custom_confusion[block]\n                else:\n                    correct_word = self.correct_word(block)\n                if block != correct_word:\n                    start_idx = idx\n                    end_idx = start_idx + len(block)\n                    details.append([block, correct_word, start_idx, end_idx])\n                    text = text.replace(block, correct_word)\n        details = sorted(details, key=lambda i: i[2])\n        return text, details\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n    if len(X) < folds:\n        raise ValueError(\n            \"There are not enough samples ({}). Need at least {}.\".format(\n                len(X), folds\n            )\n        )\n    if clf is None:\n        clf = get_model(path)\n    print(\"KFold folds={}, running {} times\".format(folds, n))\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(\"{}/{}: {}\".format(i + 1, n, sum(scores) / len(scores)))\n    print(\"-------- total --------\")\n    print(sum(scores) / len(scores))\n    return sum(scores) / len(scores)", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n        table_name = 'stellar_%s%s%s' % (\n            self.table_name,\n            self.snapshot.hash,\n            postfix\n        )\n        if old:\n            return table_name\n        return 'stellar_%s' % hashlib.md5(\n            ('%s|%s|%s' % (\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )).encode('utf-8')\n        ).hexdigest()[:16]", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] < 3:\n        if isinstance(anything, str):\n            return unicode(anything, \"utf-8\")\n        elif isinstance(anything, list):\n            return [cast_to_unicode(elem) for elem in anything]\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for (key, value) in anything.items()}\n        else:\n            return anything\n    else:\n        return anything", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is None:\n            print(text)\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if REDIRECTION_SYM in tokens:\n            redirection_index = tokens.index(REDIRECTION_SYM)\n            if redirection_index == len(tokens) - 1:\n                return (RedirectionType.quiet, None)\n            return (RedirectionType.overwrite, tokens[redirection_index + 1])\n        if REDIRECTION_APPEND_SYM in tokens:\n            redirection_index = tokens.index(REDIRECTION_APPEND_SYM)\n            if redirection_index == len(tokens) - 1:\n                return (RedirectionType.quiet, None)\n            return (RedirectionType.append, tokens[redirection_index + 1])\n        return None\n\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return AST.Alias\n        elif unit_type_str == \"slot\":\n            return AST.Slot\n        elif unit_type_str == \"intent\":\n            return AST.Intent\n        elif unit_type_str == \"template\":\n            return AST.Template\n        elif unit_type_str == \"rule\":\n            return AST.Rule\n        elif unit_type_str == \"synonym\":\n            return AST.Synonym\n        elif unit_type_str == \"regex\":\n            return AST.Regex\n        elif unit_type_str == \"context\":\n            return AST.Context\n        elif unit_type_str == \"word\":\n            return AST.Word\n        elif unit_type_str == \"wordset\":\n            return AST.Wordset\n        elif unit_type_str == \"wordlist\":\n            return AST.Wordlist\n        elif unit_type_str == \"wordlistlink\":\n            return AST.WordlistLink\n        elif unit_type_str == \"wordlistgroup\":\n            return AST.WordlistGroup\n        elif unit_type_str == \"wordlistgrouplink\":\n            return AST.WordlistGroupLink\n        elif unit_type_str == \"wordlistgroupreference\":\n            return AST.WordlistGroupReference\n        elif unit_type_str == \"wordlistgroupreferencelink\":\n            return AST.WordlistGroupReferenceLink\n        elif unit_type_str == \"wordlistgroupreferencegrouplink\":\n            return AST.WordlistGroupReferenceGroupLink\n        elif unit_type_str == \"wordlistgroupreferencegrouplinkreference\":\n            return AST.WordlistGroupReferenceGroupLinkReference\n        elif unit_type_str == \"wordlistgroupreferencegrouplinkreferencelink\":\n            return AST.WordlistGroupReferenceGroupLinkReferenceLink\n        elif unit_type_str == \"wordlistgroupreferencegrouplinkreferencelinkreference\":\n            return AST.WordlistGroupReferenceGroupLinkReferenceLinkReference\n        elif unit_type_str == \"wordlistgroupreferencegroupl", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\"Not enough arguments for the command 'unhide'.\")\n            return\n\n        unit_type = self.command_tokens[1]\n        unit_name = self.command_tokens[2]\n        if len(self.command_tokens) == 3:\n            self.execute_on_unit(unit_type, unit_name)\n        elif len(self.command_tokens) == 4:\n            variation_name = self.command_tokens[3]\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            self.print_wrapper.error_log(\"Too many arguments for the command 'unhide'.\")\n\n\n    ", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == 'rasa':\n        from chatette.adapters.rasa import RasaWriter\n        return RasaWriter(base_filepath)\n    elif adapter_name == 'rasa-md':\n        from chatette.adapters.rasa_md import RasaMdWriter\n        return RasaMdWriter(base_filepath)\n    elif adapter_name == 'rasamd':\n        from chatette.adapters.rasa_md import RasaMdWriter\n        return RasaMdWriter(base_filepath, markdown=True)\n    elif adapter_name == 'jsonl':\n        from chatette.adapters.jsonl import JsonListWriter\n        return JsonListWriter(base_filepath)\n    else:\n        raise ValueError(\"Unknown adapter name '\" + adapter_name + \"'.\")", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.unitreference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.type, self.identifier, self.variation,\n            self._build_modifiers_repr()\n        )", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        return None\n\n    resource_cls = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    resource_spec = resources[resource_kind]\n\n    if resource_spec == \"system\":\n        resource = resource_cls.from_system()\n    else:\n        resource = resource_cls.from_spec(resource_spec)\n\n    if validate:\n        resource_cls.validate(resource)\n\n    return resource", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    return {\n        resource_kind: resource.from_system()\n        for resource_kind, resource in _RESOURCE_REGISTRY.items()\n    }", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (int, float)):\n            return float(spec)\n\n        if isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return float(spec[:-1]) / 1000\n            if spec.endswith(\"%\"):\n                return float(spec[:-1]) / 100 * query_cgroup_cpu_count()\n            if spec.endswith(\"c\"):\n                return float(spec[:-1])\n            if spec == \"system\":\n                return cls.from_system()\n\n        raise ValueError(f\"Invalid CPU resource limit '{spec}'\")\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if psutil.LINUX:\n            return query_cgroup_cpu_count()\n        else:\n            return query_os_cpu_count()\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"CPU resource limit must be non-negative, got {val}\")\n\n        if val > cls.from_system():\n            raise BentoMLConfigException(\n                f\"CPU resource limit {val} is greater than the system available {cls.from_system()}\"\n            )", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if import_module:\n            try:\n                module = __import__(self.module, fromlist=[self.qualname])\n            except ImportError as e:\n                raise ValueError(\n                    f\"Failed to import {self.module}.{self.qualname}.\"\n                ) from e\n            try:\n                self._runtime_class = getattr(module, self.qualname)\n            except AttributeError as e:\n                raise ValueError(\n                    f\"Failed to get {self.module}.{self.qualname} from {module}.\"\n                ) from e\n        else:\n            raise ValueError(\n                f\"{self.module}.{self.qualname} is not available. \"\n                \"Please make sure the module is installed.\"\n            )\n\n        return self._runtime_class", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name)\n\n        if not isinstance(signatures, dict):\n            raise BentoMLException(\n                \"signatures must be a dict of method name to ModelSignature\"\n            )\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if metadata is None:\n            metadata = {}\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        info = ModelInfo(\n            tag=name,\n            module=module,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n            context=context,\n            signatures=signatures,\n            api_version=api_version,\n        )\n\n        model_fs = fs.open_fs(f\"mem://{name.name}-{name.version}\")\n        model_fs.makedirs(\"\", recreate=True)\n\n        model = Model(\n            tag=name,\n            model_fs=model_fs,\n            info=info,\n            custom_objects=custom_objects,\n            _internal=True,\n        )\n        model.flush()\n        return model\n", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        try:\n            with item_fs.open(MODEL_YAML_FILENAME, \"r\", encoding=\"utf-8\") as model_yaml:\n                info = ModelInfo.from_yaml_file(model_yaml)\n        except (fs.errors.ResourceNotFound, fs.errors.FileExpected):\n            raise BentoMLException(\n                f\"{item_fs.getsyspath('/')} is not a valid BentoML model\"\n            )\n\n        return cls(info.tag, item_fs, info, _internal=True)\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)\n\n", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        metadata[key] = _validate_metadata_entry(val)\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = os.environ.get(\"BENTOML_SERVE_ID\")\n    if serve_id is None:\n        serve_id = secrets.token_hex(16)\n        os.environ[\"BENTOML_SERVE_ID\"] = serve_id\n    return ServeInfo(\n        serve_id=serve_id, serve_started_timestamp=datetime.now(timezone.utc)\n    )\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event_properties = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        production=production,\n        serve_kind=serve_kind,\n        serve_from_server_api=from_server_api,\n        serve_started_timestamp=serve_info.serve_started_timestamp,\n        num_models=len(svc.models),\n        num_runners=len(svc.runners),\n        num_apis=len(svc.apis),\n        model_types=[model.module.__name__ for model in svc.models],\n        runner_types=[runner.cls.__name__ for runner in svc.runners],\n        api_input_types=[api.input_type.__name__ for api in svc.apis],\n        api_output_types=[api.output_type.__name__ for api in svc.apis],\n    )\n    track(event_properties)\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if user_provided_svc_name.lower() != user_provided_svc_name:\n        logger.warning(\n            \"Service name %s is not lowercase, converting it to lowercase.\",\n            user_provided_svc_name,\n        )\n    svc_name = user_provided_svc_name.lower()\n    Tag.from_str(svc_name)\n    return svc_name", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if any(p in k for p in punctuation):\n            k = f'\"{k}\"'\n        new_key = parent + sep + k if parent else k\n        if isinstance(v, dict):\n            yield from flatten_dict(v, new_key, sep=sep)\n        else:\n            yield new_key, v", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Configuration file {path} does not exist.\")\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            for i, item in enumerate(v):\n                if isinstance(item, str):\n                    v[i] = expand_env_var(item)\n\n", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n        # use nvidia gpu\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            if isinstance(workers_per_resource, float):\n                # NOTE: We hit this branch when workers_per_resource is set to\n                # float, for example 0.5 or 0.25\n                if workers_per_resource > 1:\n                    raise ValueError(\n                        \"Currently, the default strategy doesn't support workers_per_resource > 1. It is recommended that one should implement a custom strategy in this case.\"\n                    )\n                # We are round the assigned resource here. This means if workers_per_resource=.4\n                # then it will round down to 2. If workers_per_source=0.6, then it will also round up to 2.\n                assigned_resource_per_worker = round(1 / workers_per_resource)\n                if len(nvidia_gpus) < assigned_resource_per_worker:\n                    logger.warning(\n                        \"Failed to allocate %s GPUs for %s (number of available GPUs < assigned workers per resource [%s])\",\n                        nvidia_gpus,\n                        worker_index,\n                        assigned_resource_per_worker,\n                    )\n                    raise IndexError(\n                        f\"There aren't enough assigned GPU(s) for given worker id '{worker_index}' [required: {assigned_resource_per_worker}].\"\n                    )\n                assigned_gpu = nvidia_gpus[\n                    assigned_resource_per_worker\n                    * worker_index : assigned_resource_per_worker\n                    * (worker_index + 1)\n                ]\n                dev = \",\".join(map(str, assigned_gpu))\n            else:\n                dev = str(nvidia_gpus[worker_index", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return {\n                \"CUDA_VISIBLE_DEVICES\": str(nvidia_gpus[worker_index % len(nvidia_gpus)]),\n            }\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return {\n                    env: str(workers_per_resource)\n                    for env in THREAD_ENVS\n                    if env in os.environ\n                }\n\n            return {}\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )\n", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        if not batches:\n            return np.array([]), []\n\n        if batch_dim != 0:\n            batches = [np.moveaxis(batch, batch_dim, 0) for batch in batches]\n\n        indices = list(itertools.accumulate(subbatch.shape[0] for subbatch in batches))\n        indices = [0] + indices\n        return np.concatenate(batches, axis=0), indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if inp.ndim != 0:\n            if not inp.flags.c_contiguous:\n                inp = np.ascontiguousarray(inp)\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(inp)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            inp.shape[batch_dim],\n            meta=meta,\n        )\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\n                \"PandasDataFrameContainer does not support batch_dim other than 0\"\n            )\n        if isinstance(batch, pd.Series):\n            batch = batch.to_frame()\n\n        meta = {\"format\": \"pickle5\"}\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            meta[\"pickle_bytes_str\"] = bs_str\n            meta[\"indices\"] = indices\n            return cls.create_payload(concat_buffer_bs, batch.shape[batch_dim])\n        else:\n            meta[\"with_buffer\"] = False\n            return cls.create_payload(bs, batch.shape[batch_dim])\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"pickle5\")\n        if format == \"pickle5\":\n            if payload.meta[\"with_buffer\"]:\n                bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n                bs = base64.b64decode(bs_str)\n                indices = t.cast(t.List[int], payload.meta[\"indices\"])\n                return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n            else:\n                return pd.read_pickle(io.BytesIO(payload.data))\n\n        return pd.read_pickle(io.BytesIO(payload.data))\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n        return cls.create_payload(pickle.dumps(batch), batch_size=len(batch))\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        ip = None\n        if \"{\" in server_str:\n            ip = server_str.split(\"{\")[1].split(\"}\")[0]\n            server_str = server_str.split(\"{\")[0]\n\n        if \"[\" in server_str:\n            return cls._parse_ipv6_server_string(server_str)\n        elif \"[\" in ip:\n            return cls._parse_ipv6_server_string(ip)\n        else:\n            return cls._parse_ipv4_server_string(server_str)\n\n    ", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(cls._cli_option)]\n        if result.is_vulnerable_to_heartbleed:\n            result_as_txt.append(\"VULNERABLE - Server is vulnerable to Heartbleed\")\n        else:\n            result_as_txt.append(\"OK - Server is not vulnerable to Heartbleed\")\n        return result_as_txt\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(\"HTTP Security Headers\")]\n        if result.http_error_trace:\n            result_as_txt.append(cls._format_field(\"\", \"Error when retrieving HTTP headers\"))\n            result_as_txt.append(cls._format_text(str(result.http_error_trace)))\n        else:\n            result_as_txt.append(cls._format_field(\"HTTP Request Sent\", result.http_request_sent))\n            if result.http_path_redirected_to:\n                result_as_txt.append(\n                    cls._format_field(\"HTTP Path Redirected To\", result.http_path_redirected_to)\n                )\n\n            if result.strict_transport_security_header:\n                result_as_txt.append(cls._format_title(\"Strict-Transport-Security Header\"))\n                result_as_txt.append(\n                    cls._format_field(\n                        \"Max Age\",\n                        str(result.strict_transport_security_header.max_age)\n                        if result.strict_transport_security_header.max_age\n                        else \"None\",\n                    )\n                )\n                result_as_txt.append(\n                    cls._format_field(\n                        \"Include Subdomains\",\n                        \"Yes\" if result.strict_transport_security_header.include_subdomains else \"No\",\n                    )\n                )\n                result_as_txt.append(\n                    cls._format_field(\n                        \"Preload\", \"Yes\" if result.strict_transport_security_header.preload else \"No\"\n                    )\n                )\n\n        return result_as_txt\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # If the server returned a redirection, check if it is to the same server\n    if http_response.status in [301, 302, 303, 307, 308]:\n        location_header = http_response.getheader(\"location\", None)\n        if not location_header:\n            return None\n\n        # Parse the location header\n        location_url = urlsplit(location_header)\n        if location_url.hostname == server_host_name and location_url.port == server_port:\n            return location_url.path\n\n    return None", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(\"  Client-initiated Renegotiation:\")\n        result_txt.append(\n            \"    Accepted: {}\".format(\n                \"OK - Supported\" if result.is_vulnerable_to_client_renegotiation_dos else \"VULNERABLE - Rejected\"\n            )\n        )\n        result_txt.append(\"  Secure Renegotiation:\")\n        result_txt.append(\n            \"    Supported: {}\".format(\n                \"OK - Supported\" if result.supports_secure_renegotiation else \"VULNERABLE - Not Supported\"\n            )\n        )\n        return result_txt\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(f\"Certificate info for {result.server_location.hostname}:{result.server_location.port}\")]\n\n        # Display the hostname sent for SNI\n        result_as_txt.append(cls._format_field(\"Hostname sent for SNI:\", result.server_location.hostname))\n\n        # Display the number of certificates detected\n        result_as_txt.append(cls._format_field(\"Number of certificates detected:\", str(len(result.certificate_deployments))))\n\n        # Display the certificates\n        for index, cert_deployment in enumerate(result.certificate_deployments):\n            result_as_txt.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return result_as_txt\n", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    if name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME):\n        return name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME)[0].value\n    else:\n        return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the certificate chain is empty\n        if not verified_certificate_chain:\n            return None\n\n        # Check if the certificate chain contains any blacklisted certificates\n        for certificate in verified_certificate_chain:\n            if cls._is_blacklisted(certificate):\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check if the certificate chain contains any whitelisted certificates\n        for certificate in verified_certificate_chain:\n            if cls._is_whitelisted(certificate):\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If no blacklisted or whitelisted certificates are found, return None\n        return None\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension([], [])\n\n    san = cast(SubjectAlternativeName, san_extension.value)\n    dns_names = [dns_name.value for dns_name in san.get_values_for_type(DNSName)]\n    ip_addresses = [ip_address.value for ip_address in san.get_values_for_type(IPAddress)]\n\n    return SubjectAlternativeNameExtension(dns_names, ip_addresses)\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        certificate_names = certificate.subject.get_attributes_for_oid(NameOID.COMMON_NAME)\n        certificate_alt_names = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        certificate_alt_names = certificate_alt_names.value.get_values_for_type(DNSName)\n        certificate_names_dict = {\n            name.value: name.value for name in certificate_names + certificate_alt_names\n        }\n        for name in certificate_names_dict.values():\n            try:\n                match_hostname(name, server_hostname)\n                return True\n            except CertificateError:\n                pass\n        return False\n    except AttributeError:\n        return False", "idx": 1873}
